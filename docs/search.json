[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science in Python",
    "section": "",
    "text": "Preface\nData Science and Tutorial courses taught by Professor Markus Loecher (HWR Berlin).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#united-notebooks",
    "href": "index.html#united-notebooks",
    "title": "Introduction to Data Science in Python",
    "section": "United Notebooks",
    "text": "United Notebooks\nIn order to organize our lecture materials more coherently and easier to navigate, I have consolidated both classes into one place.\nIn addition, I have imposed an approximate temporal ordering (modified by relevance and topic relations) which might help you.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "SS_01-HomeOffice.html",
    "href": "SS_01-HomeOffice.html",
    "title": "1¬† Home Office Simulation",
    "section": "",
    "text": "Skeleton code for desired function:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef office_simulation(n=20, n_weeks=1000, seed=None, plot=True):\n    \"\"\"\n    Simulates office attendance given n employees each in-office 2 of 5 weekdays.\n\n    Parameters\n    ----------\n    n : int\n        Number of employees.\n    n_weeks : int\n        Number of simulated weeks.\n    seed : int or None\n        Random seed for reproducibility.\n    plot : bool\n        Whether to plot the resulting distribution.\n\n    Returns\n    -------\n    dist : array\n        Empirical distribution of #employees present on a random day.\n    overlap_prob : float\n        Probability that two specific employees overlap &gt;=2 days per week.\n    \"\"\"\n\n    rng = np.random.default_rng(seed)\n    days = np.arange(5)\n\n    day_counts = []\n    overlap_count = 0\n\n    for _ in range(n_weeks):\n        # Each employee randomly chooses 2 distinct office days\n\n\n        # Count how many employees come in on each day\n\n\n        # Overlap between two particular employees (say #0 and #1)\n\n\n    # Distribution of employees per day\n\n\n    if plot:\n        plt.figure(figsize=(7, 4))\n\n        plt.xlabel(\"Number of employees in the office\")\n        plt.ylabel(\"Probability\")\n        plt.title(f\"Distribution of employees in office (n={n})\")\n        plt.grid(alpha=0.3)\n        plt.show()\n\n    return dist, overlap_prob\n\n\n  File \"/tmp/ipython-input-3238389192.py\", line 46\n    if plot:\n    ^\nIndentationError: expected an indented block after 'for' statement on line 33\nQ: What is the avarge num employees in the office ?? (n=25)\nA: 25 *(4/10) = 10\nA: 8 or 10 or\nn=25 90% the num will be between 5 and 16 ‚Äúbetween 50% and 160%)\nn = 2500 90% the num will be between ? and ?\n‚ÄúBigger is better (risk)‚Äù\ndist, overlap_prob = office_simulation(n=25, n_weeks=2000, seed=42)\n\n\nprint(f\"\\nP(two specific employees overlap ‚â• 2 days/week): {overlap_prob:.4f}\")\n\n\n\n\n\n\n\n\nDistribution of # employees in the office on a random day:\n 1: 0.000\n 2: 0.000\n 3: 0.002\n 4: 0.007\n 5: 0.020\n 6: 0.044\n 7: 0.080\n 8: 0.120\n 9: 0.151\n10: 0.162\n11: 0.146\n12: 0.114\n13: 0.076\n14: 0.043\n15: 0.021\n16: 0.009\n17: 0.003\n18: 0.001\n19: 0.000\n20: 0.000\n21: 0.000\n22: 0.000\n\nP(two specific employees overlap ‚â• 2 days/week): 0.0998",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "SS_01-HomeOffice.html#skeleton-code-for-desired-function",
    "href": "SS_01-HomeOffice.html#skeleton-code-for-desired-function",
    "title": "1¬† Home Office Simulation",
    "section": "",
    "text": "One week, one employee:\n\nimport numpy as np\n\nrng = np.random.default_rng()\n\n\n#simulate 2 random days in the office\n\n#rng.integers(1,6,2) # this could create duplicates\n\n# WAIT We need to set replace = FALSE !! Because there should be no duplicates!\nempChoice = rng.choice(np.arange(1,6),2, replace = False )\nweekdays = np.zeros(6)\nprint(weekdays)\nprint(empChoice)\n#weekdays[0] += 1\n#weekdays[4] += 1\n\nweekdays[empChoice -1 ] += 1\nweekdays\n\n\n\n[0. 0. 0. 0. 0. 0.]\n[3 4]\n\n\narray([0., 0., 1., 1., 0., 0.])\n\n\nNow we go from one to n employees, so we need a loop !\n\nn = 20\nweekdays = np.zeros(5, dtype = \"int\")\nNumEmpInOffice = np.zeros(n+1, dtype = \"int\")\n\nfor i in range(n):\n  empChoice = rng.choice(np.arange(1,6),2, replace = False )\n  weekdays[empChoice -1 ] += 1\n\n\nNumEmpInOffice[weekdays] += 1\nprint(weekdays)\n\nif np.sum(weekdays)!= 2*n:\n  print(\"sth. is wrong!\") #sanity check!\n\nNumEmpInOffice\n\n[10  4  9 10  7]\n\n\narray([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\nLet us simulate MANY WEEKS !!\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef HomeOfficeSimulation(n = 20, n_weeks = 1000, daysInWeek = 5, days_in_office = 2, seed=123, verbose = False):\n  rng = np.random.default_rng(seed)\n\n  NumEmpInOffice = np.zeros(n+1, dtype = \"int\")#this is a 1D array of length n+1\n  for j in range(n_weeks):\n    weekdays = np.zeros(daysInWeek, dtype = \"int\")#this is a 1D array of length 5\n\n    for i in range(n):\n      empChoice = rng.choice(np.arange(1,daysInWeek+1),days_in_office, replace = False )\n      weekdays[empChoice -1 ] += 1#this is of length 5 !!\n\n    #NumEmpInOffice[weekdays] += 1 #for each of 5 elements I increment by 1\n    for w in weekdays:\n      NumEmpInOffice[w] += 1\n\n    if (verbose):\n      print(weekdays)\n      #print(NumEmpInOffice)\n      print(pd.Series(NumEmpInOffice, index=range(n + 1)).to_dict())\n\n    if np.sum(weekdays)!= 2*n:\n      print(\"sth. is wrong!\") #sanity check!\n  #we would like to return the probability !\n  #of course they have to add to one in total\n  return NumEmpInOffice/np.sum(NumEmpInOffice)\n  #return NumEmpInOffice/(n_weeks*5)#(n_weeks*daysInWeek)\n\n\n\n\none_sim = HomeOfficeSimulation(n = 6, n_weeks = 3, verbose = False)\nnp.sum(one_sim)\n\nnp.float64(5.0)\n\n\nProbability that two particular employees overlap ‚â• 2 days\n\none_sim = HomeOfficeSimulation(n = 2, n_weeks = 1000)\none_sim\n\narray([0.3572, 0.4856, 0.1572])\n\n\n\nx1 = [4, 7, -5, 3]#this is a 1D array\nx2 = [3, 5, 6, 1]#this is a 1D array\n\nx1 + x2# the \"+\" operator for lists is an append operation !!\nnp.array(x1) + np.array(x2)\nnp.array(x1) * np.array(x2)\n#It seems useful that we need numpy in adddition to lists!\n\n#Why would we need pandas on top of numpy !!\ny1 = np.array(x1)\ny1[1]#numeric indexing\n#y1[\"Wednesday\"]#numpy cannot handle other ways of indexing !\n\np1 = pd.Series(y1, index = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\"])\np1[\"Wednesday\"]\np1[[\"Wednesday\",\"Thursday\"]]\n\"Wednesday\" in p1\n\"Friday\" in p1\np1\np1.to_dict()\n\n{'Monday': 4, 'Tuesday': 7, 'Wednesday': -5, 'Thursday': 3}\n\n\n\np1.array\np1.index\n\nIndex(['Monday', 'Tuesday', 'Wednesday', 'Thursday'], dtype='object')\n\n\n\none_sim = HomeOfficeSimulation(n_weeks= 500)\n\n\none_sim = HomeOfficeSimulation(n=5, n_weeks= 2, verbose=1)\n\n[4 3 0 1 2]\n{0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 0}\n[2 2 2 2 2]\n{0: 1, 1: 1, 2: 2, 3: 1, 4: 1, 5: 0}\n\n\n\nnp.sum(one_sim)\n\nnp.int64(6)",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "SS_01-HomeOffice.html#plots",
    "href": "SS_01-HomeOffice.html#plots",
    "title": "1¬† Home Office Simulation",
    "section": "Plots",
    "text": "Plots\nWhat type of plot do we need ???\n\nHistogram ?? Maybe not because there is no need for binning !\nSimple bar plot !\n\n\n#let us plot !!\nplt.figure(figsize=(7, 4))\nplt.bar(np.arange(n+1),one_sim)\nplt.xlabel(\"Number of employees in the office\")\nplt.ylabel(\"Probability\")\nplt.title(f\"Distribution of employees in office (n={n})\")\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.set()\nsns.barplot(one_sim);",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "SS_01-HomeOffice.html#debugging",
    "href": "SS_01-HomeOffice.html#debugging",
    "title": "1¬† Home Office Simulation",
    "section": "Debugging",
    "text": "Debugging\n\nSanity check:\n\nDo the probabilities add to one ?\nRead up on pandas series\nWrite basic code to double check our indexing logic !\n\n\nn=10\nNumEmpInOffice = np.zeros(n+1, dtype = \"int\")\nweekdays = [3,3]\n#NumEmpInOffice[weekdays] += 1\nfor w in weekdays:\n  NumEmpInOffice[w] += 1\n\nprint(NumEmpInOffice)\n\n[0 0 0 1 0 0 0 0 0 0 0]",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "SS_01-HomeOffice.html#extensions",
    "href": "SS_01-HomeOffice.html#extensions",
    "title": "1¬† Home Office Simulation",
    "section": "Extensions",
    "text": "Extensions\n\nFold the plotting code into the function\nMake all ‚Äúparameters‚Äù function arguments, e.g.¬†days_in_a_week (5), days_in_office (2), ‚Ä¶\nCan you use this code ‚Äúas is‚Äù for the following related questions:\n\nProbability that two particular employees overlap ‚â• 2 days\nBirthday problem",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "SS_02-BinomialProcesses.html",
    "href": "SS_02-BinomialProcesses.html",
    "title": "2¬† Binomial Sampling",
    "section": "",
    "text": "3 Grouped Operations",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Binomial Sampling</span>"
    ]
  },
  {
    "objectID": "SS_02-BinomialProcesses.html#coin-tossing",
    "href": "SS_02-BinomialProcesses.html#coin-tossing",
    "title": "2¬† Binomial Sampling",
    "section": "Coin Tossing",
    "text": "Coin Tossing\n\nWrite a function which simulates tossing a ‚Äúbiased coin‚Äù (with \\(p=0.157407\\)) \\(108\\) times and computes the proportion of ‚Äúheads‚Äù.\nCall this function ‚Äúmany‚Äù times and plot a histogram\n\n\ndef toss_biased_coin(pHead, n, seed=None):\n  \"\"\"\n    Simulates n tosses of a biased coin and returns the proportion of heads,\n    using rng.choice() for better conceptual clarity.\n\n    Parameters\n    ----------\n    pHead : float\n        Probability of heads (between 0 and 1).\n    n : int\n        Number of tosses.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    float\n        Proportion of heads observed.\n    \"\"\"\n  rng = np.random.default_rng(seed)\n\n  tosses = rng.choice([0,1], size=n, p=[1-pHead,pHead])\n  cts = np.sum(tosses == 1)\n  cts = np.sum(tosses)\n  prop_heads = cts/n\n\n  prop_heads = np.mean(tosses)\n\n  return prop_heads\n\n\n\ntoss_biased_coin(pHead=0.157407, n= 108)#, seed = 123)\n\n0.14814814814814814\n\n\n\n#let us run this many times !\npHead_obs = np.zeros(500)\n\nfor i in range(500):\n  pHead_obs[i] = toss_biased_coin(pHead=0.157407, n= 108)\n\n\n\n#95% interval:\nnp.percentile(pHead_obs, [2.5,97.5])\n\narray([0.10185185, 0.23148148])\n\n\n\n#histogram\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\nRelated Questions:\n\nThe board of a large international company consists of 7 women and 3 men. If sex played no role, i.e.¬†if we assume that the probabilities of m/f were equal, how likely is it to see such asymmetric proportions?\nHow does this assessment change if the board consisted of 100 members and there would be 70 men and 30 women ?\n\n\n#let us run this many times !\nnSim = 1000\npHead_obs = np.zeros(nSim)\n\nfor i in range(nSim):\n  pHead_obs[i] = toss_biased_coin(pHead=0.5, n= 10)\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\n\n#how likely is it that one sees sth. \"as extreme as 7/3\"\nnp.mean(pHead_obs &gt;= 0.7) + np.mean(pHead_obs &lt;= 0.3)\n\n0.327\n\n\n\n#let us run this many times !\nnSim = 10000\npHead_obs = np.zeros(nSim)\n\nfor i in range(nSim):\n  pHead_obs[i] = toss_biased_coin(pHead=0.5, n= 100)\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\n\nnp.mean(pHead_obs &gt;= 0.7) + np.mean(pHead_obs &lt;= 0.3)\n\n0.0\n\n\n\npHead = 0.5\n#What is the exact answer of this question given that you now know sigma !\ns = np.sqrt(pHead*(1-pHead)/100)\nprint(s)\n#how likely to be above 0.7\nzScore = (0.7-0.5)/s\nzScore\n\n#scipy.stat\n\n0.05\n\n\n3.999999999999999\n\n\n\ns = np.std(pHead_obs)\nprint(s)\n0.5 + 2*s\n0.5 - 2*s\n\n0.049661881065863785\n\n\n0.40067623786827244\n\n\nSUPER IMPORTANT !! What is the stdev in the binomial distribution ??\nThe FAMOUS SQRT(N) LAW !!!\n\np= 0.5\n#Variance:\np*(1-p)\nnp.sqrt(p*(1-p)/10)\n\n0.15811388300841897",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Binomial Sampling</span>"
    ]
  },
  {
    "objectID": "SS_03-Intro_Testing.html",
    "href": "SS_03-Intro_Testing.html",
    "title": "3¬† AB Testing",
    "section": "",
    "text": "Permutation 2-sample test\nWe have used the bootstrap to compare two sets of data, both of which are samples. In particular, we can test two-sample hypotheses such as\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m \\neq \\mu_f\\)\nor the one-sided versions:\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m &gt; \\mu_f\\)\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m &lt; \\mu_f\\)\nAnother way to compare 2 distributions (in some ways much more straightforward than the bootstrap) is permutation sampling. It directly simulates the hypothesis that two variables have identical probability distributions.\nA permutation sample of two arrays having respectively \\(n_1\\) and \\(n_2\\) entries is constructed by concatenating the arrays together, scrambling the contents of the concatenated array, and then taking the first \\(n_1\\) entries as the permutation sample of the first array and the last \\(n_2\\) entries as the permutation sample of the second array.\nAt DataCamp the first example offers a nice visualization of this process:\nTake a look at the code in ourFunctions.py to run a permutation test\nLet us apply our first permutation sampling on the Titanic data. (First, we explore the data a bit)\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\n\n\n  \n    \n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\nPclassSurv = titanic.groupby(['pclass', 'survived'])\nPclassSurv.size()\n\n\n\n\n\n\n\n\n\n0\n\n\npclass\nsurvived\n\n\n\n\n\n1\n0\n80\n\n\n1\n136\n\n\n2\n0\n97\n\n\n1\n87\n\n\n3\n0\n372\n\n\n1\n119\n\n\n\n\ndtype: int64\npd.crosstab(titanic.pclass, titanic.survived,margins=True)\n\n\n\n\n\n\n\nsurvived\n0\n1\nAll\n\n\npclass\n\n\n\n\n\n\n\n1\n80\n136\n216\n\n\n2\n97\n87\n184\n\n\n3\n372\n119\n491\n\n\nAll\n549\n342\n891\nWomenOnly = titanic[titanic[\"sex\"]==\"female\"]\npd.crosstab(WomenOnly.pclass, WomenOnly.survived,margins=True)\n\n\n  \n    \n\n\n\n\n\nsurvived\n0\n1\nAll\n\n\npclass\n\n\n\n\n\n\n\n1\n3\n91\n94\n\n\n2\n6\n70\n76\n\n\n3\n72\n72\n144\n\n\nAll\n81\n233\n314\nTest the claim that the survival chances of women in 1st and 2nd class were pretty much the same.\ntitanic_women_p1= WomenOnly[WomenOnly['pclass'] == 1][\"survived\"]\ntitanic_women_p2= WomenOnly[WomenOnly['pclass'] == 2][\"survived\"]\ndiff_reps = draw_perm_reps(titanic_women_p1, titanic_women_p2, size = 500)\nobsmean_p1 = np.mean(titanic_women_p1)\nobsmean_p2 =np.mean(titanic_women_p2)\nobsDiff =obsmean_p1-obsmean_p2\n#We could assume that P1 has a higher surv probability\np_val= np.mean(diff_reps &gt;= obsDiff)\np_val\n\n# We do not reject\n\nnp.float64(0.134)\nWomenOnly = titanic[titanic[\"sex\"]==\"female\"]\nwomen1st = WomenOnly[WomenOnly[\"pclass\"]==1]\nwomen2nd = WomenOnly[WomenOnly[\"pclass\"]==2]\npermutation_sample(women1st.survived, women2nd.survived)\ndiff_reps = draw_perm_reps(women1st.survived, women2nd.survived, size=10000)\nplt.hist(diff_reps);\nplt.axvline(obsDiff, color = \"red\");\npVal = np.mean(diff_reps &lt;= obsDiff)\npVal\n\nnp.float64(0.9552)\nobsDiff\n\nnp.float64(0.04703247480403139)\nWe could choose alpha = 0.05, but keep in mind the following - would you step into a plane that has a 5% crash probability ? - Would you buy a drug that has a 5% chance of severe side effects ?\nWhat is the difference between these two methods (bootstrap, permutation) ?\nTesting the hypothesis that two samples have the same distribution may be done with a bootstrap test, but a permutation test is preferred because it is more accurate (exact, in fact). But a permutation test is not as versatile as the bootstrap.\nWe often want to test the hypothesis that population A and population B have the same mean, but not necessarily the same distribution. This is difficult with a permutation test as it assumes exchangeability.\nWe will get back to this topic!\nMore info..",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "SS_03-Intro_Testing.html#permutation-2-sample-test",
    "href": "SS_03-Intro_Testing.html#permutation-2-sample-test",
    "title": "3¬† AB Testing",
    "section": "",
    "text": "Write down the Null hypothesis and test statistic\nWrite code that generates permutation samples from two data sets\nGenerate many permutation replicates for the relevant Titanic subset\nCompute a p-value",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "SS_03-Intro_Testing.html#sample-t-test",
    "href": "SS_03-Intro_Testing.html#sample-t-test",
    "title": "3¬† AB Testing",
    "section": "2-sample t test",
    "text": "2-sample t test\nOf course there is an equivalent fully parametric 2-sample test, the t-test.\nWe first read in the The National Survey of Family Growth data from the think stats book.\nLook at section 1.7 for a description of the variables.\n\n#preg = pd.read_hdf('data/pregNSFG.h5', 'df')\n#preg = pd.read_csv('data/pregNSFG.csv.gz', compression='gzip')\nurl = \"https://raw.githubusercontent.com/markusloecher/DataScience-HWR/refs/heads/main/data/pregNSFG.csv.gz\"\npreg = pd.read_csv(url, compression='gzip')\n#only look at live births\nlive = preg[preg.outcome == 1]\n\n#define first babies\nfirsts = live[live.birthord == 1]\n\n#and all others\nothers = live[live.birthord != 1]\npreg\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ncaseid\npregordr\nhowpreg_n\nhowpreg_p\nmoscurrp\nnowprgdk\npregend1\npregend2\nnbrnaliv\n...\nlaborfor_i\nreligion_i\nmetro_i\nbasewgt\nadj_mod_basewgt\nfinalwgt\nsecu_p\nsest\ncmintvw\ntotalwgt_lb\n\n\n\n\n0\n0\n1\n1\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n3410.389399\n3869.349602\n6448.271112\n2\n9\nNaN\n8.8125\n\n\n1\n1\n1\n2\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n3410.389399\n3869.349602\n6448.271112\n2\n9\nNaN\n7.8750\n\n\n2\n2\n2\n1\nNaN\nNaN\nNaN\nNaN\n5.0\nNaN\n3.0\n...\n0\n0\n0\n7226.301740\n8567.549110\n12999.542264\n2\n12\nNaN\n9.1250\n\n\n3\n3\n2\n2\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n7226.301740\n8567.549110\n12999.542264\n2\n12\nNaN\n7.0000\n\n\n4\n4\n2\n3\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n7226.301740\n8567.549110\n12999.542264\n2\n12\nNaN\n6.1875\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13588\n13588\n12571\n1\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\n6.1875\n\n\n13589\n13589\n12571\n2\nNaN\nNaN\nNaN\nNaN\n3.0\nNaN\nNaN\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\nNaN\n\n\n13590\n13590\n12571\n3\nNaN\nNaN\nNaN\nNaN\n3.0\nNaN\nNaN\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\nNaN\n\n\n13591\n13591\n12571\n4\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\n7.5000\n\n\n13592\n13592\n12571\n5\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\n7.5000\n\n\n\n\n13593 rows √ó 245 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ntRes = stats.ttest_ind(firsts.prglngth.values, others.prglngth.values)\np = pd.Series([tRes.pvalue,tRes.statistic], index = ['p-value:', 'test statistic:'])\np\n\n\n\n\n\n\n\n\n0\n\n\n\n\np-value:\n0.167554\n\n\ntest statistic:\n1.380215\n\n\n\n\ndtype: float64\n\n\n\n#ttest_ind often underestimates p for unequal variances:\ntRes = stats.ttest_ind(firsts.prglngth.values, others.prglngth.values, equal_var = False)\np = pd.Series([tRes.pvalue,tRes.statistic], index = ['p-value:', 'test statistic:'])\np\n\n\n\n\n\n\n\n\n0\n\n\n\n\np-value:\n0.168528\n\n\ntest statistic:\n1.377059\n\n\n\n\ndtype: float64\n\n\n\nRun a permutation test instead\nExtra credit: Can you reproduce the first p-value from the test statistic ?\n\n\nfrom scipy.stats import binom\n\n\n\nA/B Testing\n\nPerform a permutation test on the DataCamp example:\n\n\n\n\nimage.png\n\n\n\nclickthroughA  = [True] * 45 + [False]* 455\nclickthroughB\n\n[True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False]",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "SS_04-Coin-Toss-Startup.html",
    "href": "SS_04-Coin-Toss-Startup.html",
    "title": "4¬† p-Value Hacking",
    "section": "",
    "text": "Coin Toss Startup - ‚ÄúProfit or Perish‚Äù\nCoin Toss Startup is a decision experiment about risk, incentives, and ‚Äúp-hacking for profit.‚Äù\nBelow is a complete, ready-to-run Python game (works in Google Colab, Jupyter, or VS Code). The scientific message is: early stopping inflates false positives\nIn addition we can frame it as a business decision: when to ‚Äúgo public‚Äù with results.",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>p-Value Hacking</span>"
    ]
  },
  {
    "objectID": "SS_04-Coin-Toss-Startup.html#coin-toss-startup-profit-or-perish",
    "href": "SS_04-Coin-Toss-Startup.html#coin-toss-startup-profit-or-perish",
    "title": "4¬† p-Value Hacking",
    "section": "‚ÄúCoin Toss Startup ‚Äî Profit or Perish‚Äù",
    "text": "‚ÄúCoin Toss Startup ‚Äî Profit or Perish‚Äù\n\nLearning goals\nStudents experience how:\n\nIncentives + noise ‚Üí bad inference\nPeeking too often ‚Üí false success\nStatistical rigor costs short-term profit\n\n\n\n\nModel\n\nEach ‚Äústartup‚Äù flips a coin (true win prob =p_true).\nEach flip costs 1 unit (R&D cost).\nIf they ‚Äúfind significance‚Äù (p &lt; Œ±), they may cash out.\nIf they stop early and it‚Äôs a false positive ‚Üí funders lose everything.\nIf the effect is real ‚Üí reward = base √ó (1 / flip_count) (early discovery = bigger payoff).\n\n\n\n\n\nWhat happens in class\n\n\n\n\n\n\n\nParameter\nBehavior\n\n\n\n\np_true = 0.5\nMost startups lose money, but some ‚Äúget lucky‚Äù ‚Üí false discovery profits\n\n\np_true = 0.55\nTrue effects can be profitable but need more patience\n\n\nHigh peek_every (frequent peeking)\nHigher ‚Äúsuccess‚Äù rate but misleading profits\n\n\nHigh cost_per_flip\nIncreases pressure to stop early\n\n\nLower Œ±\nReduces false profits but increases missed real effects\n\n\n\n\n\n\nüí¨ Discussion prompts\n\nHow often do ‚Äúsuccessful‚Äù startups actually have a real edge?\nWhat happens when cost pressure (or publish-or-perish) grows?\nHow could funders change incentives to reward replication instead of significance?\n\n\nimport numpy as np\n\nnp.sum(np.random.rand(5000) &lt; 0.25)/5000\n\nnp.float64(0.2462)\n\n\n\n5 % 2\n#you can define steps !!\n\nfor x in range(20):\n  print(x % 10 == 0)\n\n\n### üêç Python Code\n\n# If you're in Google Colab, first run this setup cell:\n!pip install ipywidgets --quiet\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider, IntSlider\n\nnp.random.seed(42)\n\ndef simulate_startup_with_payoff(p_true=0.5, n_max=200, alpha=0.05,min_tosses=10,\n                                 peek_every=5, cost_per_flip=1, base_reward=100):\n    \"\"\"\n    Simulate one startup that may stop early when p &lt; alpha.\n    Returns profit, detection_time, and p-value trajectory.\n    \"\"\"\n    heads = 0\n    p_values = []\n    detection_time = None\n    profit = -cost_per_flip * n_max  # assume they run full unless success\n    reward = 0\n\n    for n in range(1, n_max + 1):\n        #this is a coin flip !!! with prob pTrue !\n        if np.random.rand() &lt; p_true:\n            heads += 1\n        #do sth, every peek time !\n        if n % peek_every == 0:\n        #if n &gt;= min_tosses and (n % peek_every == 0):\n            p_hat = heads / n # observed proportion\n            z = (p_hat - 0.5) / np.sqrt(0.25 / n) # test statistic !! z-score!\n            p_val = 1 - stats.norm.cdf(z) # pvalue\n            p_values.append((n, p_val))\n\n            if p_val &lt; alpha: #\n                detection_time = n\n                # compute profit: reward minus accumulated cost\n                reward = base_reward / (n / 10)  # earlier = larger reward\n                profit = reward - cost_per_flip * n\n                break # terminates the for loop !!\n\n    return profit, detection_time, p_values\n\n\ndef plot_startup_with_payoff(p_true=0.5, n_max=200, alpha=0.05,\n                             peek_every=5, n_reps=500, cost_per_flip=1, base_reward=100):\n    \"\"\"\n    Simulate many startups and plot typical trajectory + profit distribution.\n    \"\"\"\n    profits, detections = [], []\n\n    for _ in range(n_reps):\n        prof, det, _ = simulate_startup_with_payoff(p_true, n_max, alpha,\n                                                    peek_every, cost_per_flip, base_reward)\n        profits.append(prof)\n        detections.append(det is not None)\n\n    avg_profit = np.mean(profits)\n    success_rate = np.mean(detections)\n\n    # One example trajectory\n    prof_ex, det_ex, pv_ex = simulate_startup_with_payoff(p_true, n_max, alpha,\n                                                          peek_every, cost_per_flip, base_reward)\n    n_vals, pvals = zip(*pv_ex)\n    plt.figure(figsize=(11,4))\n    plt.subplot(1,2,1)\n    plt.plot(n_vals, pvals, 'o-', label=\"Sequential p-values\")\n    plt.axhline(alpha, color='gray', linestyle='--', label=f'Œ±={alpha}')\n    if det_ex:\n        plt.axvline(det_ex, color='green', linestyle='--', label=f'Stopped at {det_ex}')\n    plt.xlabel(\"Number of tosses\")\n    plt.ylabel(\"p-value\")\n    plt.title(\"Typical startup trajectory\")\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.hist(profits, bins=20)\n    plt.axvline(0, color='red', linestyle='--')\n    plt.title(f\"Profit distribution\\nMean={avg_profit:.1f}, Success rate={success_rate:.2f}\")\n    plt.xlabel(\"Profit\")\n    plt.ylabel(\"Frequency\")\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_startup_with_payoff(p_true=0.5, n_reps=1)\n\n\n\n\n\n\n\n\n\n# Interactive version\ninteract(plot_startup_with_payoff,\n         p_true=FloatSlider(value=0.5, min=0.5, max=0.7, step=0.01, description=\"True p\"),\n         n_max=IntSlider(value=200, min=50, max=1000, step=50, description=\"Max flips\"),\n         alpha=FloatSlider(value=0.05, min=0.001, max=0.2, step=0.005, description=\"Œ±\"),\n         peek_every=IntSlider(value=5, min=1, max=20, step=1, description=\"Peek every\"),\n         n_reps=IntSlider(value=500, min=50, max=2000, step=50, description=\"Startups\"),\n         cost_per_flip=FloatSlider(value=1, min=0.1, max=5, step=0.1, description=\"Cost/flip\"),\n         base_reward=IntSlider(value=100, min=50, max=500, step=10, description=\"Base reward\"));",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>p-Value Hacking</span>"
    ]
  },
  {
    "objectID": "SS_04-Coin-Toss-Startup.html#gamified-version",
    "href": "SS_04-Coin-Toss-Startup.html#gamified-version",
    "title": "4¬† p-Value Hacking",
    "section": "Gamified version",
    "text": "Gamified version\n\n# ================================\n# ü™ô COIN TOSS STARTUP GAME\n# ================================\n# Copy this cell into Google Colab and run all\n# Author: adapted for teaching from ChatGPT (GPT-5)\n# -------------------------------\n\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(12345)\n\n# ========== SIMULATION CORE ==========\n\ndef simulate_startup_with_payoff(p_true=0.5, n_max=200, alpha=0.05,\n                                 peek_every=5, min_tosses=10,\n                                 cost_per_flip=1.0, base_reward=300.0,\n                                 reward_decay_lambda=0.03, rng=None):\n    \"\"\"Simulate one startup run with a peeking testing rule.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    heads = 0\n    detection_time = None\n\n    for n in range(1, n_max + 1):\n        if rng.random() &lt; p_true:\n            heads += 1\n\n        if n &gt;= min_tosses and (n % peek_every == 0):\n            p_hat = heads / n\n            denom = np.sqrt(max(1e-9, 0.25 / n))\n            z = (p_hat - 0.5) / denom\n            p_val = 1 - stats.norm.cdf(z)\n            if p_val &lt; alpha:\n                detection_time = n\n                break\n\n    total_cost = cost_per_flip * (detection_time if detection_time is not None else n_max)\n\n    if detection_time is not None:\n        if p_true &gt; 0.5:\n            reward = base_reward * np.exp(-reward_decay_lambda * detection_time)\n            profit = reward - total_cost\n            false_positive = False\n            success = True\n        else:\n            profit = -total_cost\n            false_positive = True\n            success = False\n    else:\n        profit = -cost_per_flip * n_max if p_true &gt; 0.5 else 0.0\n        false_positive = False\n        success = False\n\n    return {'profit': profit,\n            'detection_time': detection_time,\n            'success': success,\n            'false_positive': false_positive}\n\n\ndef evaluate_team_strategy(strategy, p_true=0.5, n_reps=1000, n_max=200, rng=None):\n    \"\"\"Monte Carlo evaluation of one team.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    profits, successes, false_pos, det_times = [], 0, 0, []\n\n    for _ in range(n_reps):\n        res = simulate_startup_with_payoff(p_true=p_true, n_max=n_max, rng=rng, **strategy)\n        profits.append(res['profit'])\n        if res['success']: successes += 1; det_times.append(res['detection_time'])\n        if res['false_positive']: false_pos += 1\n\n    return {\n        'mean_profit': np.mean(profits),\n        'median_profit': np.median(profits),\n        'success_rate': successes / n_reps,\n        'false_positive_rate': false_pos / n_reps,\n        'mean_detection_time': np.mean(det_times) if det_times else np.nan\n    }\n\n\ndef run_tournament(teams, p_true=0.5, n_reps=1000, n_max=200, rng_seed=123):\n    \"\"\"Evaluate multiple teams and return leaderboard.\"\"\"\n    rng = np.random.default_rng(rng_seed)\n    rows = []\n    for name, strat in teams.items():\n        stats_ = evaluate_team_strategy(strat, p_true=p_true, n_reps=n_reps, n_max=n_max, rng=rng)\n        rows.append({'team': name, **stats_, **strat})\n    df = pd.DataFrame(rows).sort_values('mean_profit', ascending=False).reset_index(drop=True)\n    return df\n\n# ========== DEFINE TEAMS HERE ==========\n\n\n\nteams = {\n    # Teams pick their peek_every, alpha, and min_tosses\n    'team1': {'peek_every':, 'alpha':, 'min_tosses':30,\n                     'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'team2': {'peek_every':, 'alpha':, 'min_tosses':5,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'team3': {'peek_every':1, 'alpha':0.05, 'min_tosses':30,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'team4': {'peek_every':1, 'alpha':0.05, 'min_tosses':30,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'team5': {'peek_every':1, 'alpha':0.05, 'min_tosses':30,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n}\n\n\nteams = {\n    # Teams pick their peek_every, alpha, and min_tosses\n    'Conservative': {'peek_every':20, 'alpha':0.01, 'min_tosses':30,\n                     'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'Aggressive': {'peek_every':1, 'alpha':0.05, 'min_tosses':5,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'Balanced': {'peek_every':5, 'alpha':0.02, 'min_tosses':10,\n                 'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'SlowButSure': {'peek_every':10, 'alpha':0.01, 'min_tosses':20,\n                    'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n}\n\nteams = {\n    'Conservative': {'peek_every':20, 'alpha':0.01, 'min_tosses':30,\n                     'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'Aggressive': {'peek_every':1, 'alpha':0.05, 'min_tosses':5,\n                   'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'Balanced': {'peek_every':5, 'alpha':0.02, 'min_tosses':10,\n                 'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'SlowButSure': {'peek_every':10, 'alpha':0.01, 'min_tosses':20,\n                    'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n}\n\n\n# ========== RUN TOURNAMENTS ==========\n\nn_reps = 1500\nn_max = 200\n\nprint(\"üîπ Null world (p_true = 0.50)\")\nleader_null = run_tournament(teams, p_true=0.5, n_reps=n_reps, n_max=n_max)\ndisplay(leader_null[['team', 'mean_profit', 'false_positive_rate', 'alpha', 'peek_every']])\n\nprint(\"\\nüîπ Weak effect world (p_true = 0.55)\")\nleader_eff = run_tournament(teams, p_true=0.55, n_reps=n_reps, n_max=n_max)\ndisplay(leader_eff[['team', 'mean_profit', 'success_rate', 'mean_detection_time', 'alpha', 'peek_every']])\n\nprint(\"\\nüîπ Stron effect world (p_true = 0.65)\")\nleader_strong = run_tournament(teams, p_true=0.65, n_reps=n_reps, n_max=n_max)\ndisplay(leader_strong[['team', 'mean_profit', 'success_rate', 'mean_detection_time', 'alpha', 'peek_every']])\n\n# ========== VISUALIZATION ==========\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.bar(leader_eff['team'], leader_eff['mean_profit'], color='skyblue', edgecolor='black')\nax.axhline(0, color='k', lw=1)\nax.set_ylabel(\"Mean Profit (p_true=0.55)\")\nax.set_title(\"üèÜ Team Performance ‚Äî Coin Toss Startup Game\")\nplt.show()\n\n\nleader_null\n\nprint(\"\\nüîπ Strong effect world (p_true = 0.65)\")\nleader_strong = run_tournament(teams, p_true=0.65, n_reps=n_reps, n_max=n_max)\ndisplay(leader_strong[['team', 'mean_profit', 'success_rate', 'mean_detection_time', 'alpha', 'peek_every']])\n\n\nüîπ Stron effect world (p_true = 0.65)\n\n\n\n  \n    \n\n\n\n\n\n\nteam\nmean_profit\nsuccess_rate\nmean_detection_time\nalpha\npeek_every\n\n\n\n\n0\nAggressive\n47.001107\n1.000000\n28.940667\n0.05\n1\n\n\n1\nBalanced\n5.981218\n0.996000\n48.514056\n0.02\n5\n\n\n2\nSlowButSure\n-29.439449\n0.987333\n67.434166\n0.01\n10\n\n\n3\nConservative\n-47.257600\n0.985333\n77.280108\n0.01\n20\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\nteams = {\n    'Conservative': {'peek_every':20, 'alpha':0.01, 'min_tosses':30,\n                     'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'Aggressive': {'peek_every':1, 'alpha':0.05, 'min_tosses':5,\n                   'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'Balanced': {'peek_every':5, 'alpha':0.02, 'min_tosses':10,\n                 'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'SlowButSure': {'peek_every':10, 'alpha':0.01, 'min_tosses':20,\n                    'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n}\n\ndone with p= 0.5\n\n\n\nteams = {\n    # Teams pick their peek_every, alpha, and min_tosses\n    'team1': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03},\n    'team2': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03},\n    'team3': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03},\n    'team4': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03},\n    'team5': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03}\n\n}\n\n\n# ================================\n# üîπ MULTI-WORLD TOURNAMENT LEADERBOARD\n# ================================\n\np_values = [0.50, 0.55, 0.60]\nresults = []\n\nfor p in p_values:\n    df = run_tournament(teams, p_true=p, n_reps=500, n_max=200)\n    print(\"done with p=\",p)\n    df['p_true'] = p\n    results.append(df)\n\n# Combine\ndf_all = pd.concat(results, ignore_index=True)\n\n# --- Aggregate performance across worlds ---\nsummary = (\n    df_all.groupby('team')\n    .agg(mean_profit_overall=('mean_profit', 'mean'),\n         min_profit=('mean_profit', 'min'),\n         max_profit=('mean_profit', 'max'),\n         avg_success_rate=('success_rate', 'mean'),\n         avg_false_pos=('false_positive_rate', 'mean'))\n    .sort_values('mean_profit_overall', ascending=False)\n    .reset_index()\n)\n\ndisplay(summary)\n\n# --- Plot ---\nfig, ax = plt.subplots(figsize=(8,4))\nfor team in df_all['team'].unique():\n    subset = df_all[df_all['team'] == team]\n    ax.plot(subset['p_true'], subset['mean_profit'], marker='o', label=team)\n\nax.set_xlabel(\"True success probability (p_true)\")\nax.set_ylabel(\"Mean Profit\")\nax.set_title(\"üìà Team Robustness Across Worlds\")\nax.axhline(0, color='k', lw=1)\nax.legend()\nplt.show()\n\ndone with p= 0.5\ndone with p= 0.55\ndone with p= 0.6\n\n\n\n  \n    \n\n\n\n\n\n\nteam\nmean_profit_overall\nmin_profit\nmax_profit\navg_success_rate\navg_false_pos\n\n\n\n\n0\nteam1\n34.214896\n-27.520000\n101.182283\n0.628667\n0.179333\n\n\n1\nteam5\n20.826768\n-30.600000\n76.154211\n0.620000\n0.175333\n\n\n2\nteam2\n11.599734\n-20.272166\n70.887367\n0.560667\n0.111333\n\n\n3\nteam4\n-20.746398\n-66.027067\n12.887874\n0.502000\n0.063333\n\n\n4\nteam3\n-38.008873\n-105.636113\n1.269494\n0.468000\n0.052667\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>p-Value Hacking</span>"
    ]
  },
  {
    "objectID": "DS_01-DataScience_with_Penguins.html",
    "href": "DS_01-DataScience_with_Penguins.html",
    "title": "5¬† Data Science with Penguins",
    "section": "",
    "text": "6 Palmer Penguins",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "DS_01-DataScience_with_Penguins.html#look-at-the-data",
    "href": "DS_01-DataScience_with_Penguins.html#look-at-the-data",
    "title": "5¬† Data Science with Penguins",
    "section": "Look at the data",
    "text": "Look at the data\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npenguins = sns.load_dataset(\"penguins\")\npenguins\n\n\n  \n    \n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nGentoo\nBiscoe\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nGentoo\nBiscoe\n46.8\n14.3\n215.0\n4850.0\nFemale\n\n\n341\nGentoo\nBiscoe\n50.4\n15.7\n222.0\n5750.0\nMale\n\n\n342\nGentoo\nBiscoe\n45.2\n14.8\n212.0\n5200.0\nFemale\n\n\n343\nGentoo\nBiscoe\n49.9\n16.1\n213.0\n5400.0\nMale\n\n\n\n\n344 rows √ó 7 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n#√§map:\n\npDict = {\"Adelie\": 1,\n         \"Chintoo\": 2}",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "DS_01-DataScience_with_Penguins.html#grouping",
    "href": "DS_01-DataScience_with_Penguins.html#grouping",
    "title": "5¬† Data Science with Penguins",
    "section": "Grouping",
    "text": "Grouping\nThe seaborn library offers simple grouping in their plots via the hue argument:",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "DS_01-DataScience_with_Penguins.html#box-plots",
    "href": "DS_01-DataScience_with_Penguins.html#box-plots",
    "title": "5¬† Data Science with Penguins",
    "section": "Box plots",
    "text": "Box plots\nSide-by-side comparisons are much easier with box plots:\n\n# Box plot of body mass by species and sex\nplt.figure(figsize=(5, 2.5))\nsns.boxplot(data = penguins, y =\"body_mass_g\", x=\"species\", hue = \"sex\")\nplt.title('Body Mass by Species ');\nplt.show();",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "DS_01-DataScience_with_Penguins.html#scatter-plots",
    "href": "DS_01-DataScience_with_Penguins.html#scatter-plots",
    "title": "5¬† Data Science with Penguins",
    "section": "Scatter Plots",
    "text": "Scatter Plots\n\n# @title bill_length_mm vs bill_depth_mm\nplt.figure(figsize=(4, 2));\nfrom matplotlib import pyplot as plt\npenguins.plot(x = \"bill_depth_mm\", y = \"bill_length_mm\", kind = \"scatter\");\nplt.figure(figsize=(4, 2));\n# there are clearly MANY scatter plots that we can look at !\n# It would require a lot of code to create that many\n# 4*3/2\n#correlation coefficient quantifies the linear dependence\n# it is in the range [-1,1]\nnp.corrcoef(penguins.bill_depth_mm, penguins.bill_length_mm)\n#What is nan ?\n\narray([[nan, nan],\n       [nan, nan]])\n\n\n&lt;Figure size 400x200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 400x200 with 0 Axes&gt;",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "DS_01-DataScience_with_Penguins.html#categorical-variables",
    "href": "DS_01-DataScience_with_Penguins.html#categorical-variables",
    "title": "5¬† Data Science with Penguins",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npenguins = sns.load_dataset(\"penguins\")\n\n\nlm2a = smf.ols('flipper_length_mm ~ species -1', penguins).fit()\nlm2a.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nspecies[Adelie]\n189.9536\n0.540\n351.454\n0.000\n188.891\n191.017\n\n\nspecies[Chinstrap]\n195.8235\n0.805\n243.137\n0.000\n194.239\n197.408\n\n\nspecies[Gentoo]\n217.1870\n0.599\n362.676\n0.000\n216.009\n218.365\n\n\n\n\n\nQuizzes\n\nimport numpy as np\nfrom numpy.random import default_rng\n\nrng = default_rng(70)\n#Generate 500 random integers between 80 and 90 (both boundaries are inclusive!)\nx = rng.integers(80,90, 500)\nx\nnp.mean(x)\nx.mean()\n#every \"fourth value\"\n#print(x[0:10])\n#print(x[:10:4])\nnp.sum(x[::4])\nx &gt; 85\nnp.sum(x &gt; 85)\nnp.sum(x == 81)\nx = np.arange(1,13)\nx\nx.reshape(4,3)\n\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\n\n\n\nimport numpy as np\n\nurl = \"https://raw.githubusercontent.com/markusloecher/DataScience2018/master/data/diabetes.csv\"\ndiabetes = np.loadtxt(url, delimiter=\",\", skiprows=1)\nendRow=444\ndiabetes = diabetes[0:endRow,:]\n\n\nnp.mean(diabetes[:,0])\nmax(diabetes[:,1] ) - min(diabetes[:,1] )\ndiabetes[-1,0]\nnp.sum(diabetes[:,6]&gt;0.3825)\nnp.unique(diabetes[:,0], return_counts=True)\n\n(array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n        13., 14., 15., 17.]),\n array([62, 77, 57, 42, 42, 45, 22, 31, 21, 15, 11,  4,  6,  6,  1,  1,  1]))",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html",
    "href": "DS_02-sklearn_pipelines.html",
    "title": "6¬† Sklearn pipelines",
    "section": "",
    "text": "Learning Objectives\nAfter completing this lesson, you will: - understand the need for preprocessing data in general - recognize pitfalls and ‚Äúdanger‚Äù in the process - appreciate the structured and systematic approach offered by pipelines",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#mlr-on-housing-data",
    "href": "DS_02-sklearn_pipelines.html#mlr-on-housing-data",
    "title": "6¬† Sklearn pipelines",
    "section": "1. MLR on housing data",
    "text": "1. MLR on housing data\nWe will use the California Housing Dataset from sklearn.datasets.\nIt contains data on housing prices and district-level demographics in California.\n\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\nimport numpy as np\n\ndata = fetch_california_housing(as_frame=True)\ndf = data.frame\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\nThe variable MedHouseVal is the median house value in $100,000s.\nWe will predict it using a few numeric predictors.\n\nSelect a Subset of Variables\nUse only the following features:\n\n\n\nFeature\nDescription\n\n\n\n\nMedInc\nMedian income in block group\n\n\nAveRooms\nAverage number of rooms per household\n\n\nAveBedrms\nAverage number of bedrooms per household\n\n\nPopulation\nBlock group population\n\n\nHouseAge\nMedian age of houses in the block group\n\n\n\n\ncols = ['MedHouseVal', 'MedInc', 'AveRooms', 'AveBedrms', 'Population', 'HouseAge']\ndf_sub = df[cols]\ndf_sub.head()\n\n\n\n\n\n\n\n\nMedHouseVal\nMedInc\nAveRooms\nAveBedrms\nPopulation\nHouseAge\n\n\n\n\n0\n4.526\n8.3252\n6.984127\n1.023810\n322.0\n41.0\n\n\n1\n3.585\n8.3014\n6.238137\n0.971880\n2401.0\n21.0\n\n\n2\n3.521\n7.2574\n8.288136\n1.073446\n496.0\n52.0\n\n\n3\n3.413\n5.6431\n5.817352\n1.073059\n558.0\n52.0\n\n\n4\n3.422\n3.8462\n6.281853\n1.081081\n565.0\n52.0\n\n\n\n\n\n\n\n\n\nMultiple Regression using statsmodels\nUse statsmodels.api.OLS to fit a multiple regression model.\n\nimport statsmodels.api as sm\n\nX = df_sub[['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'HouseAge']]\ny = df_sub['MedHouseVal']\n\nX = sm.add_constant(X)\nmodel_sm = sm.OLS(y, X).fit()\nprint(model_sm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            MedHouseVal   R-squared:                       0.538\nModel:                            OLS   Adj. R-squared:                  0.538\nMethod:                 Least Squares   F-statistic:                     4801.\nDate:                Tue, 25 Nov 2025   Prob (F-statistic):               0.00\nTime:                        17:02:38   Log-Likelihood:                -24278.\nNo. Observations:               20640   AIC:                         4.857e+04\nDf Residuals:                   20634   BIC:                         4.862e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.4407      0.028    -15.945      0.000      -0.495      -0.387\nMedInc         0.5360      0.004    129.735      0.000       0.528       0.544\nAveRooms      -0.2112      0.006    -35.074      0.000      -0.223      -0.199\nAveBedrms      0.9908      0.030     33.503      0.000       0.933       1.049\nPopulation  1.848e-05   5.09e-06      3.628      0.000     8.5e-06    2.85e-05\nHouseAge       0.0163      0.000     35.182      0.000       0.015       0.017\n==============================================================================\nOmnibus:                     4414.212   Durbin-Watson:                   0.938\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            16609.838\nSkew:                           1.035   Prob(JB):                         0.00\nKurtosis:                       6.876   Cond. No.                     1.16e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.16e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#predictions",
    "href": "DS_02-sklearn_pipelines.html#predictions",
    "title": "6¬† Sklearn pipelines",
    "section": "Predictions",
    "text": "Predictions\n\n# New data equal to an average house !\nX_new = X.mean()\nprint(pd.DataFrame(X_new))\n\n# Predict\npred = model_sm.predict(X_new)\nprint(pred)\nbeta = model_sm.params\nprint(X_new @ beta)\n\n                      0\nconst          1.000000\nMedInc         3.870671\nAveRooms       5.429000\nAveBedrms      1.096675\nPopulation  1425.476744\nHouseAge      28.639486\nNone    2.068558\ndtype: float64\n2.0685581690891244\n\n\n\ndef print_dot_product(beta, x, names, intercept_name=\"Intercept\"):\n    terms = []\n    for b, v, n in zip(beta, x, names):\n        terms.append(f\"{b:.3f}*{v:.3f}\")\n    equation = \" + \".join(terms)\n    print(equation)\n\nprint_dot_product(beta, X_new, names = beta.index.tolist())\n\n-0.441*1.000 + 0.536*3.871 + -0.211*5.429 + 0.991*1.097 + 0.000*1425.477 + 0.016*28.639",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#scikit-learn",
    "href": "DS_02-sklearn_pipelines.html#scikit-learn",
    "title": "6¬† Sklearn pipelines",
    "section": "scikit-learn",
    "text": "scikit-learn\nFrom now on we will switch almost entirely to the sklearn library!\nLet us fit the same model using LinearRegression.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nX = df_sub[['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'HouseAge']]\ny = df_sub['MedHouseVal']\n\nmodel_sk = LinearRegression()\nmodel_sk.fit(X, y)\n\nprint(\"Intercept:\", model_sk.intercept_)\nprint(\"Coefficients:\", model_sk.coef_)\n\nIntercept: -0.440721743746872\nCoefficients: [ 5.36014757e-01 -2.11185756e-01  9.90813314e-01  1.84789639e-05\n  1.63455751e-02]\n\n\n\n# Predictions\nX_new_sk = X.mean().to_frame().T ## convert Series ‚Üí DataFrame with one row\n#print(X_new_sk)\npred = model_sk.predict(X_new_sk)\nprint(pred)\n\n[2.06855817]\n\n\n\n# Compute R¬≤\ny_pred = model_sk.predict(X)\nr2_score(y, y_pred)\n\n0.5377839208402416",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#data-scaling",
    "href": "DS_02-sklearn_pipelines.html#data-scaling",
    "title": "6¬† Sklearn pipelines",
    "section": "Data Scaling",
    "text": "Data Scaling\nMany ML methods need data to be scaled\n\ndf_sub.describe()\n\n\n\n\n\n\n\n\nMedHouseVal\nMedInc\nAveRooms\nAveBedrms\nPopulation\nHouseAge\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n2.068558\n3.870671\n5.429000\n1.096675\n1425.476744\n28.639486\n\n\nstd\n1.153956\n1.899822\n2.474173\n0.473911\n1132.462122\n12.585558\n\n\nmin\n0.149990\n0.499900\n0.846154\n0.333333\n3.000000\n1.000000\n\n\n25%\n1.196000\n2.563400\n4.440716\n1.006079\n787.000000\n18.000000\n\n\n50%\n1.797000\n3.534800\n5.229129\n1.048780\n1166.000000\n29.000000\n\n\n75%\n2.647250\n4.743250\n6.052381\n1.099526\n1725.000000\n37.000000\n\n\nmax\n5.000010\n15.000100\n141.909091\n34.066667\n35682.000000\n52.000000\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n#Manual:\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit + transform\nX_scaled = scaler.fit_transform(X)\n\n# Convert back to a DataFrame with the same column names\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n\nX_scaled.describe()\n\n\n\n\n\n\n\n\nMedInc\nAveRooms\nAveBedrms\nPopulation\nHouseAge\n\n\n\n\ncount\n2.064000e+04\n2.064000e+04\n2.064000e+04\n2.064000e+04\n2.064000e+04\n\n\nmean\n6.609700e-17\n6.609700e-17\n-1.060306e-16\n-1.101617e-17\n5.508083e-18\n\n\nstd\n1.000024e+00\n1.000024e+00\n1.000024e+00\n1.000024e+00\n1.000024e+00\n\n\nmin\n-1.774299e+00\n-1.852319e+00\n-1.610768e+00\n-1.256123e+00\n-2.196180e+00\n\n\n25%\n-6.881186e-01\n-3.994496e-01\n-1.911716e-01\n-5.638089e-01\n-8.453931e-01\n\n\n50%\n-1.767951e-01\n-8.078489e-02\n-1.010650e-01\n-2.291318e-01\n2.864572e-02\n\n\n75%\n4.593063e-01\n2.519615e-01\n6.015869e-03\n2.644949e-01\n6.643103e-01\n\n\nmax\n5.858286e+00\n5.516324e+01\n6.957171e+01\n3.025033e+01\n1.856182e+00\n\n\n\n\n\n\n\n\nmodel_sk = LinearRegression()\nmodel_sk.fit(X_scaled, y)\n\nprint(\"Intercept:\", model_sk.intercept_)\nprint(\"Coefficients:\", model_sk.coef_)\n\nIntercept: 2.068558169089147\nCoefficients: [ 1.01830781 -0.52249747  0.46954581  0.02092622  0.20571319]",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#train-test-splits",
    "href": "DS_02-sklearn_pipelines.html#train-test-splits",
    "title": "6¬† Sklearn pipelines",
    "section": "Train Test Splits",
    "text": "Train Test Splits\nIn the absence of ‚Äúnew‚Äù data we can simulate the process by splitting the data set ourselves and calling one part ‚Äútraining‚Äù and the other ‚Äútest‚Äù data.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nprint(X.shape)\nprint(X_train.shape)\nprint(X_test.shape)\n\n(20640, 5)\n(16512, 5)\n(4128, 5)",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#putting-it-all-together",
    "href": "DS_02-sklearn_pipelines.html#putting-it-all-together",
    "title": "6¬† Sklearn pipelines",
    "section": "Putting it ‚Äúall‚Äù together",
    "text": "Putting it ‚Äúall‚Äù together\nWe should apply most preprocessing steps to both training and test data. That is easier said than done, because\n\nwe need to apply the identical algorithm to both parts, and\nwe need to avoid data leakage!\n\nImagine the overhead in bookkeeping of manually have to store all the parameters from scaling and e.g.¬†mean/median imputations, etc‚Ä¶\nThat is where pipelines come in and make life much easier.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    ('model', LinearRegression())\n])\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('scale', StandardScaler()), ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scale', StandardScaler()), ('model', LinearRegression())])StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\n\nScore the Pipeline\nThe code below looks very simple and innocent but just pause and think what all is going on here !\n\ntest_score = pipe.score(X_test, y_test)\ntest_score\n\n0.5089947802907753",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#missing-values",
    "href": "DS_02-sklearn_pipelines.html#missing-values",
    "title": "6¬† Sklearn pipelines",
    "section": "Missing values",
    "text": "Missing values\n\n# Set specific rows to missing\n\n#Example: First 50 rows:\n\ndf_sub.loc[:49, \"HouseAge\"] = np.nan\n\nX = df_sub[['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'HouseAge']]\ny = df_sub['MedHouseVal']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nmodel_sk = LinearRegression()\n#model_sk.fit(X_train, y_train)",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#full-pipeline",
    "href": "DS_02-sklearn_pipelines.html#full-pipeline",
    "title": "6¬† Sklearn pipelines",
    "section": "Full Pipeline",
    "text": "Full Pipeline\nIncluding an imputer!\n\nfrom sklearn.impute import SimpleImputer\n\npipe = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"scale\", StandardScaler()),\n    (\"model\", LinearRegression())\n])\n#up to here it's only been setup work\n\npipe.fit(X_train, y_train)#all the action is here !!\n\nPipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('scale', StandardScaler()), ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('scale', StandardScaler()), ('model', LinearRegression())])SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\n\ntest_score = pipe.score(X_test, y_test)\ntest_score\n\n0.5089722962899934\n\n\n\ntrain_score = pipe.score(X_train, y_train)\ntrain_score\n\n0.5436563358368143",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#onehot-encoder",
    "href": "DS_02-sklearn_pipelines.html#onehot-encoder",
    "title": "6¬† Sklearn pipelines",
    "section": "OneHot Encoder",
    "text": "OneHot Encoder\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npenguins = sns.load_dataset(\"penguins\").dropna()\npenguins\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\nGentoo\nBiscoe\n47.2\n13.7\n214.0\n4925.0\nFemale\n\n\n340\nGentoo\nBiscoe\n46.8\n14.3\n215.0\n4850.0\nFemale\n\n\n341\nGentoo\nBiscoe\n50.4\n15.7\n222.0\n5750.0\nMale\n\n\n342\nGentoo\nBiscoe\n45.2\n14.8\n212.0\n5200.0\nFemale\n\n\n343\nGentoo\nBiscoe\n49.9\n16.1\n213.0\n5400.0\nMale\n\n\n\n\n333 rows √ó 7 columns\n\n\n\n\n#train/test split 80/20\n\nfrom sklearn.model_selection import train_test_split\n\n#penguins = penguins.dropna()\n\n#X\nX = penguins.drop(columns=\"body_mass_g\")\n#\ny = penguins.body_mass_g\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nX_train\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nsex\n\n\n\n\n230\nGentoo\nBiscoe\n40.9\n13.7\n214.0\nFemale\n\n\n84\nAdelie\nDream\n37.3\n17.8\n191.0\nFemale\n\n\n303\nGentoo\nBiscoe\n50.0\n15.9\n224.0\nMale\n\n\n22\nAdelie\nBiscoe\n35.9\n19.2\n189.0\nFemale\n\n\n29\nAdelie\nBiscoe\n40.5\n18.9\n180.0\nMale\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n194\nChinstrap\nDream\n50.9\n19.1\n196.0\nMale\n\n\n77\nAdelie\nTorgersen\n37.2\n19.4\n184.0\nMale\n\n\n112\nAdelie\nBiscoe\n39.7\n17.7\n193.0\nFemale\n\n\n277\nGentoo\nBiscoe\n45.5\n15.0\n220.0\nMale\n\n\n108\nAdelie\nBiscoe\n38.1\n17.0\n181.0\nFemale\n\n\n\n\n266 rows √ó 6 columns\n\n\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\ncategorical = X_train.select_dtypes(include=['object']).columns\nnumeric = X_train.select_dtypes(include=['number']).columns\n\ncategorical\nnumeric\n\nIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object')\n\n\n\npreprocessor = ColumnTransformer([\n    ('cat', OneHotEncoder(handle_unknown='ignore', drop=\"first\"), categorical),\n    ('num', StandardScaler(), numeric)\n])\n\npreprocessor\n\nColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore'),\n                                 Index(['species', 'island', 'sex'], dtype='object')),\n                                ('num', StandardScaler(),\n                                 Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore'),\n                                 Index(['species', 'island', 'sex'], dtype='object')),\n                                ('num', StandardScaler(),\n                                 Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])catIndex(['species', 'island', 'sex'], dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore')numIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object')StandardScalerStandardScaler()\n\n\n\npipe = Pipeline([\n    #(\"impute\", SimpleImputer(strategy=\"median\")),\n    ('preprocess', preprocessor)\n    #('model', LinearRegression())\n])\n\n\npipe = Pipeline([\n    #(\"impute\", SimpleImputer(strategy=\"median\")),\n    ('preprocess', preprocessor),\n    ('model', LinearRegression())\n])\n\npipe\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore'),\n                                                  Index(['species', 'island', 'sex'], dtype='object')),\n                                                 ('num', StandardScaler(),\n                                                  Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore'),\n                                                  Index(['species', 'island', 'sex'], dtype='object')),\n                                                 ('num', StandardScaler(),\n                                                  Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])),\n                ('model', LinearRegression())])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore'),\n                                 Index(['species', 'island', 'sex'], dtype='object')),\n                                ('num', StandardScaler(),\n                                 Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])catIndex(['species', 'island', 'sex'], dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore')numIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object')StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\n\n#Let us do the work\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore'),\n                                                  Index(['species', 'island', 'sex'], dtype='object')),\n                                                 ('num', StandardScaler(),\n                                                  Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore'),\n                                                  Index(['species', 'island', 'sex'], dtype='object')),\n                                                 ('num', StandardScaler(),\n                                                  Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])),\n                ('model', LinearRegression())])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore'),\n                                 Index(['species', 'island', 'sex'], dtype='object')),\n                                ('num', StandardScaler(),\n                                 Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])catIndex(['species', 'island', 'sex'], dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore')numIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object')StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\nHow do you judge the qualitry of a regression ?\n\npipe.score(X_test, y_test)\n\n\n0.8961688345769456",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#the-predictors-explain-89-of-the-variation-of-body-mass",
    "href": "DS_02-sklearn_pipelines.html#the-predictors-explain-89-of-the-variation-of-body-mass",
    "title": "6¬† Sklearn pipelines",
    "section": "The predictors explain 89% of the variation of body mass !",
    "text": "The predictors explain 89% of the variation of body mass !\n‚ÄúTransform‚Äù is only sensible for steops UP TO the final model\n\n\npipe = Pipeline([\n    #(\"impute\", SimpleImputer(strategy=\"median\")),\n    ('preprocess', preprocessor),\n    ('model', LinearRegression())\n])\n\nX_trans = pipe[:-1].transform(X_train)\npd.DataFrame(X_trans)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n0\n0.0\n1.0\n0.0\n0.0\n0.0\n-0.593727\n-1.750939\n0.935943\n\n\n1\n0.0\n0.0\n1.0\n0.0\n0.0\n-1.261043\n0.323107\n-0.719956\n\n\n2\n0.0\n1.0\n0.0\n0.0\n1.0\n1.093099\n-0.638036\n1.655899\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.520555\n1.031318\n-0.863947\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n-0.667873\n0.879558\n-1.511908\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n261\n1.0\n0.0\n1.0\n0.0\n1.0\n1.259928\n0.980731\n-0.359978\n\n\n262\n0.0\n0.0\n0.0\n1.0\n1.0\n-1.279579\n1.132491\n-1.223925\n\n\n263\n0.0\n0.0\n0.0\n0.0\n0.0\n-0.816166\n0.272520\n-0.575965\n\n\n264\n0.0\n1.0\n0.0\n0.0\n1.0\n0.258954\n-1.093315\n1.367916\n\n\n265\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.112750\n-0.081585\n-1.439912\n\n\n\n\n266 rows √ó 8 columns\n\n\n\n#Task: fit a linear regression without a pipe\nComment: you still need to one-hot encode !\n\nnumeric_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\ncategorical_cols = [\"species\", \"island\", \"sex\"]\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(drop=None, handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\nX_trans = preprocess.fit_transform(X_train[categorical_cols + numeric_cols])\n\ncatNames = preprocess.named_transformers_[\"cat\"].get_feature_names_out()\n#numNames = preprocess.named_transformers_[\"num\"].get_feature_names_out()\n#list(catNames)+list(numNames)\n#X_df = pd.DataFrame(X_trans, columns=list(catNames)+list(numNames))\n#X_df.head()\n\n\nX_train_oh = pd.get_dummies(X_train)\nX_train_oh\n#X_train_oh = pipe[:-1].transform(X_train)\n#pd.DataFrame(X_train_oh)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nspecies_Adelie\nspecies_Chinstrap\nspecies_Gentoo\nisland_Biscoe\nisland_Dream\nisland_Torgersen\nsex_Female\nsex_Male\n\n\n\n\n230\n40.9\n13.7\n214.0\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n84\n37.3\n17.8\n191.0\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n303\n50.0\n15.9\n224.0\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n22\n35.9\n19.2\n189.0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n29\n40.5\n18.9\n180.0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n194\n50.9\n19.1\n196.0\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n77\n37.2\n19.4\n184.0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n112\n39.7\n17.7\n193.0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n277\n45.5\n15.0\n220.0\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n108\n38.1\n17.0\n181.0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n266 rows √ó 11 columns\n\n\n\n\nmodel_sk = LinearRegression()\n\nmodel_sk.fit(X_train_oh, y_train)\n\nprint(\"Intercept:\", model_sk.intercept_)\nprint(\"Coefficients:\", model_sk.coef_)\n\nIntercept: -846.0265146981037\nCoefficients: [  17.14973198   66.91629434   15.30729934 -266.29400401 -514.5295567\n  780.82356071   13.14977171   26.0672393   -39.21701101 -195.50683155\n  195.50683155]\n\n\n\npenguins.columns\n\nIndex(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n       'flipper_length_mm', 'body_mass_g', 'sex'],\n      dtype='object')\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\nlm2a = smf.ols('body_mass_g ~ species + island + bill_length_mm +bill_depth_mm + flipper_length_mm + sex', penguins).fit()\nlm2a.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-1500.0291\n575.822\n-2.605\n0.010\n-2632.852\n-367.207\n\n\nspecies[T.Chinstrap]\n-260.3063\n88.551\n-2.940\n0.004\n-434.513\n-86.100\n\n\nspecies[T.Gentoo]\n987.7614\n137.238\n7.197\n0.000\n717.771\n1257.752\n\n\nisland[T.Dream]\n-13.1031\n58.541\n-0.224\n0.823\n-128.271\n102.065\n\n\nisland[T.Torgersen]\n-48.0636\n60.922\n-0.789\n0.431\n-167.915\n71.788\n\n\nsex[T.Male]\n387.2243\n48.138\n8.044\n0.000\n292.521\n481.927\n\n\nbill_length_mm\n18.1893\n7.136\n2.549\n0.011\n4.150\n32.229\n\n\nbill_depth_mm\n67.5754\n19.821\n3.409\n0.001\n28.581\n106.570\n\n\nflipper_length_mm\n16.2385\n2.939\n5.524\n0.000\n10.456\n22.021",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_02-sklearn_pipelines.html#diamonds",
    "href": "DS_02-sklearn_pipelines.html#diamonds",
    "title": "6¬† Sklearn pipelines",
    "section": "Diamonds",
    "text": "Diamonds\nTasks:\n\nFit a linear regression (price as outcome) including all columns !\nGet the R^2 on the test data and compare to training\nWhich features seem important\nWhat about x,y,z ? Do they make sense to include in a linear fashion ?\n\n\nimport statsmodels.api as sm\ndf = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\ndf\n\n\n  \n    \n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n53935\n0.72\nIdeal\nD\nSI1\n60.8\n57.0\n2757\n5.75\n5.76\n3.50\n\n\n53936\n0.72\nGood\nD\nSI1\n63.1\n55.0\n2757\n5.69\n5.75\n3.61\n\n\n53937\n0.70\nVery Good\nD\nSI1\n62.8\n60.0\n2757\n5.66\n5.68\n3.56\n\n\n53938\n0.86\nPremium\nH\nSI2\n61.0\n58.0\n2757\n6.15\n6.12\n3.74\n\n\n53939\n0.75\nIdeal\nD\nSI2\n62.2\n55.0\n2757\n5.83\n5.87\n3.64\n\n\n\n\n53940 rows √ó 10 columns",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "DS_03-PolynomialFeatures.html",
    "href": "DS_03-PolynomialFeatures.html",
    "title": "7¬† Polynomial Features",
    "section": "",
    "text": "The Diamonds data\nhttps://bookdown.org/yih_huynh/Guide-to-R-Book/diamonds.html\nTasks:\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# -------------------------------------------------------\n# 1. Load diamonds data\n# -------------------------------------------------------\ndf = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\n\n# Outcome:\ny = df[\"price\"]\n\n# Predictors:\nX = df.drop(columns=[\"price\"])\nprint(\"original number of columns:\", X.shape[1])\n\n# Identify categorical vs numeric columns\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\ncat_cols = categorical_cols\nnumeric_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n\ndf\n\n\noriginal number of columns: 9\n\n\n\n  \n    \n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n53935\n0.72\nIdeal\nD\nSI1\n60.8\n57.0\n2757\n5.75\n5.76\n3.50\n\n\n53936\n0.72\nGood\nD\nSI1\n63.1\n55.0\n2757\n5.69\n5.75\n3.61\n\n\n53937\n0.70\nVery Good\nD\nSI1\n62.8\n60.0\n2757\n5.66\n5.68\n3.56\n\n\n53938\n0.86\nPremium\nH\nSI2\n61.0\n58.0\n2757\n6.15\n6.12\n3.74\n\n\n53939\n0.75\nIdeal\nD\nSI2\n62.2\n55.0\n2757\n5.83\n5.87\n3.64\n\n\n\n\n53940 rows √ó 10 columns\n# Save to Colab's local filesystem\ndf.to_csv(\"diamonds.csv\", index=False)\nPolynomialFeatures from scikit-learn can only be applied to numeric input. So if you have mixed data types (VERY common!), such as the diamonds dataset (which contains categorical columns like cut, color, clarity), you must first convert the categorical variables into numeric form (e.g., one-hot encoding), then apply PolynomialFeatures.\nBelow are the typical ways to do it.",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynomial Features</span>"
    ]
  },
  {
    "objectID": "DS_03-PolynomialFeatures.html#solution",
    "href": "DS_03-PolynomialFeatures.html#solution",
    "title": "7¬† Polynomial Features",
    "section": "Solution",
    "text": "Solution",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynomial Features</span>"
    ]
  },
  {
    "objectID": "DS_03-PolynomialFeatures.html#statsmodels",
    "href": "DS_03-PolynomialFeatures.html#statsmodels",
    "title": "7¬† Polynomial Features",
    "section": "Statsmodels",
    "text": "Statsmodels\n\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\"price ~ carat + depth + cut + color + clarity+ depth+ table + x + y+ z\", data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.916\nModel:                            OLS   Adj. R-squared:                  0.916\nMethod:                 Least Squares   F-statistic:                 2.942e+04\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        13:58:49   Log-Likelihood:            -4.5696e+05\nNo. Observations:               53940   AIC:                         9.140e+05\nDf Residuals:                   53919   BIC:                         9.141e+05\nDf Model:                          20                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept        -4555.1714    373.482    -12.197      0.000   -5287.199   -3823.144\ncut[T.Good]        614.4239     34.337     17.894      0.000     547.122     681.726\ncut[T.Ideal]       877.5691     34.152     25.696      0.000     810.631     944.507\ncut[T.Premium]     806.0243     32.954     24.459      0.000     741.434     870.615\ncut[T.Very Good]   778.4282     32.936     23.635      0.000     713.874     842.982\ncolor[T.E]        -210.8490     18.304    -11.519      0.000    -246.726    -174.972\ncolor[T.F]        -304.2876     18.498    -16.450      0.000    -340.543    -268.032\ncolor[T.G]        -506.9637     18.116    -27.984      0.000    -542.472    -471.456\ncolor[T.H]        -977.9737     19.269    -50.754      0.000   -1015.741    -940.207\ncolor[T.I]       -1438.2773     21.642    -66.459      0.000   -1480.695   -1395.860\ncolor[T.J]       -2322.5649     26.715    -86.940      0.000   -2374.926   -2270.204\nclarity[T.IF]     5404.2365     52.174    103.582      0.000    5301.976    5506.497\nclarity[T.SI1]    3567.7938     44.587     80.020      0.000    3480.404    3655.184\nclarity[T.SI2]    2619.0040     44.788     58.476      0.000    2531.220    2706.788\nclarity[T.VS1]    4525.4001     45.547     99.356      0.000    4436.127    4614.673\nclarity[T.VS2]    4210.1943     44.840     93.893      0.000    4122.307    4298.082\nclarity[T.VVS1]   5061.7344     48.224    104.964      0.000    4967.216    5156.253\nclarity[T.VVS2]   4957.3104     46.901    105.697      0.000    4865.384    5049.237\ncarat             8895.1940     12.079    736.390      0.000    8871.518    8918.870\ndepth              -21.0236      4.079     -5.154      0.000     -29.019     -13.028\ntable              -24.8027      2.978     -8.329      0.000     -30.639     -18.966\n==============================================================================\nOmnibus:                    15255.775   Durbin-Watson:                   0.908\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           184788.880\nSkew:                           1.016   Prob(JB):                         0.00\nKurtosis:                      11.837   Cond. No.                     6.35e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.35e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nmodel = smf.ols(\"carat ~  x + y+ z\", data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  carat   R-squared:                       0.952\nModel:                            OLS   Adj. R-squared:                  0.952\nMethod:                 Least Squares   F-statistic:                 3.536e+05\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        14:00:24   Log-Likelihood:                 45412.\nNo. Observations:               53940   AIC:                        -9.082e+04\nDf Residuals:                   53936   BIC:                        -9.078e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.5668      0.002   -669.401      0.000      -1.571      -1.562\nx              0.3591      0.002    156.316      0.000       0.355       0.364\ny              0.0052      0.002      2.914      0.004       0.002       0.009\nz              0.0784      0.003     29.396      0.000       0.073       0.084\n==============================================================================\nOmnibus:                    57670.233   Durbin-Watson:                   0.784\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         25221338.363\nSkew:                           4.766   Prob(JB):                         0.00\nKurtosis:                     108.504   Cond. No.                         65.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynomial Features</span>"
    ]
  },
  {
    "objectID": "DS_03-PolynomialFeatures.html#interactions",
    "href": "DS_03-PolynomialFeatures.html#interactions",
    "title": "7¬† Polynomial Features",
    "section": "Interactions",
    "text": "Interactions\nAlso known in the business context as synergies\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\"price ~ carat + depth\", data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.851\nModel:                            OLS   Adj. R-squared:                  0.851\nMethod:                 Least Squares   F-statistic:                 1.536e+05\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        13:35:39   Log-Likelihood:            -4.7249e+05\nNo. Observations:               53940   AIC:                         9.450e+05\nDf Residuals:                   53937   BIC:                         9.450e+05\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   4045.3332    286.205     14.134      0.000    3484.368    4606.298\ncarat       7765.1407     14.009    554.282      0.000    7737.682    7792.599\ndepth       -102.1653      4.635    -22.041      0.000    -111.251     -93.080\n==============================================================================\nOmnibus:                    14148.858   Durbin-Watson:                   0.992\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           148236.675\nSkew:                           0.962   Prob(JB):                         0.00\nKurtosis:                      10.890   Cond. No.                     2.66e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.66e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nmodel = smf.ols(\"price ~ carat * depth\"), data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.852\nModel:                            OLS   Adj. R-squared:                  0.852\nMethod:                 Least Squares   F-statistic:                 1.036e+05\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        14:05:07   Log-Likelihood:            -4.7223e+05\nNo. Observations:               53940   AIC:                         9.445e+05\nDf Residuals:                   53936   BIC:                         9.445e+05\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -7823.7383    592.049    -13.215      0.000   -8984.158   -6663.318\ncarat        2.074e+04    567.672     36.540      0.000    1.96e+04    2.19e+04\ndepth          90.0432      9.588      9.391      0.000      71.251     108.836\ncarat:depth  -210.0753      9.187    -22.868      0.000    -228.081    -192.070\n==============================================================================\nOmnibus:                    14524.513   Durbin-Watson:                   0.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           134929.068\nSkew:                           1.030   Prob(JB):                         0.00\nKurtosis:                      10.469   Cond. No.                     9.78e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.78e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nsklearn\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Load data\ndf = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\n\nX = df[[\"carat\", \"depth\"]]    # same variables as in statsmodels example\n\npoly = PolynomialFeatures(\n    degree=2,\n    include_bias=False,\n    interaction_only=True\n)\n\nX_poly = poly.fit_transform(X)\n\nprint(\"Feature names:\", poly.get_feature_names_out([\"carat\", \"depth\"]))\nprint(pd.DataFrame(X_poly, columns=poly.get_feature_names_out([\"carat\", \"depth\"])).head())\n\nFeature names: ['carat' 'depth' 'carat depth']\n   carat  depth  carat depth\n0   0.23   61.5       14.145\n1   0.21   59.8       12.558\n2   0.23   56.9       13.087\n3   0.29   62.4       18.096\n4   0.31   63.3       19.623\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\n# Load diamonds\n#df = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\n\nX = df[[\"carat\", \"depth\"]]   # same variables as in statsmodels formula\ny = df[\"price\"]\n\n# Pipeline: interaction terms only\npipe = Pipeline([\n    (\"interaction\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n    (\"model\", LinearRegression())\n])\n\npipe.fit(X, y)\n\nprint(\"Coefficients:\", pipe.named_steps[\"model\"].coef_)\nprint(\"Intercept:\", pipe.named_steps[\"model\"].intercept_)\n\nCoefficients: [20742.59987132    90.04321842  -210.07533218]\nIntercept: -7823.738250998009\n\n\nshow the expanded feature matrix\n\nimport numpy as np\n\nX_trans = pipe.named_steps[\"interaction\"].fit_transform(X)\nprint(X_trans[:5])\n\n[[ 0.23  61.5   14.145]\n [ 0.21  59.8   12.558]\n [ 0.23  56.9   13.087]\n [ 0.29  62.4   18.096]\n [ 0.31  63.3   19.623]]",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynomial Features</span>"
    ]
  },
  {
    "objectID": "DS_04-Patsy_Interactions.html",
    "href": "DS_04-Patsy_Interactions.html",
    "title": "8¬† Patsy",
    "section": "",
    "text": "The Diamonds Data again\nhttps://bookdown.org/yih_huynh/Guide-to-R-Book/diamonds.html\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# -------------------------------------------------------\n# 1. Load diamonds data\n# -------------------------------------------------------\ndf = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\n#df = pd.read_csv(\"diamonds.csv\")\n\n# Outcome:\ny = df[\"price\"]\n\n# Predictors:\nX = df.drop(columns=[\"price\"])\nprint(\"original number of columns:\", X.shape[1])\n\n# Identify categorical vs numeric columns\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\ncat_cols = categorical_cols\nnumeric_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n\ndf\n\noriginal number of columns: 9\n\n\n\n  \n    \n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n53935\n0.72\nIdeal\nD\nSI1\n60.8\n57.0\n2757\n5.75\n5.76\n3.50\n\n\n53936\n0.72\nGood\nD\nSI1\n63.1\n55.0\n2757\n5.69\n5.75\n3.61\n\n\n53937\n0.70\nVery Good\nD\nSI1\n62.8\n60.0\n2757\n5.66\n5.68\n3.56\n\n\n53938\n0.86\nPremium\nH\nSI2\n61.0\n58.0\n2757\n6.15\n6.12\n3.74\n\n\n53939\n0.75\nIdeal\nD\nSI2\n62.2\n55.0\n2757\n5.83\n5.87\n3.64\n\n\n\n\n53940 rows √ó 10 columns\nPolynomialFeatures from scikit-learn can only be applied to numeric input.\nStatsmodels allows powerful and compact formula notation, which automatically handles categorical features:\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\"price ~ carat*depth\", data=df).fit()\nprint(model.summary().tables[1])\n\n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -7823.7383    592.049    -13.215      0.000   -8984.158   -6663.318\ncarat        2.074e+04    567.672     36.540      0.000    1.96e+04    2.19e+04\ndepth          90.0432      9.588      9.391      0.000      71.251     108.836\ncarat:depth  -210.0753      9.187    -22.868      0.000    -228.081    -192.070\n===============================================================================\nmodel = smf.ols(\"price ~ carat*C(cut)\", data=df).fit()\nprint(model.summary().tables[1])\n\n=============================================================================================\n                                coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                 -1839.0737     84.354    -21.802      0.000   -2004.408   -1673.739\nC(cut)[T.Good]             -583.6544     95.774     -6.094      0.000    -771.372    -395.936\nC(cut)[T.Ideal]            -461.3000     86.569     -5.329      0.000    -630.975    -291.625\nC(cut)[T.Premium]          -540.8313     88.124     -6.137      0.000    -713.554    -368.108\nC(cut)[T.Very Good]        -578.5866     88.732     -6.521      0.000    -752.502    -404.672\ncarat                      5924.4951     72.309     81.933      0.000    5782.769    6066.221\ncarat:C(cut)[T.Good]       1555.1412     86.296     18.021      0.000    1386.000    1724.283\ncarat:C(cut)[T.Ideal]      2267.8962     76.053     29.820      0.000    2118.831    2416.961\ncarat:C(cut)[T.Premium]    1883.2569     76.429     24.641      0.000    1733.456    2033.057\ncarat:C(cut)[T.Very Good]  2011.4769     78.156     25.737      0.000    1858.291    2164.663\n=============================================================================================\nmodel = smf.ols(\"carat ~  x + y+ z\", data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  carat   R-squared:                       0.952\nModel:                            OLS   Adj. R-squared:                  0.952\nMethod:                 Least Squares   F-statistic:                 3.536e+05\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        14:00:24   Log-Likelihood:                 45412.\nNo. Observations:               53940   AIC:                        -9.082e+04\nDf Residuals:                   53936   BIC:                        -9.078e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.5668      0.002   -669.401      0.000      -1.571      -1.562\nx              0.3591      0.002    156.316      0.000       0.355       0.364\ny              0.0052      0.002      2.914      0.004       0.002       0.009\nz              0.0784      0.003     29.396      0.000       0.073       0.084\n==============================================================================\nOmnibus:                    57670.233   Durbin-Watson:                   0.784\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         25221338.363\nSkew:                           4.766   Prob(JB):                         0.00\nKurtosis:                     108.504   Cond. No.                         65.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Patsy</span>"
    ]
  },
  {
    "objectID": "DS_04-Patsy_Interactions.html#patsy",
    "href": "DS_04-Patsy_Interactions.html#patsy",
    "title": "8¬† Patsy",
    "section": "patsy",
    "text": "patsy\nBringing Statsmodels (R) formula notation to sklearn !\n‚Ä¶and automatically get:\n\nDesign matrices with dummy coding\nPolynomial terms, interactions, splines, contrasts\nFeature Names\nA record of transformations, so predictions on new data work\nNo manual preprocessing (no OneHotEncoder, no PolynomialFeatures)\n\nAll of this is extremely useful before passing the data into sklearn, which only expects numeric matrices.\n\nimport pandas as pd\nimport patsy\nfrom sklearn.linear_model import LinearRegression\n\n# ---------------------------\n# 1. Create example data\n# ---------------------------\ndf = pd.DataFrame({\n    \"y\":   [1, 2, 3, 4, 5, 6],\n    \"x1\":  [1, 2, 3, 4, 5, 6],\n    \"x2\":  [2, 1, 2, 1, 2, 1],\n    \"grp\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\"]\n})\n\n# ---------------------------\n# 2. Build the design matrices with patsy\n# ---------------------------\n# R-style formula:\nformula = \"y ~ x1 + x2 + C(grp) + x1:x2 + I(x1**2)\"\n\nformula = \"y ~ x1*x2 + C(grp) + I(x1**2)\"\n\ny, X = patsy.dmatrices(formula, df, return_type=\"dataframe\")\n\nprint(X)\n\n   Intercept  C(grp)[T.B]   x1   x2  x1:x2  I(x1 ** 2)\n0        1.0          0.0  1.0  2.0    2.0         1.0\n1        1.0          1.0  2.0  1.0    2.0         4.0\n2        1.0          0.0  3.0  2.0    6.0         9.0\n3        1.0          1.0  4.0  1.0    4.0        16.0\n4        1.0          0.0  5.0  2.0   10.0        25.0\n5        1.0          1.0  6.0  1.0    6.0        36.0\n\n\n\nformula = \"y ~ x1*x2 + C(grp) + I(x1**2)\"\ny, X = patsy.dmatrices(formula, df, return_type=\"dataframe\")\n\n# no Y\nformula = \"x1*x2 + C(grp) + I(x1**2)\"\nX = patsy.dmatrix(formula, df, return_type=\"dataframe\")\nX\n\n\n  \n    \n\n\n\n\n\n\nIntercept\nC(grp)[T.B]\nx1\nx2\nx1:x2\nI(x1 ** 2)\n\n\n\n\n0\n1.0\n0.0\n1.0\n2.0\n2.0\n1.0\n\n\n1\n1.0\n1.0\n2.0\n1.0\n2.0\n4.0\n\n\n2\n1.0\n0.0\n3.0\n2.0\n6.0\n9.0\n\n\n3\n1.0\n1.0\n4.0\n1.0\n4.0\n16.0\n\n\n4\n1.0\n0.0\n5.0\n2.0\n10.0\n25.0\n\n\n5\n1.0\n1.0\n6.0\n1.0\n6.0\n36.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nNow you can send this to sklearn:\n\nmodel = LinearRegression().fit(X, y)\nprint(model.coef_)\n\n[[ 0.00000000e+00 -2.22044605e-16  1.00000000e+00  4.78349999e-15\n  -8.32667268e-16  8.32667268e-17]]",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Patsy</span>"
    ]
  },
  {
    "objectID": "DS_05-Target_Encoding_Leakage.html",
    "href": "DS_05-Target_Encoding_Leakage.html",
    "title": "9¬† Target Coding",
    "section": "",
    "text": "‚úÖ Statsmodels Example\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\n\n# -----------------------\n# Example data\n# -----------------------\ndf = pd.DataFrame({\n    \"group\": [\"A\", \"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"C\", \"C\"],\n    \"y\":     [10, 12, 11, 20, 22, 30, 31, 29, 32]\n})\n\n# -----------------------\n# 1. Dummy-coded regression\n# -----------------------\ndf_dum = pd.get_dummies(df[\"group\"], drop_first=True).astype(float)\n# now df_dum is numeric\n\nX_dum = sm.add_constant(df_dum.astype(float))\ny = df[\"y\"].astype(float)\n\nmodel_dummy = sm.OLS(y, X_dum).fit()\npred_dummy = model_dummy.predict(X_dum)\n\nprint(\"Dummy coding predictions:\", pred_dummy.values)\n\n# -----------------------\n# 2. Target encoding regression\n# -----------------------\nmeans = df.groupby(\"group\")[\"y\"].mean()\ndf[\"target_enc\"] = df[\"group\"].map(means).astype(float)\n\nX_te = sm.add_constant(df[[\"target_enc\"]])\nmodel_te = sm.OLS(y, X_te).fit()\npred_te = model_te.predict(X_te)\n\nprint(\"\\nTarget encoding predictions:\", pred_te.values)\n\nprint(\"\\nIdentical predictions?\", np.allclose(pred_dummy, pred_te))\n\nDummy coding predictions: [11.  11.  11.  21.  21.  30.5 30.5 30.5 30.5]\n\nTarget encoding predictions: [11.  11.  11.  21.  21.  30.5 30.5 30.5 30.5]\n\nIdentical predictions? True",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Target Coding</span>"
    ]
  },
  {
    "objectID": "DS_05-Target_Encoding_Leakage.html#target-coding-leakage",
    "href": "DS_05-Target_Encoding_Leakage.html#target-coding-leakage",
    "title": "9¬† Target Coding",
    "section": "Target Coding Leakage",
    "text": "Target Coding Leakage\nData Leakage can be especially severe in target coding for the case of rare categories.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nnp.random.seed(0)\n\n# --------------------------\n# 1. Create synthetic data\n# --------------------------\n\nn_categories = 200     # many categories\nsamples_per_cat = 3    # extremely rare\nN = n_categories * samples_per_cat\n\ncategories = np.repeat(np.arange(n_categories), samples_per_cat)\nnp.random.shuffle(categories)\n\n# Regression target: PURE NOISE\ny = np.random.normal(loc=0.0, scale=1.0, size=N)\n\ndf = pd.DataFrame({\"cat\": categories, \"y\": y})\n\nglobal_mean = df[\"y\"].mean()\n\n# ----------------------------------------------\n# 2. FULL-DATA target encoding (deliberate leak)\n# ----------------------------------------------\n\nmeans = df.groupby(\"cat\")[\"y\"].mean()\ndf[\"te_full\"] = df[\"cat\"].map(means).fillna(global_mean)\n\nX_full = df[[\"te_full\"]].values\nreg = LinearRegression()\n\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\nr2_full = cross_val_score(reg, X_full, y, cv=cv, scoring=\"r2\")\n\nprint(\"R¬≤ using FULL-DATA target encoding (inflated due to leakage):\")\nprint(r2_full)\nprint(\"Mean R¬≤:\", r2_full.mean())\n\n# --------------------------------------------\n# 3. Evaluate on a true independent test split\n# --------------------------------------------\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df[[\"cat\"]], y, test_size=0.4, random_state=0\n)\n\n# Compute TE only from the training split\ntrain_means = (\n    X_train.join(pd.Series(y_train, name=\"y\"))\n            .groupby(\"cat\")[\"y\"].mean()\n)\n\nfold_global_mean = y_train.mean()\n\n# Apply to both splits\nX_train_enc = X_train[\"cat\"].map(train_means).fillna(fold_global_mean)\nX_test_enc  = X_test[\"cat\"].map(train_means).fillna(fold_global_mean)\n\nreg.fit(X_train_enc.values.reshape(-1,1), y_train)\npred_test = reg.predict(X_test_enc.values.reshape(-1,1))\n\nprint(\"\\nTrue Test R¬≤ (should be ‚âà 0):\")\nprint(r2_score(y_test, pred_test))\n\n# ------------------------------------------------------------\n# 4. Correct CV-safe target encoding (fold-by-fold construction)\n# ------------------------------------------------------------\n\nte_cv = np.zeros(N)\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\nfor train_idx, valid_idx in kf.split(df):\n    train_fold = df.iloc[train_idx]\n    valid_fold = df.iloc[valid_idx]\n\n    fold_means = train_fold.groupby(\"cat\")[\"y\"].mean()\n    fold_global = train_fold[\"y\"].mean()\n\n    enc_vals = valid_fold[\"cat\"].map(fold_means).fillna(fold_global)\n    te_cv[valid_idx] = enc_vals\n\nX_te_cv = te_cv.reshape(-1,1)\nr2_cvsafe = cross_val_score(reg, X_te_cv, y, cv=5, scoring=\"r2\")\n\nprint(\"\\nR¬≤ using CV-safe target encoding (correct, no leakage):\")\nprint(r2_cvsafe)\nprint(\"Mean R¬≤:\", r2_cvsafe.mean())\n\nR¬≤ using FULL-DATA target encoding (inflated due to leakage):\n[0.22622513 0.30693162 0.26777767 0.28403269 0.37383319]\nMean R¬≤: 0.2917600586203898\n\nTrue Test R¬≤ (should be ‚âà 0):\n-0.031266384042003104\n\nR¬≤ using CV-safe target encoding (correct, no leakage):\n[-0.00865704  0.01285604 -0.01020858  0.00923428 -0.00502922]\nMean R¬≤: -0.00036090500984973237\n\n\n\nUsing a pipeline\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\n\nnp.random.seed(0)\n\n# ---------------------------------------\n# Custom Target Encoder (safe for sklearn)\n# ---------------------------------------\n\nclass TargetEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Simple target encoder for regression:\n    - fit computes category -&gt; mean(target)\n    - transform maps categories and fills unseen with global mean\n    \"\"\"\n    def __init__(self):\n        pass\n\n    def fit(self, X, y):\n        X = X.copy()\n        X[\"y\"] = y\n        self.global_mean_ = y.mean()\n        self.cat_means_ = X.groupby(\"cat\")[\"y\"].mean()\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        out = X[\"cat\"].map(self.cat_means_)\n        return out.fillna(self.global_mean_).to_frame()\n\n\n# -------------------------\n# Create synthetic dataset\n# -------------------------\n\nn_categories = 200\nsamples_per_cat = 3\nN = n_categories * samples_per_cat\n\ncategories = np.repeat(np.arange(n_categories), samples_per_cat)\nnp.random.shuffle(categories)\n\n# Pure noise regression target\ny = np.random.normal(0, 1, size=N)\n\ndf = pd.DataFrame({\"cat\": categories, \"y\": y})\nglobal_mean = df[\"y\"].mean()\n\n# -------------------------------------------\n# 1. BAD: Full-data target encoding (leakage)\n# -------------------------------------------\n\nmeans_full = df.groupby(\"cat\")[\"y\"].mean()\ndf[\"te_full\"] = df[\"cat\"].map(means_full).fillna(global_mean)\n\nX_full = df[[\"te_full\"]]\nreg = LinearRegression()\n\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\nr2_full_data = cross_val_score(reg, X_full, y, cv=kf, scoring=\"r2\")\n\nprint(\"R¬≤ using FULL-DATA target encoding (leakage):\")\nprint(r2_full_data)\nprint(\"Mean:\", r2_full_data.mean())\n\n# -----------------------------------------------------\n# 2. GOOD: Pipeline with correct target encoding (safe)\n# -----------------------------------------------------\n\npipeline = Pipeline([\n    (\"te\", TargetEncoder()),\n    (\"reg\", LinearRegression())\n])\n\nr2_pipeline_safe = cross_val_score(pipeline, df[[\"cat\"]], y, cv=kf, scoring=\"r2\")\n\nprint(\"\\nR¬≤ using sklearn Pipeline target encoding (no leakage):\")\nprint(r2_pipeline_safe)\nprint(\"Mean:\", r2_pipeline_safe.mean())\n\n# ---------------------------------------\n# 3. True hold-out test set performance\n# ---------------------------------------\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df[[\"cat\"]], y, test_size=0.4, random_state=0\n)\n\npipeline.fit(X_train, y_train)\npred_test = pipeline.predict(X_test)\n\nprint(\"\\nTrue Test R¬≤ (should be ‚âà 0):\")\nprint(r2_score(y_test, pred_test))\n\nR¬≤ using FULL-DATA target encoding (leakage):\n[0.22622513 0.30693162 0.26777767 0.28403269 0.37383319]\nMean: 0.2917600586203898\n\nR¬≤ using sklearn Pipeline target encoding (no leakage):\n[-0.81546011 -0.85654367 -0.86557566 -0.79999908 -0.55563539]\nMean: -0.7786427810075826\n\nTrue Test R¬≤ (should be ‚âà 0):\n-0.6846761186521659\n\n\n\n\nhash(\"dog\")\n\n-4253240428867011524\n\n\n\n\nhash(\"cat\") % 10\n\n9\n\n\n\nhash(\"giraffe\") % 10 #this gives us the remainder of a division by 10\n#This number is between which values ?\nAfter this you do OHE\n\n6\n\n\nThis is a number betwen zero and nine !\nThe hashing function creates a fixed lenght integer out of any category !!\nA collision would be the same integer value fro distinct categories",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Target Coding</span>"
    ]
  },
  {
    "objectID": "DS_06-Logistic_Regression.html",
    "href": "DS_06-Logistic_Regression.html",
    "title": "10¬† Logistic Regression",
    "section": "",
    "text": "Coefficients\nCan we compare the coefficients from a linear regression with a logistic regression?\nimport seaborn as sns\nimport pandas as pd\nimport statsmodels.api as sm\n\n# 1. Load data\ndf = sns.load_dataset(\"titanic\")\n\n# 2. Select variables and drop missing rows\nvars = [\"survived\", \"pclass\", \"sex\", \"age\"]\ndf = df[vars].dropna()\n\n# 3. Convert all categorical variables to numeric dummy variables\ndf = pd.get_dummies(df, drop_first=True)\n\n# 4. Force EVERY column to float (this kills all object dtypes)\ndf = df.astype(float)\n\n# 5. Build X and y\ny = df[\"survived\"].astype(float)\nX = df.drop(\"survived\", axis=1).astype(float)\n\n# 6. Add intercept\nX = sm.add_constant(X)\n\n# 7. Fit logistic model\nmodel = sm.Logit(y, X)\nresult = model.fit()\n\nprint(result.summary2().tables[1])\n\nOptimization terminated successfully.\n         Current function value: 0.453285\n         Iterations 6\n             Coef.  Std.Err.          z         P&gt;|z|    [0.025    0.975]\nconst     5.056006  0.502128  10.069153  7.562686e-24  4.071853  6.040160\npclass   -1.288545  0.139259  -9.252845  2.185952e-20 -1.561488 -1.015602\nage      -0.036929  0.007628  -4.841456  1.288912e-06 -0.051879 -0.021979\nsex_male -2.522131  0.207283 -12.167572  4.626405e-34 -2.928398 -2.115864\n# Fit linear regression (OLS) on the same X and y\nols_model = sm.OLS(y, X)\nols_result = ols_model.fit()\n\n# Print only the coefficient table (same style)\nols_coef_table = ols_result.summary2().tables[1]\nprint(ols_coef_table)\n\n             Coef.  Std.Err.          t         P&gt;|t|    [0.025    0.975]\nconst     1.326066  0.062606  21.181209  1.498756e-77  1.203151  1.448981\npclass   -0.202916  0.018891 -10.741281  4.844250e-25 -0.240006 -0.165827\nage      -0.005453  0.001082  -5.041812  5.859689e-07 -0.007577 -0.003330\nsex_male -0.479293  0.030671 -15.626841  1.568080e-47 -0.539510 -0.419076",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "DS_06-Logistic_Regression.html#predictions",
    "href": "DS_06-Logistic_Regression.html#predictions",
    "title": "10¬† Logistic Regression",
    "section": "Predictions",
    "text": "Predictions\nThere are two different types of predictions:\n\nprobabilities\nhard decisions\n\n\nprobs = model.predict_proba(X_test)[:, 1]\nprobs[0:10]\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 probs = model.predict_proba(X_test)[:, 1]\n      2 probs[0:10]\n\nAttributeError: 'Logit' object has no attribute 'predict_proba'\n\n\n\n\nSurv_pred = model.predict(X_test)\nSurv_pred[0:10]\n\narray([0, 0, 0, 1, 1, 0, 1, 1, 0, 1])\n\n\nWith the latter, we can build a confusion matrix:\n\ncm = confusion_matrix(y_test, Surv_pred)\npd.DataFrame(cm, index=[\"Act0\", \"Act1\"], columns=[\"Pred0\",\"Pred1\"])\n\n\n\n\n\n\n\n\nPred0\nPred1\n\n\n\n\nAct0\n140\n28\n\n\nAct1\n29\n71\n\n\n\n\n\n\n\n\ncm.ravel()\n\narray([140,  28,  29,  71])\n\n\nTasks\n\nCompute precision and recall\nIs this the only confusion matrix possible ?\n\nHere is another way:\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, Surv_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.83      0.83      0.83       168\n           1       0.72      0.71      0.71       100\n\n    accuracy                           0.79       268\n   macro avg       0.77      0.77      0.77       268\nweighted avg       0.79      0.79      0.79       268\n\n\n\n\n\nMultinomial Logistic Regression With a 3√ó3 Confusion Matrix\nWe‚Äôll use the Palmer Penguins data (penguins), predict species using two simple predictors like bill_length_mm and bill_depth_mm, and produce a 3√ó3 confusion matrix.\nPerfect to demonstrate multiclass evaluation, class boundaries, and how multinomial logistic regression generalizes the binary model.\n\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\n# 1. Load penguins data\npenguins = sns.load_dataset(\"penguins\").dropna()\n\n# Features + target\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"]   # 3 classes\n\n# 2. Train / test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 3. Multinomial logistic regression\nmodel = LogisticRegression(multi_class=\"multinomial\", max_iter=2000)\nmodel.fit(X_train, y_train)\n\n# 4. Predictions\npreds = model.predict(X_test)\n\n# 5. 3√ó3 confusion matrix\ncm = confusion_matrix(y_test, preds, labels=model.classes_)\n\ncm_df = pd.DataFrame(cm, index=model.classes_, columns=model.classes_)\ncm_df\n\n/Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\n\n\n\n\nAdelie\n44\n0\n0\n\n\nChinstrap\n0\n17\n3\n\n\nGentoo\n0\n0\n36",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "DS_07-ROC_curves.html",
    "href": "DS_07-ROC_curves.html",
    "title": "11¬† ROC curves",
    "section": "",
    "text": "Below is a clean example using scikit-learn to demonstrate ROC curves and AUC with Logistic Regression, first without a pipeline and then with a pipeline. The dataset used is the diabetes dataset from sklearn.datasets.load_diabetes.\n‚ö†Ô∏è Note: The diabetes dataset is originally for regression. For teaching binary classification, we will convert it into a binary label (‚Äúdisease severity above median‚Äù). This works great for demonstrations of ROC/AUC.\nFor a real intrinsic classification dataset, we include an extra block using the breast-cancer dataset.\n\nimport numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# -----------------------\n# Load data and binarize target\n# -----------------------\ndata = load_diabetes()\nX = data.data\ny = (data.target &gt; np.median(data.target)).astype(int)  # turn into binary problem\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# -----------------------\n# Fit logistic regression\n# -----------------------\nclf = LogisticRegression(max_iter=5000)\nclf.fit(X_train, y_train)\n\n# Predicted probabilities for ROC\ny_score = clf.predict_proba(X_test)[:, 1]\n\n# -----------------------\n# ROC & AUC\n# -----------------------\nfpr, tpr, thresholds = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f\"LogReg ROC curve (AUC = {roc_auc:.3f})\")\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC curve ‚Äì Logistic Regression (no pipeline)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nUsing a pipeline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# -----------------------\n# Build pipeline\n# -----------------------\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"logreg\", LogisticRegression(max_iter=5000))\n])\n\npipe.fit(X_train, y_train)\n\ny_score_pipe = pipe.predict_proba(X_test)[:, 1]\n\n# ROC\nfpr_p, tpr_p, _ = roc_curve(y_test, y_score_pipe)\nauc_p = auc(fpr_p, tpr_p)\n\nplt.figure()\nplt.plot(fpr_p, tpr_p, label=f\"Pipeline ROC (AUC = {auc_p:.3f})\")\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC curve ‚Äì Pipeline (Scaler + Logistic Regression)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPrecision-Recall Curve (no pipe)\n\nfrom sklearn.metrics import (\n    roc_curve, auc,\n    precision_recall_curve, average_precision_score\n)\n\nclf = LogisticRegression(max_iter=5000)\nclf.fit(X_train, y_train)\n\n# Predict probabilities\ny_score = clf.predict_proba(X_test)[:, 1]\n\n# PR curve\nprec, rec, pr_thresh = precision_recall_curve(y_test, y_score)\nap = average_precision_score(y_test, y_score)\n\nplt.figure()\nplt.plot(rec, prec, label=f\"AP = {ap:.3f}\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision‚ÄìRecall Curve (No Pipeline)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPrecision‚ÄìRecall Curve (WITH pipeline)\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"logreg\", LogisticRegression(max_iter=5000))\n])\n\npipe.fit(X_train, y_train)\ny_score_pipe = pipe.predict_proba(X_test)[:, 1]\n\nprec_p, rec_p, _ = precision_recall_curve(y_test, y_score_pipe)\nap_p = average_precision_score(y_test, y_score_pipe)\n\nplt.figure()\nplt.plot(rec_p, prec_p, label=f\"AP = {ap_p:.3f}\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision‚ÄìRecall Curve (Pipeline: Scaler + LogReg)\")\nplt.legend()\nplt.show()\n\n\n\nCross-validated ROC and PR Curves (NO pipeline)\n\nfrom sklearn.model_selection import train_test_split, cross_val_predict, StratifiedKFold\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nclf = LogisticRegression(max_iter=5000)\n\n# Out-of-fold decision scores\ny_cv_scores = cross_val_predict(\n    clf, X, y,\n    cv=cv,\n    method=\"predict_proba\"\n)[:, 1]\n\n# ROC\nfpr_cv, tpr_cv, _ = roc_curve(y, y_cv_scores)\nauc_cv = auc(fpr_cv, tpr_cv)\n\nplt.figure()\nplt.plot(fpr_cv, tpr_cv, label=f\"CV AUC = {auc_cv:.3f}\")\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Cross-validated ROC (No Pipeline)\")\nplt.legend()\nplt.show()\n\n# PR\nprec_cv, rec_cv, _ = precision_recall_curve(y, y_cv_scores)\nap_cv = average_precision_score(y, y_cv_scores)\n\nplt.figure()\nplt.plot(rec_cv, prec_cv, label=f\"CV AP = {ap_cv:.3f}\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Cross-validated PR Curve (No Pipeline)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCross-validated ROC & PR (WITH pipeline)\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"logreg\", LogisticRegression(max_iter=5000))\n])\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\ny_cv_scores_pipe = cross_val_predict(\n    pipe, X, y,\n    cv=cv,\n    method=\"predict_proba\"\n)[:, 1]\n\n# ROC\nfpr_p, tpr_p, _ = roc_curve(y, y_cv_scores_pipe)\nauc_p = auc(fpr_p, tpr_p)\n\nplt.figure()\nplt.plot(fpr_p, tpr_p, label=f\"CV AUC = {auc_p:.3f}\")\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Cross-validated ROC (Pipeline)\")\nplt.legend()\nplt.show()\n\n# PR\nprec_p, rec_p, _ = precision_recall_curve(y, y_cv_scores_pipe)\nap_p = average_precision_score(y, y_cv_scores_pipe)\n\nplt.figure()\nplt.plot(rec_p, prec_p, label=f\"CV AP = {ap_p:.3f}\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Cross-validated PR Curve (Pipeline)\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreast Cancer Example\n\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42, stratify=y\n)\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"logreg\", LogisticRegression(max_iter=5000))\n])\n\npipe.fit(X_train, y_train)\ny_score = pipe.predict_proba(X_test)[:, 1]\n\nfpr, tpr, _ = roc_curve(y_test, y_score)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\nplt.plot([0, 1], [0, 1], \"k--\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"ROC curve ‚Äì Breast Cancer dataset\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>ROC curves</span>"
    ]
  },
  {
    "objectID": "CC_01-Dictionaries.html",
    "href": "CC_01-Dictionaries.html",
    "title": "12¬† Dictionaries",
    "section": "",
    "text": "In this lesson we will get to know and become experts in:\n\nDictionaries\n\nDataCamp, Intermediate Python, Chap 2\n\n\n\nDictionaries\nA dictionary is basically a lookup table. It stores a collection of key-value pairs, where key and value are Python objects. Each key is associated with a value so that a value can be conveniently retrieved, inserted, modified, or deleted given a particular key.\nThe dictionary or dict may be the most important built-in Python data structure. In other programming languages, dictionaries are sometimes called hash maps or associative arrays.\n\n#Analogy to a language dictionary:\nEnglishGerman = {\"slander\": \"Verleumdung\",\n                 \"salient\" : [\"auffallend\", \"hervorstechend\"],\n                 \"secular\" : \"profan\",\n                 \"rejoice\" : \"jubeln\"}\n\n\n#adding elements:\nEnglishGerman[\"rain\"] = \"Regen\"\nEnglishGerman\n\n{'slander': 'Verleumdung',\n 'salient': ['auffallend', 'hervorstechend'],\n 'secular': 'profan',\n 'rejoice': 'jubeln',\n 'rain': 'Regen'}\n\n\n\neurope = {'spain':'madrid', 'france' : 'paris'}\nprint(europe[\"spain\"])\nprint(\"france\" in europe)\nprint(\"paris\" in europe)#only checks the keys!\neurope[\"germany\"] = \"berlin\"\nprint(europe.keys())\nprint(europe.values())\n\"paris\" in europe.values()\n\nmadrid\nTrue\nFalse\ndict_keys(['spain', 'france', 'germany'])\ndict_values(['madrid', 'paris', 'berlin'])\n\n\nTrue\n\n\n\n\nDictionaries from lists\nHow would we convert two lists into a key: value pair dictionary?\nMethod 1: using zip\n\nrooms=['hallway', 'kitchen', 'living room', 'bedroom', 'bathroom']\nareas=[11.25, 18.0, 20.0, 10.75, 9.5]\n#create list of tuples\nhouse = list(zip(rooms, areas))\nhouse\n#house[\"kitchen\"]\n\n[('hallway', 11.25),\n ('kitchen', 18.0),\n ('living room', 20.0),\n ('bedroom', 10.75),\n ('bathroom', 9.5)]\n\n\n\nhouse = dict(zip(rooms, areas))\nhouse\n#house[\"kitchen\"]\n\n{'hallway': 11.25,\n 'kitchen': 18.0,\n 'living room': 20.0,\n 'bedroom': 10.75,\n 'bathroom': 9.5}\n\n\nYou can directly convert a dictionary to a one-column DataFrame where the keys become the row index and the values become the column entries.\n\nimport pandas as pd\ndf = pd.DataFrame.from_dict(house, orient='index', columns=['value'])\nprint(df)\n\n             value\nhallway      11.25\nkitchen      18.00\nliving room  20.00\nbedroom      10.75\nbathroom      9.50\n\n\nIf you need to iterate over both the keys and values, you can use the items method to iterate over the keys and values as 2-tuples:\n\nfor i in range(5):\n  print(i)\n\n0\n1\n2\n3\n4\n\n\n\n#print(list(europe.items()))\n\nfor country, capital in europe.items():\n    print(capital, \"is the capital of\", country)\n\nmadrid is the capital of spain\nparis is the capital of france\nberlin is the capital of germany\n\n\nNote: You can use integers as keys as well. However -unlike in lists- one should not think of them as positional indices!\n\n#Assume you have a basement:\nhouse[0] = 21.5\nhouse\n\n{'hallway': 11.25,\n 'kitchen': 18.0,\n 'living room': 20.0,\n 'bedroom': 10.75,\n 'bathroom': 9.5,\n 0: 21.5}\n\n\n\n#And there is a difference between the string and the integer index!\nhouse[\"0\"] = 30.5\nhouse\n\n{'hallway': 11.25,\n 'kitchen': 18.0,\n 'living room': 20.0,\n 'bedroom': 10.75,\n 'bathroom': 9.5,\n 0: 21.5,\n '0': 30.5}\n\n\nCategorize a list of words by their first letters as a dictionary of lists:\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\n\nby_letter = {}\n\nfor word in words:\n     letter = word[0]\n     if letter not in by_letter:#if key does not exist yet then add the key value pair !\n        by_letter[letter] = [word]\n     else:#otherwise key exists, so just append\n         by_letter[letter].append(word)\n\nby_letter\nby_letter[\"b\"]\n\n['bat', 'bar', 'book']\n\n\n\nw = \"apple\"\nw[0]\n\n'a'\n\n\n\nTasks\n\nWrite a function named word_count() that takes a string as input and returns a dictionary with each word in the string as a key and the number of times it appears as the value, e.g.¬†word_count(\"I really really really like this really nice book\") should yield: {\"I\":1, \"book\":1 , \"like\":1, \"really\": 4, \"this\":1}\nConvert the dict into a pd dataframe and -using Boolean subsetting- only keep rows with counts &gt; 1!\n\n\ndef word_count(text):\n  words = text.split()\n  frequency = {}\n\n  for i in words:\n      if i not in frequency:\n        frequency[i] = 1\n      else:\n        frequency[i] += 1\n\n  return frequency\n\ntext = \"I really, really, really, like this really nice book!\"\nfinal_counts = word_count(text)\nprint(final_counts)\n\n{'I': 1, 'really,': 3, 'like': 1, 'this': 1, 'really': 1, 'nice': 1, 'book!': 1}\n\n\n\ntext = \"I really, really, really, like this really nice book!\"\ntext.split()\n\n['I',\n 'really,',\n 'really,',\n 'really,',\n 'like',\n 'this',\n 'really',\n 'nice',\n 'book!']\n\n\n\n#is there another function that counts how often values occur in an array ?\nimport numpy as np\nx = [1,2,1,1,2,3,3,4,1]#.count()\nnp.unique(x,return_counts=True)\n\n(array([1, 2, 3, 4]), array([4, 2, 2, 1]))\n\n\n\nx = np.array([1,2,1,1,2,3,3,4,1])\nx &gt; 2\n\n#x[x&gt;2]#maybe this retrieves\n\nx == 2#question !! It is not an assignment !!\n\narray([False,  True, False, False,  True, False, False, False, False])\n\n\n\nimport pandas as pd\n\ny = pd.DataFrame(x)\ny[x&gt;2]\n\n\n  \n    \n\n\n\n\n\n\n0\n\n\n\n\n5\n3\n\n\n6\n3\n\n\n7\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\ndf = pd.DataFrame.from_dict(europe, orient='index', columns=['value'])\ndf\n\n\n  \n    \n\n\n\n\n\n\nvalue\n\n\n\n\nspain\nmadrid\n\n\nfrance\nparis\n\n\ngermany\nberlin\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n#df[df[\"value\"] == \"berlin\"]\n\nselRows = (df[\"value\"] == \"berlin\")\ndf[selRows]\n\n\n  \n    \n\n\n\n\n\n\nvalue\n\n\n\n\ngermany\nberlin\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\n\n13 Titanic\n\nimport seaborn as sns\n\ntitanic = sns.load_dataset('titanic')\ntitanic\n\n\n  \n    \n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows √ó 15 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n#get the age column:\ntitanic[\"age\"]\ntitanic.age\n#this was column subsetting\n#can you get rows 1:2\n#if this was numpy you could just do the folllowing:\n#titanic[0:2,:]\n#one reason is the ability to use strings as indices, so integer indexing needs its own handling.\n#titanic.iloc[0:2,:]\n#titanic.row = range(1)\n#get me rows 13 to 24 (ends inclusive)\ntitanic.iloc[12:24,:]\n\n\n  \n    \n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n12\n0\n3\nmale\n20.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n13\n0\n3\nmale\n39.0\n1\n5\n31.2750\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n14\n0\n3\nfemale\n14.0\n0\n0\n7.8542\nS\nThird\nchild\nFalse\nNaN\nSouthampton\nno\nTrue\n\n\n15\n1\n2\nfemale\n55.0\n0\n0\n16.0000\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n16\n0\n3\nmale\n2.0\n4\n1\n29.1250\nQ\nThird\nchild\nFalse\nNaN\nQueenstown\nno\nFalse\n\n\n17\n1\n2\nmale\nNaN\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n18\n0\n3\nfemale\n31.0\n1\n0\n18.0000\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n19\n1\n3\nfemale\nNaN\n0\n0\n7.2250\nC\nThird\nwoman\nFalse\nNaN\nCherbourg\nyes\nTrue\n\n\n20\n0\n2\nmale\n35.0\n0\n0\n26.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n21\n1\n2\nmale\n34.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nD\nSouthampton\nyes\nTrue\n\n\n22\n1\n3\nfemale\n15.0\n0\n0\n8.0292\nQ\nThird\nchild\nFalse\nNaN\nQueenstown\nyes\nTrue\n\n\n23\n1\n1\nmale\n28.0\n0\n0\n35.5000\nS\nFirst\nman\nTrue\nA\nSouthampton\nyes\nTrue\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nWhat are the estimated survival probabilities by sex and class separately?\n\nbySex = titanic.groupby(\"sex\").survived\nbySex.mean()\n\n\n\n\n\n\n\n\nsurvived\n\n\nsex\n\n\n\n\n\nfemale\n0.742038\n\n\nmale\n0.188908\n\n\n\n\ndtype: float64\n\n\nCan you repeat the above but only for passengers with age &gt;= 18\n\nbySex = titanic[titanic[\"age\"] &gt;= 18].groupby(\"sex\").survived\nbySex.mean()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 bySex = titanic[titanic[\"age\"] &gt;= 18].groupby(\"sex\").survived\n      2 bySex.mean()\n\nNameError: name 'titanic' is not defined\n\n\n\n\nnoChildren = titanic[titanic[\"age\"] &gt;= 18]\nbySex = noChildren.groupby(\"sex\").survived\nbySex.mean()\n\n\n\n\n\n\n\n\nsurvived\n\n\nsex\n\n\n\n\n\nfemale\n0.771845\n\n\nmale\n0.177215\n\n\n\n\ndtype: float64\n\n\n\nbyClass = titanic.groupby(\"pclass\").survived\nbyClass.mean()\n\nWhat are the estimated survival probabilities by class and sex simultaneously ?\n\nbyClass = titanic.groupby([\"pclass\",\"sex\"]).survived\nbyClass.mean()\n\n\n\n\n\n\n\n\n\nsurvived\n\n\npclass\nsex\n\n\n\n\n\n1\nfemale\n0.968085\n\n\nmale\n0.368852\n\n\n2\nfemale\n0.921053\n\n\nmale\n0.157407\n\n\n3\nfemale\n0.500000\n\n\nmale\n0.135447\n\n\n\n\ndtype: float64\n\n\nCan we also get counts ?\n\nbyClass = titanic.groupby([\"pclass\",\"sex\"]).survived\n#how do you add multiple functions\nbyClass.agg([\"mean\", \"count\"])\n\n\n  \n    \n\n\n\n\n\n\n\nmean\ncount\n\n\npclass\nsex\n\n\n\n\n\n\n1\nfemale\n0.968085\n94\n\n\nmale\n0.368852\n122\n\n\n2\nfemale\n0.921053\n76\n\n\nmale\n0.157407\n108\n\n\n3\nfemale\n0.500000\n144\n\n\nmale\n0.135447\n347\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nThere seem to be clear differences in probabilties/proportions.\n\nHow can we be sure that they are not due to small sample sizes ?\nCan we put an uncertainty on the estimates ?\nWrite a function which simulates tossing a ‚Äúbiased coin‚Äù (with \\(p=0.157407\\)) \\(108\\) times and computes the proportion of ‚Äúheads‚Äù.\nCall this function ‚Äúmany‚Äù times and plot a histogram\n\n\ndef toss_biased_coin(pHead, n, seed=None):\n  \"\"\"\n    Simulates n tosses of a biased coin and returns the proportion of heads,\n    using rng.choice() for better conceptual clarity.\n\n    Parameters\n    ----------\n    pHead : float\n        Probability of heads (between 0 and 1).\n    n : int\n        Number of tosses.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    float\n        Proportion of heads observed.\n    \"\"\"\n  rng = np.random.default_rng(seed)\n\n  tosses = rng.choice([0,1], size=n, p=[1-pHead,pHead])\n  cts = np.sum(tosses == 1)\n  cts = np.sum(tosses)\n  prop_heads = cts/n\n\n  prop_heads = np.mean(tosses)\n\n  return prop_heads\n\n\n\ntoss_biased_coin(pHead=0.157407, n= 108)#, seed = 123)\n\nnp.float64(0.17592592592592593)\n\n\n\n#let us run this many times !\npHead_obs = np.zeros(500)\n\nfor i in range(500):\n  pHead_obs[i] = toss_biased_coin(pHead=0.157407, n= 108)\n\n\n\n#95% interval:\nnp.percentile(pHead_obs, [2.5,97.5])\n\narray([0.09259259, 0.22222222])\n\n\n\n#histogram\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\nRelated Questions:\n\nThe board of a large international company consists of 7 women and 3 men. If sex played no role, i.e.¬†if we assume that the probabilities of m/f were equal, how likely is it to see such asymmetric proportions?\nHow does this assessment change if the board consisted of 100 members and there would be 70 men and 30 women ?\n\n\n#let us run this many times !\nnSim = 1000\npHead_obs = np.zeros(nSim)\n\nfor i in range(nSim):\n  pHead_obs[i] = toss_biased_coin(pHead=0.5, n= 10)\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\n\n#how likely is it that one sees sth. \"as extreme as 7/3\"\nnp.mean(pHead_obs &gt;= 0.7) + np.mean(pHead_obs &lt;= 0.3)\n\nnp.float64(0.347)\n\n\n\n#let us run this many times !\nnSim = 10000\npHead_obs = np.zeros(nSim)\n\nfor i in range(nSim):\n  pHead_obs[i] = toss_biased_coin(pHead=0.5, n= 100)\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\n\nnp.mean(pHead_obs &gt;= 0.7) + np.mean(pHead_obs &lt;= 0.3)\n\nnp.float64(0.0)\n\n\n\n#What is the exact answer of this question given that you now know sigma !\ns = np.sqrt(p*(1-p)/100)\ns\n#how likely to be above 0.7\nzScore = (0.7-0.5)/s\nzScore\n\n#scipy.stat\n\nnp.float64(3.999999999999999)\n\n\n\ns = np.std(pHead_obs)\nprint(s)\n0.5 + 2*s\n0.5 - 2*s\n\n0.15604406428954612\n\n\nnp.float64(0.18791187142090776)\n\n\nSUPER IMPORTANT !! What is the stdev in the binomial distribution ??\nThe FAMOUS SQRT(N) LAW !!!\n\np= 0.5\n#Variance:\np*(1-p)\nnp.sqrt(p*(1-p)/10)\n\nnp.float64(0.15811388300841897)",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "CC_02-Intro2Pandas.html",
    "href": "CC_02-Intro2Pandas.html",
    "title": "13¬† Intro to pandas",
    "section": "",
    "text": "Introduction to pandas\nWhile numpy offers a lot of powerful numerical capabilities it lacks some of the necessary convenience and natural of handling data as we encounter them. For example, we would typically like to - mix data types (strings, numbers, categories, Boolean, ‚Ä¶) - refer to columns and rows by names - summarize and visualize data in efficient pivot style manners\nAll of the above (and more) can be achieved easily by extending the concept of an array (or a matrix) to a so called dataframe.",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "CC_02-Intro2Pandas.html#introduction-to-pandas",
    "href": "CC_02-Intro2Pandas.html#introduction-to-pandas",
    "title": "13¬† Intro to pandas",
    "section": "",
    "text": "Series\nFirst we review the concept of a one-dimensional array, which in pandas is referred to as a series. Take a look at chapter 5 in the Python for Data Analysis book.\nA Series is a one-dimensional array-like object containing a sequence of values (of similar types to NumPy types) of the same type and an associated array of data labels, called its index. The simplest Series is formed from only an array of data:\n\nobj = pd.Series([4, 7, -5, 3])\nobj\n\nOften, you‚Äôll want to create a Series with an index identifying each data point with a label:\n\nobj2 = pd.Series([4, 7, -5, 3], index=[\"d\", \"b\", \"a\", \"c\"])\nobj2\n\nCompared with NumPy arrays, you can use labels in the index when selecting single values or a set of values:\n\nprint(obj2[\"a\"])\nobj2[[\"c\", \"a\", \"d\"]]\n\n\n\nData Frames\nThere are many ways to construct a DataFrame, though one of the most common is from a dictionary of equal-length lists or NumPy arrays:\n\ndata = {\"state\": [\"Ohio\", \"Ohio\", \"Ohio\", \"Nevada\", \"Nevada\", \"Nevada\"],\n        \"year\": [2000, 2001, 2002, 2001, 2002, 2003],\n        \"pop\": [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nframe = pd.DataFrame(data)# creates a dataframe out of the data given!\nframe.head(3)\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\nyear\npop\n\n\n\n\n0\nOhio\n2000\n1.5\n\n\n1\nOhio\n2001\n1.7\n\n\n2\nOhio\n2002\n3.6\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe[2,1]#too bad\n\n\n#to get the full row: use the .iloc method\nframe.iloc[2]\nframe.iloc[2,1]\n\n2002\n\n\n\n\nSubsetting/Slicing\nWe first need to understand the attributes index (=rownames) and columns (= column names):\n\nframe.index\n\nRangeIndex(start=0, stop=6, step=1)\n\n\n\n#We can set a column as an index:\nframe2 = frame.set_index(\"year\")\nprint(frame2)\n#\n\n       state  pop\nyear             \n2000    Ohio  1.5\n2001    Ohio  1.7\n2002    Ohio  3.6\n2001  Nevada  2.4\n2002  Nevada  2.9\n2003  Nevada  3.2\n\n\n\n#it would be nice to access elements in the same fashion as numpy\n#frame2[1,1]\nframe[\"pop\"]\n\n0    1.5\n1    1.7\n2    3.6\n3    2.4\n4    2.9\n5    3.2\nName: pop, dtype: float64\n\n\n\nframe.pop\n\n&lt;bound method DataFrame.pop of     state  year  pop\n0    Ohio  2000  1.5\n1    Ohio  2001  1.7\n2    Ohio  2002  3.6\n3  Nevada  2001  2.4\n4  Nevada  2002  2.9\n5  Nevada  2003  3.2&gt;\n\n\n\n\nAsking for rows\nUnfortunately, we cannot use the simple [row,col] notation that we are used to from numpy arrays. (Try asking for frame[0,1])\nInstead, row subsetting can be achieved with either the .loc() or the .iloc() methods. The latter takes integers, the former indices:\n\nframe2.loc[2001] #note that I am not using quotes !!\n#at first glance this looks like I am asking for the row number 2001 !!\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\npop\n\n\nyear\n\n\n\n\n\n\n2001\nOhio\n1.7\n\n\n2001\nNevada\n2.4\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe2.loc[2001,\"state\"]\n\nyear\n2001      Ohio\n2001    Nevada\nName: state, dtype: object\n\n\n\nframe.iloc[0]#first row\n\nstate    Ohio\nyear     2000\npop       1.5\nName: 0, dtype: object\n\n\n\nframe3 = frame.set_index(\"state\", drop=False)\nprint(frame3)\n\n         state  year  pop\nstate                    \nOhio      Ohio  2000  1.5\nOhio      Ohio  2001  1.7\nOhio      Ohio  2002  3.6\nNevada  Nevada  2001  2.4\nNevada  Nevada  2002  2.9\nNevada  Nevada  2003  3.2\n\n\n\nframe3.loc[\"Ohio\"]\n\n\n  \n    \n      \n\n\n\n\n\n\nyear\npop\n\n\nstate\n\n\n\n\n\n\nOhio\n2000\n1.5\n\n\nOhio\n2001\n1.7\n\n\nOhio\n2002\n3.6\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe.iloc[2001]# this does not work because we do not have 2001 rows !\n\n\nframe.iloc[0,1]\n\n2000\n\n\n\n\nAsking for columns\n\n#The columns are also an index:\nframe.columns\n\nIndex(['state', 'year', 'pop'], dtype='object')\n\n\nA column in a DataFrame can be retrieved MUCH easier: as a Series either by dictionary-like notation or by using the dot attribute notation:\n\nframe[\"state\"]\n\n0      Ohio\n1      Ohio\n2      Ohio\n3    Nevada\n4    Nevada\n5    Nevada\nName: state, dtype: object\n\n\n\nframe.year#equivalent to frame[\"year\"]\n\n0    2000\n1    2001\n2    2002\n3    2001\n4    2002\n5    2003\nName: year, dtype: int64\n\n\n\n\nSummary Stats\nJust like in numpy you can compute sums, means, counts and many other summaries along rows and columns, by specifying the axis argument:\n\nheight = np.array([1.79, 1.85, 1.95, 1.55])\nweight = np.array([70, 80, 85, 65])\nhw = np.array([height, weight]).transpose()\n\nhw\n\narray([[ 1.79, 70.  ],\n       [ 1.85, 80.  ],\n       [ 1.95, 85.  ],\n       [ 1.55, 65.  ]])\n\n\n\ndf = pd.DataFrame(hw, columns = [\"height\", \"weight\"])\nprint(df)\n\n   height  weight\n0    1.79    70.0\n1    1.85    80.0\n2    1.95    85.0\n3    1.55    65.0\n\n\n\ndf = pd.DataFrame(hw , columns = [\"height\", \"weight\"],\n                  index = [\"Peter\", \"Matilda\", \"Bee\", \"Tom\"])\nprint(df)\n\n         height  weight\nPeter      1.79    70.0\nMatilda    1.85    80.0\nBee        1.95    85.0\nTom        1.55    65.0\n\n\nCan you extract:\n\nAll weights\nPeter‚Äôs height\nBee‚Äôs full info\nthe average height\nget all persons with height greater than 180cm\n\n\n#see Lab5\n\n\nprint(df.mean(axis=0))\nprint(df.mean(axis=1))# are these averages sensible ?\n\nheight     1.785\nweight    75.000\ndtype: float64\nPeter      35.895\nMatilda    40.925\nBee        43.475\nBee        33.275\ndtype: float64\n\n\nSome methods are neither reductions nor accumulations. describe is one such example, producing multiple summary statistics in one shot:\n\ndf.describe()\n\n\n\n\n\n\n\n\nheight\nweight\n\n\n\n\ncount\n4.000\n4.000000\n\n\nmean\n1.785\n75.000000\n\n\nstd\n0.170\n9.128709\n\n\nmin\n1.550\n65.000000\n\n\n25%\n1.730\n68.750000\n\n\n50%\n1.820\n75.000000\n\n\n75%\n1.875\n81.250000\n\n\nmax\n1.950\n85.000000",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "CC_02-Intro2Pandas.html#built-in-data-sets",
    "href": "CC_02-Intro2Pandas.html#built-in-data-sets",
    "title": "13¬† Intro to pandas",
    "section": "Built in data sets",
    "text": "Built in data sets",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "CC_02-Intro2Pandas.html#gapminder-data",
    "href": "CC_02-Intro2Pandas.html#gapminder-data",
    "title": "13¬† Intro to pandas",
    "section": "Gapminder Data",
    "text": "Gapminder Data\nhttps://www.gapminder.org/fw/world-health-chart/\nhttps://www.ted.com/talks/hans_rosling_the_best_stats_you_ve_ever_seen#t-241405\n\nYou‚Äôve never seen data presented like this. With the drama and urgency of a sportscaster, statistics guru Hans Rosling debunks myths about the so-called ‚Äúdeveloping world.‚Äù\n\n\n#!pip install gapminder\n#!conda install gapminder\nfrom gapminder import gapminder\n#gapminder.to_csv(\"../datasets/gapminder.csv\")\n\n\ngapminder\n\n\n  \n    \n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1699\nZimbabwe\nAfrica\n1987\n62.351\n9216418\n706.157306\n\n\n1700\nZimbabwe\nAfrica\n1992\n60.377\n10704340\n693.420786\n\n\n1701\nZimbabwe\nAfrica\n1997\n46.809\n11404948\n792.449960\n\n\n1702\nZimbabwe\nAfrica\n2002\n39.989\n11926563\n672.038623\n\n\n1703\nZimbabwe\nAfrica\n2007\n43.487\n12311143\n469.709298\n\n\n\n\n1704 rows √ó 6 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nrowNum = range(1, len(gapminder) + 1)\nrowNum[1703]\nlen(gapminder)\nN=gapminder.shape[0]\nN\n\n1704\n\n\n\n#gapminder.insert(0, \"rownum\", range(1, len(gapminder) + 1))\ngapminder.iloc[0:(7+1)]\n\n\n  \n    \n\n\n\n\n\n\nrownum\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\n1\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\n2\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\n3\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\n4\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\n5\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n5\n6\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.113360\n\n\n6\n7\nAfghanistan\nAsia\n1982\n39.854\n12881816\n978.011439\n\n\n7\n8\nAfghanistan\nAsia\n1987\n40.822\n13867957\n852.395945\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n#find the unique years\n\n#get the years:\ngapminder[\"year\"]\nnp.unique(gapminder.year)\n\narray([1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002,\n       2007])\n\n\n\n#get all rows with year 1952:\n#Hint:\n#either use Boolean subsetting\ngapminder[\"year\"] == 1952\ngapminder[gapminder[\"year\"] == 1952]\n#or use an index !!\n\n\n  \n    \n      \n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n12\nAlbania\nEurope\n1952\n55.230\n1282697\n1601.056136\n\n\n24\nAlgeria\nAfrica\n1952\n43.077\n9279525\n2449.008185\n\n\n36\nAngola\nAfrica\n1952\n30.015\n4232095\n3520.610273\n\n\n48\nArgentina\nAmericas\n1952\n62.485\n17876956\n5911.315053\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1644\nVietnam\nAsia\n1952\n40.412\n26246839\n605.066492\n\n\n1656\nWest Bank and Gaza\nAsia\n1952\n43.160\n1030585\n1515.592329\n\n\n1668\nYemen, Rep.\nAsia\n1952\n32.548\n4963829\n781.717576\n\n\n1680\nZambia\nAfrica\n1952\n42.038\n2672000\n1147.388831\n\n\n1692\nZimbabwe\nAfrica\n1952\n48.451\n3080907\n406.884115\n\n\n\n\n142 rows √ó 6 columns",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "CC_02-Intro2Pandas.html#handling-files",
    "href": "CC_02-Intro2Pandas.html#handling-files",
    "title": "13¬† Intro to pandas",
    "section": "Handling Files",
    "text": "Handling Files\nGet to know your friends\n\npd.read_csv\npd.read_table\npd.read_excel\n\n\n'''url = \"https://drive.google.com/file/d/1oIvCdN15UEwt4dCyjkArekHnTrivN43v/view?usp=share_link\"\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\ngapminder = pd.read_csv(url, index_col=0)\ngapminder.head()'''\n\n\n  \n    \n      \n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ngapminder.sort_values(by=\"year\").head()\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n528\nFrance\nEurope\n1952\n67.410\n42459667\n7029.809327\n\n\n540\nGabon\nAfrica\n1952\n37.003\n420702\n4293.476475\n\n\n1656\nWest Bank and Gaza\nAsia\n1952\n43.160\n1030585\n1515.592329\n\n\n552\nGambia\nAfrica\n1952\n30.000\n284320\n485.230659\n\n\n\n\n\n\n\n\n#How many countries?\nCtryCts = gapminder[\"country\"].value_counts()\nCtryCts\n#note the similarity with np.unique(..., return_counts=True)\n\nAfghanistan          12\nPakistan             12\nNew Zealand          12\nNicaragua            12\nNiger                12\n                     ..\nEritrea              12\nEquatorial Guinea    12\nEl Salvador          12\nEgypt                12\nZimbabwe             12\nName: country, Length: 142, dtype: int64\n\n\n\nfrom numpy.random import default_rng\nrng = default_rng()\nrng.choice(gapminder[\"country\"].unique(),2)\ngapminder[\"year\"].unique()\n\narray([1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002,\n       2007])\n\n\n\n#How meaningful are the column stats?\nprint(gapminder.mean(axis=0))\ngapminder.describe()\n\nyear         1.979500e+03\nlifeExp      5.947444e+01\npop          2.960121e+07\ngdpPercap    7.215327e+03\ndtype: float64\n\n\n/var/folders/h4/k73g68ds6xj791sf8cpmlxlc0000gn/T/ipykernel_33611/633466148.py:2: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  print(gapminder.mean(axis=0))\n\n\n\n\n\n\n\n\n\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\ncount\n1704.00000\n1704.000000\n1.704000e+03\n1704.000000\n\n\nmean\n1979.50000\n59.474439\n2.960121e+07\n7215.327081\n\n\nstd\n17.26533\n12.917107\n1.061579e+08\n9857.454543\n\n\nmin\n1952.00000\n23.599000\n6.001100e+04\n241.165876\n\n\n25%\n1965.75000\n48.198000\n2.793664e+06\n1202.060309\n\n\n50%\n1979.50000\n60.712500\n7.023596e+06\n3531.846988\n\n\n75%\n1993.25000\n70.845500\n1.958522e+07\n9325.462346\n\n\nmax\n2007.00000\n82.603000\n1.318683e+09\n113523.132900\n\n\n\n\n\n\n\nSort the index before you slice!!\nChoose a time range and specific countries\n\ngapminder2 = gapminder.set_index(\"year\").sort_index()\ngap1982_92 = gapminder2.loc[1982:1992].reset_index()\ngap1982_92 = gap1982_92.set_index(\"country\").sort_index()\ngap1982_92.loc[\"Afghanistan\":\"Albania\"]\n\n\n\n\n\n\n\n\nyear\ncontinent\nlifeExp\npop\ngdpPercap\n\n\ncountry\n\n\n\n\n\n\n\n\n\nAfghanistan\n1982\nAsia\n39.854\n12881816\n978.011439\n\n\nAfghanistan\n1987\nAsia\n40.822\n13867957\n852.395945\n\n\nAfghanistan\n1992\nAsia\n41.674\n16317921\n649.341395\n\n\nAlbania\n1992\nEurope\n71.581\n3326498\n2497.437901\n\n\nAlbania\n1987\nEurope\n72.000\n3075321\n3738.932735\n\n\nAlbania\n1982\nEurope\n70.420\n2780097\n3630.880722\n\n\n\n\n\n\n\n\ngap1982_92.loc[\"Afghanistan\":\"Albania\",\"lifeExp\"].mean()\n\n56.0585\n\n\n\nimport pandas as pd\n\ncars = pd.read_csv(\"https://raw.githubusercontent.com/markusloecher/DataScience-HWR/main/data/Auto.csv\")\n#d\ncars.shape\n\n(392, 10)\n\n\n\ncars.head()\n\n\n  \n    \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\nManufacturer\n\n\n\n\n0\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\nchevrolet\n\n\n1\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\nbuick\n\n\n2\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\nplymouth\n\n\n3\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\namc\n\n\n4\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\nford",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "CC_03-Dimensions.html",
    "href": "CC_03-Dimensions.html",
    "title": "14¬† Dimensions as ‚Äúaxis‚Äù",
    "section": "",
    "text": "Row-wise vs.¬†Column-wise operations",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Dimensions as \"axis\"</span>"
    ]
  },
  {
    "objectID": "CC_03-Dimensions.html#averages",
    "href": "CC_03-Dimensions.html#averages",
    "title": "14¬† Dimensions as ‚Äúaxis‚Äù",
    "section": "Averages",
    "text": "Averages\nHere‚Äôs a quick example showing how axis works for row-wise vs column-wise averages in pandas.\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'A': [1, 4, 7],\n    'B': [2, 5, 8],\n    'C': [3, 6, 9]\n})\n\n# Column-wise mean (down the rows)\ncol_means = df.mean(axis=0)\n\n# Row-wise mean (across the columns)\nrow_means = df.mean(axis=1)\n\nprint(\"Column-wise means:\")\nprint(col_means)\n\nprint(\"\\nRow-wise means:\")\nprint(row_means)\n\nOutput:\nColumn-wise means:\nA    4.0\nB    5.0\nC    6.0\ndtype: float64\n\nRow-wise means:\n0    2.0\n1    5.0\n2    8.0\ndtype: float64\n\naxis=0 ‚Üí operate down columns ‚Üí gives you column-wise results.\naxis=1 ‚Üí operate across columns ‚Üí gives you row-wise results.\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'A': [1, 4, 7],\n    'B': [2, 5, 8],\n    'C': [3, 6, 9]\n})\n\n# Column-wise mean (down the rows)\ncol_means = df.mean(axis=0)\n\n# Row-wise mean (across the columns)\nrow_means = df.mean(axis=1)\n\nprint(\"Column-wise means:\")\nprint(col_means)\n\nprint(\"\\nRow-wise means:\")\nprint(row_means)\n\nColumn-wise means:\nA    4.0\nB    5.0\nC    6.0\ndtype: float64\n\nRow-wise means:\n0    2.0\n1    5.0\n2    8.0\ndtype: float64\n\n\nThink about data for which it is genuinely meaningful to compute both row-wise and column-wise averages. Here are the some candidates you can actually load directly in Python or R, with explanations why both directions matter.",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Dimensions as \"axis\"</span>"
    ]
  },
  {
    "objectID": "CC_03-Dimensions.html#horizontal-vs.-vertical-concatenations",
    "href": "CC_03-Dimensions.html#horizontal-vs.-vertical-concatenations",
    "title": "14¬† Dimensions as ‚Äúaxis‚Äù",
    "section": "Horizontal vs.¬†vertical concatenations",
    "text": "Horizontal vs.¬†vertical concatenations\nhttps://wesmckinney.com/book/data-wrangling#prep_concat\nBelow is a minimal example showing horizontal vs.¬†vertical concatenation in pandas.",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Dimensions as \"axis\"</span>"
    ]
  },
  {
    "objectID": "CC_04-DateTimes.html",
    "href": "CC_04-DateTimes.html",
    "title": "15¬† Dates and Times in python",
    "section": "",
    "text": "Also see\nhttps://codeandstats.github.io/Introduction_Python_Lectures/IntroCoding_Lecture10.html\n\nfrom datetime import date, datetime\n\ntoday = date.today()\ntoday\n\ndatetime.date(2025, 12, 8)\n\n\n\ntoday + 1\n#unspecified as a dat range object\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/tmp/ipython-input-808742446.py in &lt;cell line: 0&gt;()\n----&gt; 1 today + 1\n\nTypeError: unsupported operand type(s) for +: 'datetime.date' and 'int'\n\n\n\n\ntoday.year\ntoday.month\ntoda\n\ndatetime.date(1, 1, 1)\n\n\n\n#Brexit February 1, 2020\n\nbrexit = date(2020, 2, 1)\ntoday - brexit\n\ndatetime.timedelta(days=2137)\n\n\nThe pandas way\n\nimport pandas as pd\n\npd.to_datetime(\"2025-12-08\")\n\nTimestamp('2025-12-08 00:00:00')\n\n\n\n#how many days are left until the shortest day of the year ??\nwinter_solstice = date(2025, 12, 21)\nwinter_solstice-today\n\ndatetime.timedelta(days=13)\n\n\nHow much longer until the days get longer ??\n\ndef days_until_winter_solstice():\n    today = date.today()\n    current_year = today.year\n\n    # Winter solstice is usually on Dec 21\n    solstice = date(current_year, 12, 21)\n\n    # If already past this year's solstice, compute for next year\n    if today &gt; solstice:\n        solstice = date(current_year + 1, 12, 21)\n\n    days_left = (solstice - today).days\n    print(f\"Only {days_left} days left until the shortest day of the year!\")\n    return days_left\n\n# Example:\ndays_left = days_until_winter_solstice()\n\nOnly 13 days left until the shortest day of the year!",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Dates and Times in python</span>"
    ]
  },
  {
    "objectID": "SolutionsExercises/01-Titanic_dpatsy.html",
    "href": "SolutionsExercises/01-Titanic_dpatsy.html",
    "title": "16¬† Titanic dpatsy",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nfrom patsy import dmatrices, build_design_matrices\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Load Titanic data\ndf = sns.load_dataset(\"titanic\").dropna()\n\n# Formula\nformula1 = \"\"\"\nsurvived ~ C(sex)*C(pclass) + C(embarked) + C(alone)\n           + age + fare + sibsp + parch +\n           + I(age**2) + I(fare**2)\n\"\"\"\n\nformula2 = \"\"\"\nsurvived ~ C(sex)*C(pclass) + C(embarked) + C(alone)\n           + age + fare + sibsp + parch + pclass\n           + I(age**2) + I(fare**2)\n           + (age + fare + sibsp + parch + pclass)**2\n\"\"\"\n#do NOT follow this order !! leads to data leakage!!\ny, X = dmatrices(formula1, data=df, return_type=\"dataframe\")\ntrain_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.2, random_state=123)\n\n# Split\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=123)\n\n# Train: learn + transform\ny_train, X_train = dmatrices(formula1, data=train_df, return_type=\"dataframe\")\nprint(X_train.shape)\nX_train.head()\n\n(145, 15)\n\n\n\n\n\n\n\n\n\nIntercept\nC(sex)[T.male]\nC(pclass)[T.2]\nC(pclass)[T.3]\nC(embarked)[T.Q]\nC(embarked)[T.S]\nC(alone)[T.True]\nC(sex)[T.male]:C(pclass)[T.2]\nC(sex)[T.male]:C(pclass)[T.3]\nage\nfare\nsibsp\nparch\nI(age ** 2)\nI(fare ** 2)\n\n\n\n\n700\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n18.0\n227.525\n1.0\n0.0\n324.0\n51767.625625\n\n\n748\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n19.0\n53.100\n1.0\n0.0\n361.0\n2819.610000\n\n\n544\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n50.0\n106.425\n1.0\n0.0\n2500.0\n11326.280625\n\n\n62\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n45.0\n83.475\n1.0\n0.0\n2025.0\n6968.075625\n\n\n327\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n36.0\n13.000\n0.0\n0.0\n1296.0\n169.000000\n\n\n\n\n\n\n\n\nimport numpy as np\nX.shape\ny = np.zeros(182)\ny[:5] = 1\ntrain_X, test_X, train_y, test_y = train_test_split(X,y, test_size=0.2, random_state=123)\nprint(np.mean(train_y))\n\n0.027586206896551724\n\n\n\ntrain_X, test_X, train_y_s, test_y = train_test_split(X,y, test_size=0.2, stratify=y)\nprint(np.mean(train_y_s))\n\n0.027586206896551724\n\n\n\n# Test: transform using stored design_info\ndesign_info = X_train.design_info\nX_test = build_design_matrices([design_info], test_df, return_type=\"dataframe\")[0]\ny_test = test_df[\"survived\"]\nprint(X_test.shape)\nX_test.head()\n\n(37, 15)\n\n\n\n\n\n\n\n\n\nIntercept\nC(sex)[T.male]\nC(pclass)[T.2]\nC(pclass)[T.3]\nC(embarked)[T.Q]\nC(embarked)[T.S]\nC(alone)[T.True]\nC(sex)[T.male]:C(pclass)[T.2]\nC(sex)[T.male]:C(pclass)[T.3]\nage\nfare\nsibsp\nparch\nI(age ** 2)\nI(fare ** 2)\n\n\n\n\n806\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n39.0\n0.0000\n0.0\n0.0\n1521.0\n0.000000\n\n\n516\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n34.0\n10.5000\n0.0\n0.0\n1156.0\n110.250000\n\n\n248\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n37.0\n52.5542\n1.0\n1.0\n1369.0\n2761.943938\n\n\n11\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n58.0\n26.5500\n0.0\n0.0\n3364.0\n704.902500\n\n\n853\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n16.0\n39.4000\n0.0\n1.0\n256.0\n1552.360000\n\n\n\n\n\n\n\n\n# Fit a linear regression\n# Eveb though we all know deep inside that this is not a good model !!\n\nmodel = LinearRegression().fit(X_train, y_train)\n\nprint(\"Train R¬≤:\", model.score(X_train, y_train))\nprint(\"Test  R¬≤:\", model.score(X_test, y_test))\nprint(\"Num features:\", X_train.shape[1])\n\nTrain R¬≤: 0.4224044585557233\nTest  R¬≤: -0.20543988189216833\nNum features: 15",
    "crumbs": [
      "Solutions to Exercises",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Titanic dpatsy</span>"
    ]
  },
  {
    "objectID": "SolutionsExercises/02-Target_Encoding_kaggle.html",
    "href": "SolutionsExercises/02-Target_Encoding_kaggle.html",
    "title": "17¬† Target Encoding",
    "section": "",
    "text": "Load the small kagggle data\nImpute missing values for the categorical column.\nApply target encoding with smoothing and CV-safety.\nFit a linear regression on numerical features + target-encoded feature.\nReport R¬≤.\n\nWe‚Äôll use pandas groupbyfunctions for target encoding:\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ndf = pd.read_csv('https://raw.githubusercontent.com/codeandstats/DataScience_Tutorial/refs/heads/main/data/train_small.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nid\ntemp_outdoor\nhumidity\ninsulation\nheating_type\noccupancy\nfloor_area\nwall_thickness\nroof_age\nwindow_age\nbasement_depth\ngarden_size\nsolar_irradiance\nwind_exposure\nair_quality_index\nstreet_noise_db\nconstruction_year\nenergy_consumption\n\n\n\n\n0\n10650\n14.539571\nNaN\nmedium\ngas\n2\nNaN\n19.250735\n10.989307\n12.160120\n1.729808\n6.959375\n111.543136\n7.267778\n64.318067\n70.249003\n1984\n11.881250\n\n\n1\n2041\n-3.100954\n46.081049\nhigh\nelectric\n1\n87.572031\n18.350328\n17.919226\n16.787705\n3.073186\n0.000000\n93.602058\n9.979652\n46.757586\n53.697994\n2005\n6.508803\n\n\n2\n8668\n19.113873\n41.197906\nlow\nelectric\n5\n146.807567\n15.448155\n19.128304\n13.633392\n2.519892\n23.548927\n124.754506\n4.743144\n40.569975\n68.006558\n2019\n22.196774\n\n\n3\n1114\n20.033119\n83.102412\nlow\nheatpump\n4\n98.795254\n22.616765\n3.649585\n10.350284\n2.723889\n58.652426\n117.456445\n6.498325\n44.741417\n70.375131\n1975\n26.475491\n\n\n4\n13902\n11.324897\n58.924253\nlow\ngas\n5\nNaN\n21.785369\n12.913624\n16.892840\n2.277545\n41.546852\n112.299329\n2.781272\n55.432727\n64.486913\n1982\n24.578805\n\n\n\n\n\n\n\n\nnp.random.seed(42)\n\ny = df[\"energy_consumption\"]\nX = df.drop(columns = \"energy_consumption\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\n\n# -----------------------------\n# 2a. Impute missing categorical values\n# -----------------------------\n\ncat_cols = [\"insulation\",   \"heating_type\"]\nfor cat in cat_cols:\n  X_train[cat] = X_train[cat].fillna(\"__MISSING__\")\n  X_test[cat] = X_test[cat].fillna(\"__MISSING__\")\n\n#X_train\n\n\n# -----------------------------\n# 2a. Impute missing numerical values\n# -----------------------------\nnum_cols = X_train.drop(columns = cat_cols).columns.tolist()[1:]\nfor num in num_cols:\n  X_train[num] = X_train[num].fillna(X_train[num].mean())\n  X_test[num] = X_test[num].fillna(X_train[num].mean())\n\n\n# -----------------------------\n# 3. Manual target encoding\n# -----------------------------\n# Compute training category means\ncategory_means = {}\nfor cat in cat_cols:\n  category_means[cat] = {}\n  for cat_val in X_train[cat].unique():\n      category_means[cat][cat_val] = y_train[X_train[cat] == cat_val].mean()\ncategory_means\n\n{'insulation': {'medium': 20.136618404130296,\n  'high': 20.397723500701762,\n  'low': 20.757434779630888},\n 'heating_type': {'gas': 20.595937705865463,\n  'electric': 20.793531402585423,\n  'heatpump': 19.194368663812373}}\n\n\n\ncat+\"_te\"\n\n'heating_type_te'\n\n\n\n# Create target-encoded column in training set\nfor cat in cat_cols:\n  cat_te_train = []\n  for val in X_train[cat]:\n      cat_te_train.append(category_means[cat][val])\n\n  X_train[cat+\"_te\"] = cat_te_train\nX_train.head()\n\n\n\n\n\n\n\n\nid\ntemp_outdoor\nhumidity\ninsulation\nheating_type\noccupancy\nfloor_area\nwall_thickness\nroof_age\nwindow_age\nbasement_depth\ngarden_size\nsolar_irradiance\nwind_exposure\nair_quality_index\nstreet_noise_db\nconstruction_year\ninsulation_te\nheating_type_te\n\n\n\n\n1988\n327\n10.218900\n87.421650\nmedium\ngas\n3\n145.228717\n20.693007\n9.682532\n11.471663\n2.049166\n36.145855\n119.307116\n8.801288\n50.945316\n69.207335\n1981\n20.136618\n20.595938\n\n\n1969\n14236\n13.354197\n55.594930\nmedium\ngas\n3\n64.662861\n22.079126\n17.960203\n8.453288\n1.433280\n69.709399\n99.808137\n3.805065\n59.248720\n57.054178\n1970\n20.136618\n20.595938\n\n\n1368\n1162\n10.218900\n71.001771\nmedium\nelectric\n5\n171.387288\n19.013422\n13.977685\n7.886164\n2.385981\n30.010319\n81.574516\n5.176806\n47.599320\n47.231849\n1981\n20.136618\n20.793531\n\n\n840\n4351\n9.685168\n69.130154\nhigh\nheatpump\n5\n105.802513\n21.093154\n15.421337\n10.298256\n2.120558\n41.330226\n57.988314\n7.314081\n43.781749\n60.112373\n2016\n20.397724\n19.194369\n\n\n2214\n2760\n3.904204\n69.693708\nmedium\nheatpump\n4\n147.566633\n19.945204\n20.566943\n3.082664\n0.453332\n0.000000\n133.721511\n1.414210\n56.950178\n48.453110\n2000\n20.136618\n19.194369\n\n\n\n\n\n\n\n\n# Create target-encoded column in test set\nglobal_mean = y_train.mean()\nfor cat in cat_cols:\n  cat_te_test = []\n  for val in X_test[cat]:\n      cat_te_test.append(category_means[cat].get(val, global_mean))  # use global mean for unseen categories\n  X_test[cat+\"_te\"] = cat_te_test\nX_test.head()\n\n\n\n\n\n\n\n\nid\ntemp_outdoor\nhumidity\ninsulation\nheating_type\noccupancy\nfloor_area\nwall_thickness\nroof_age\nwindow_age\nbasement_depth\ngarden_size\nsolar_irradiance\nwind_exposure\nair_quality_index\nstreet_noise_db\nconstruction_year\ninsulation_te\nheating_type_te\n\n\n\n\n53\n13419\n10.606436\n85.510202\nlow\ngas\n3\n119.911249\n26.256041\n7.945219\n8.468899\n1.771369\n36.350311\n101.565574\n5.942758\n51.959592\n68.229898\n1961\n20.757435\n20.595938\n\n\n2391\n10140\n20.007721\n69.464751\nhigh\ngas\n5\n119.911249\n23.426615\n13.805261\n12.618491\n1.530409\n34.868587\n111.460804\n0.644825\n54.133422\n67.970909\n1969\n20.397724\n20.595938\n\n\n2310\n487\n13.936286\n45.414106\nhigh\nelectric\n5\n61.481062\n22.394250\n9.394539\n3.885724\n1.566648\n39.157305\n114.306529\n0.879033\n53.585548\n58.378970\n2007\n20.397724\n20.793531\n\n\n728\n1290\n-8.844763\n35.726184\nmedium\ngas\n1\n192.299705\n17.916447\n17.049300\n12.836285\n1.538130\n37.130478\n125.236632\n2.228402\n42.521395\n54.356426\n2013\n20.136618\n20.595938\n\n\n850\n17766\n8.619824\n49.154236\nhigh\nelectric\n2\n90.556393\n20.886543\n13.483405\n13.060809\n2.152600\n3.983295\n102.714074\n6.049406\n56.904073\n63.882503\n2009\n20.397724\n20.793531\n\n\n\n\n\n\n\n\n# -----------------------------\n# 4. Fit regression on numerical + target-encoded feature\n# -----------------------------\n#X_train_num = X_train.drop(columns = cat_cols)\n#X_test_num = X_test.drop(columns = cat_cols)\nfeatures = num_cols + [cat+\"_te\" for cat in cat_cols]\n\nreg = LinearRegression()\nreg.fit(X_train[features], y_train)\n\n# -----------------------------\n# 5. Predict and evaluate\n# -----------------------------\ny_pred = reg.predict(X_train[features])\nr2 = r2_score(y_train, y_pred)\nprint(\"R¬≤ on train set:\", r2)\n\ny_pred = reg.predict(X_test[features])\nr2 = r2_score(y_test, y_pred)\nprint(\"R¬≤ on test set:\", r2)\n\nR¬≤ on train set: 0.7078327956103337\nR¬≤ on test set: 0.7472543478181957\n\n\n\nVersion with library category_encoders (a convenient library for target encoding with smoothing)\n\n!pip install category_encoders\n\nRequirement already satisfied: category_encoders in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (2.9.0)\nRequirement already satisfied: numpy&gt;=1.14.0 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from category_encoders) (1.26.1)\nRequirement already satisfied: pandas&gt;=1.0.5 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from category_encoders) (2.1.2)\nRequirement already satisfied: patsy&gt;=0.5.1 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from category_encoders) (1.0.1)\nRequirement already satisfied: scikit-learn&gt;=1.6.0 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from category_encoders) (1.7.2)\nRequirement already satisfied: scipy&gt;=1.0.0 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from category_encoders) (1.11.3)\nRequirement already satisfied: statsmodels&gt;=0.9.0 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from category_encoders) (0.14.5)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from pandas&gt;=1.0.5-&gt;category_encoders) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from pandas&gt;=1.0.5-&gt;category_encoders) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from pandas&gt;=1.0.5-&gt;category_encoders) (2023.3)\nRequirement already satisfied: six&gt;=1.5 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.0.5-&gt;category_encoders) (1.16.0)\nRequirement already satisfied: joblib&gt;=1.2.0 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from scikit-learn&gt;=1.6.0-&gt;category_encoders) (1.3.2)\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from scikit-learn&gt;=1.6.0-&gt;category_encoders) (3.2.0)\nRequirement already satisfied: packaging&gt;=21.3 in /Users/loecherm/.virtualenvs/venv312/lib/python3.12/site-packages (from statsmodels&gt;=0.9.0-&gt;category_encoders) (23.2)\n\n[notice] A new release of pip is available: 25.1.1 -&gt; 25.3\n[notice] To update, run: pip install --upgrade pip\n\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport category_encoders as ce\n\nnp.random.seed(42)\n\n# -----------------------------\n# 1. Create small synthetic data\n# -----------------------------\ndata = {\n    \"num1\": np.random.normal(size=20),\n    \"num2\": np.random.normal(size=20),\n    \"cat\": ['A', 'B', 'C', 'A', np.nan, 'B', 'C', 'D', 'D', np.nan,\n            'A', 'C', 'B', 'D', 'E', np.nan, 'E', 'B', 'C', 'A'],\n    \"y\": np.random.normal(size=20)\n}\n\ndf = pd.DataFrame(data)\n\n# Split into train/test\nX = df[[\"num1\", \"num2\", \"cat\"]]\ny = df[\"y\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# -----------------------------\n# 2. Build pipeline\n# -----------------------------\n# Impute missing categorical values with constant \"__MISSING__\"\ncat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"__MISSING__\")\n\n# Target encoding with smoothing\n# smoothing parameter alpha=5 (adjustable)\ntarget_encoder = ce.TargetEncoder(cols=[\"cat\"], smoothing=5)\n\n# Regression\nreg = LinearRegression()\n\n# Full pipeline\npipeline = Pipeline([\n    (\"imputer\", cat_imputer),\n    (\"te\", target_encoder),\n    (\"reg\", reg)\n])\n\n# -----------------------------\n# 3. Fit pipeline on training data\n# -----------------------------\npipeline.fit(X_train, y_train)\n\n# -----------------------------\n# 4. Predict and evaluate\n# -----------------------------\ny_pred = pipeline.predict(X_test)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"R¬≤ on test set:\", r2)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[11], line 53\n     44 pipeline = Pipeline([\n     45     (\"imputer\", cat_imputer),\n     46     (\"te\", target_encoder),\n     47     (\"reg\", reg)\n     48 ])\n     50 # -----------------------------\n     51 # 3. Fit pipeline on training data\n     52 # -----------------------------\n---&gt; 53 pipeline.fit(X_train, y_train)\n     55 # -----------------------------\n     56 # 4. Predict and evaluate\n     57 # -----------------------------\n     58 y_pred = pipeline.predict(X_test)\n\nFile ~/.virtualenvs/venv312/lib/python3.12/site-packages/sklearn/base.py:1365, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)\n   1358     estimator._validate_params()\n   1360 with config_context(\n   1361     skip_parameter_validation=(\n   1362         prefer_skip_nested_validation or global_skip_validation\n   1363     )\n   1364 ):\n-&gt; 1365     return fit_method(estimator, *args, **kwargs)\n\nFile ~/.virtualenvs/venv312/lib/python3.12/site-packages/sklearn/pipeline.py:655, in Pipeline.fit(self, X, y, **params)\n    648     raise ValueError(\n    649         \"The `transform_input` parameter can only be set if metadata \"\n    650         \"routing is enabled. You can enable metadata routing using \"\n    651         \"`sklearn.set_config(enable_metadata_routing=True)`.\"\n    652     )\n    654 routed_params = self._check_method_params(method=\"fit\", props=params)\n--&gt; 655 Xt = self._fit(X, y, routed_params, raw_params=params)\n    656 with _print_elapsed_time(\"Pipeline\", self._log_message(len(self.steps) - 1)):\n    657     if self._final_estimator != \"passthrough\":\n\nFile ~/.virtualenvs/venv312/lib/python3.12/site-packages/sklearn/pipeline.py:589, in Pipeline._fit(self, X, y, routed_params, raw_params)\n    582 # Fit or load from cache the current transformer\n    583 step_params = self._get_metadata_for_step(\n    584     step_idx=step_idx,\n    585     step_params=routed_params[name],\n    586     all_params=raw_params,\n    587 )\n--&gt; 589 X, fitted_transformer = fit_transform_one_cached(\n    590     cloned_transformer,\n    591     X,\n    592     y,\n    593     weight=None,\n    594     message_clsname=\"Pipeline\",\n    595     message=self._log_message(step_idx),\n    596     params=step_params,\n    597 )\n    598 # Replace the transformer of the step with the fitted\n    599 # transformer. This is necessary when loading the transformer\n    600 # from the cache.\n    601 self.steps[step_idx] = (name, fitted_transformer)\n\nFile ~/.virtualenvs/venv312/lib/python3.12/site-packages/joblib/memory.py:353, in NotMemorizedFunc.__call__(self, *args, **kwargs)\n    352 def __call__(self, *args, **kwargs):\n--&gt; 353     return self.func(*args, **kwargs)\n\nFile ~/.virtualenvs/venv312/lib/python3.12/site-packages/sklearn/pipeline.py:1540, in _fit_transform_one(transformer, X, y, weight, message_clsname, message, params)\n   1538 with _print_elapsed_time(message_clsname, message):\n   1539     if hasattr(transformer, \"fit_transform\"):\n-&gt; 1540         res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n   1541     else:\n   1542         res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n   1543             X, **params.get(\"transform\", {})\n   1544         )\n\nFile ~/.virtualenvs/venv312/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316, in _wrap_method_output.&lt;locals&gt;.wrapped(self, X, *args, **kwargs)\n    314 @wraps(f)\n    315 def wrapped(self, X, *args, **kwargs):\n--&gt; 316     data_to_wrap = f(self, X, *args, **kwargs)\n    317     if isinstance(data_to_wrap, tuple):\n    318         # only wrap the first output for cross decomposition\n    319         return_tuple = (\n    320             _wrap_data_with_container(method, data_to_wrap[0], X, self),\n    321             *data_to_wrap[1:],\n    322         )\n\nFile ~/.virtualenvs/venv312/lib/python3.12/site-packages/category_encoders/utils.py:643, in SupervisedTransformerMixin.fit_transform(self, X, y, **fit_params)\n    641 if y is None:\n    642     raise TypeError('fit_transform() missing argument: ' 'y' '')\n--&gt; 643 return self.fit(X, y, **fit_params).transform(X, y)\n\nFile ~/.virtualenvs/venv312/lib/python3.12/site-packages/category_encoders/utils.py:468, in BaseEncoder.fit(self, X, y, **kwargs)\n    465 self._determine_fit_columns(X)\n    467 if not set(self.cols).issubset(X.columns):\n--&gt; 468     raise ValueError('X does not contain the columns listed in cols')\n    470 if self.handle_missing == 'error':\n    471     if X[self.cols].isna().any().any():\n\nValueError: X does not contain the columns listed in cols\n\n\n\n\n\n\nNotes\n\nImputation: We fill missing categories with \"__MISSING__\".\nTarget encoding: category_encoders.TargetEncoder automatically computes smoothed means for each category.\nPipeline ensures no leakage ‚Äî target encoding is fit only on the training data and applied to test data safely.\nNumerical features (num1, num2) are used directly.\nThe R¬≤ output gives model performance on the test set.",
    "crumbs": [
      "Solutions to Exercises",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Target Encoding</span>"
    ]
  },
  {
    "objectID": "SolutionsExercises/03-CrossValidation_kaggle.html",
    "href": "SolutionsExercises/03-CrossValidation_kaggle.html",
    "title": "18¬† 1. Load data",
    "section": "",
    "text": "Cross Validation",
    "crumbs": [
      "Solutions to Exercises",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "SolutionsExercises/03-CrossValidation_kaggle.html#cross-validation",
    "href": "SolutionsExercises/03-CrossValidation_kaggle.html#cross-validation",
    "title": "18¬† 1. Load data",
    "section": "",
    "text": "Exercise 1 ‚Äî Cross-Validation without a Pipeline\n\nStarter Code\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n\n# 2. WRONG: manual scaling before CV (leakage!)\n# This warning, while strictly speaking is true its effect in practice is almost zero !!\n# other data lkeakage issues shoukld be taken seriously but standardscaling is almost harmless\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 3. Regression model\nmodel = LinearRegression()\n\n# 4. Cross-validation\nscores = cross_val_score(model, X_scaled, y, cv=5,\n                         scoring=\"neg_mean_squared_error\")\n\nprint(\"Fold MSEs:\", np.round(-scores, 3))\n#so strictly speaking we neeed to weigh the folds\n#\nprint(\"Mean MSE:\", -scores.mean())\n\nFold MSEs: [0.485 0.622 0.646 0.543 0.495]\nMean MSE: 0.5582901717686555\n\n\n\nX_scaled.shape\n20638/5\n#len(X_scaled)/5\nscores.\n\n4127.6\n\n\n\nnp.mean([0.485, 0.622, 0.646, 0.543, 0.495])\n\nnp.float64(0.5582)",
    "crumbs": [
      "Solutions to Exercises",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "SolutionsExercises/03-CrossValidation_kaggle.html#cross-validation-with-a-pipeline",
    "href": "SolutionsExercises/03-CrossValidation_kaggle.html#cross-validation-with-a-pipeline",
    "title": "18¬† 1. Load data",
    "section": "Cross-Validation with a Pipeline",
    "text": "Cross-Validation with a Pipeline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n\n# 2. Pipeline\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"linreg\", LinearRegression())\n])\n\n# 3. CV evaluation\nscores = cross_val_score(pipe, X, y, cv=5,\n                         scoring=\"neg_mean_squared_error\")\n\nprint(\"Fold MSEs:\", np.round(-scores, 3))\nprint(\"Mean MSE:\", -scores.mean())\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n/tmp/ipython-input-3733852175.py in &lt;cell line: 0&gt;()\n      1 # 2. Pipeline\n----&gt; 2 pipe = Pipeline([\n      3     (\"scaler\", StandardScaler()),\n      4     (\"linreg\", LinearRegression())\n      5 ])\n\nNameError: name 'Pipeline' is not defined",
    "crumbs": [
      "Solutions to Exercises",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "SolutionsExercises/03-CrossValidation_kaggle.html#tasks",
    "href": "SolutionsExercises/03-CrossValidation_kaggle.html#tasks",
    "title": "18¬† 1. Load data",
    "section": "Tasks",
    "text": "Tasks\nPerform 10-fold CV on our small kaggle data !\n\nimport pandas as pd\n# read in a small version of our kaggle train.csv:\n\ndf = pd.read_csv('https://raw.githubusercontent.com/codeandstats/DataScience_Tutorial/refs/heads/main/data/train_small.csv')\ndf.head()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2500 entries, 0 to 2499\nData columns (total 18 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   id                  2500 non-null   int64  \n 1   temp_outdoor        2167 non-null   float64\n 2   humidity            2047 non-null   float64\n 3   insulation          2500 non-null   object \n 4   heating_type        2500 non-null   object \n 5   occupancy           2500 non-null   int64  \n 6   floor_area          2329 non-null   float64\n 7   wall_thickness      2500 non-null   float64\n 8   roof_age            2500 non-null   float64\n 9   window_age          2500 non-null   float64\n 10  basement_depth      2500 non-null   float64\n 11  garden_size         2500 non-null   float64\n 12  solar_irradiance    2500 non-null   float64\n 13  wind_exposure       2500 non-null   float64\n 14  air_quality_index   2500 non-null   float64\n 15  street_noise_db     2500 non-null   float64\n 16  construction_year   2500 non-null   int64  \n 17  energy_consumption  2500 non-null   float64\ndtypes: float64(13), int64(3), object(2)\nmemory usage: 351.7+ KB\n\n\n\nfrom patsy import dmatrices\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n# Example df\n# df = pd.read_csv(\"your_data.csv\")\n\nnum_cols = df.select_dtypes(include=\"number\").columns.tolist()\ncat_cols = df.select_dtypes(exclude=\"number\").columns.tolist()\nall_cols = num_cols + cat_cols\n\n# Impute\nimputer = ColumnTransformer(\n    transformers=[\n        (\"num\", SimpleImputer(strategy=\"median\"), num_cols),\n        (\"cat\", SimpleImputer(strategy=\"most_frequent\"), cat_cols)\n    ],\n    remainder=\"drop\"\n)\n\nX_imp = imputer.fit_transform(df)\ndf_imputed = pd.DataFrame(X_imp, columns=all_cols)\n\n# Convert categorical columns to str\nfor col in cat_cols:\n    df_imputed[col] = df_imputed[col].astype(str)\n\nfor col in num_cols:\n    df_imputed[col] = pd.to_numeric(df_imputed[col])\n\ndf_imputed.head()\n#df_imputed.info()\n\n\n  \n    \n\n\n\n\n\n\nid\ntemp_outdoor\nhumidity\noccupancy\nfloor_area\nwall_thickness\nroof_age\nwindow_age\nbasement_depth\ngarden_size\nsolar_irradiance\nwind_exposure\nair_quality_index\nstreet_noise_db\nconstruction_year\nenergy_consumption\ninsulation\nheating_type\n\n\n\n\n0\n10650.0\n14.539571\n54.861686\n2.0\n119.144884\n19.250735\n10.989307\n12.160120\n1.729808\n6.959375\n111.543136\n7.267778\n64.318067\n70.249003\n1984.0\n11.881250\nmedium\ngas\n\n\n1\n2041.0\n-3.100954\n46.081049\n1.0\n87.572031\n18.350328\n17.919226\n16.787705\n3.073186\n0.000000\n93.602058\n9.979652\n46.757586\n53.697994\n2005.0\n6.508803\nhigh\nelectric\n\n\n2\n8668.0\n19.113873\n41.197906\n5.0\n146.807567\n15.448155\n19.128304\n13.633392\n2.519892\n23.548927\n124.754506\n4.743144\n40.569975\n68.006558\n2019.0\n22.196774\nlow\nelectric\n\n\n3\n1114.0\n20.033119\n83.102412\n4.0\n98.795254\n22.616765\n3.649585\n10.350284\n2.723889\n58.652426\n117.456445\n6.498325\n44.741417\n70.375131\n1975.0\n26.475491\nlow\nheatpump\n\n\n4\n13902.0\n11.324897\n58.924253\n5.0\n119.144884\n21.785369\n12.913624\n16.892840\n2.277545\n41.546852\n112.299329\n2.781272\n55.432727\n64.486913\n1982.0\n24.578805\nlow\ngas\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n# Patsy\nall_cols.remove(\"energy_consumption\")\nformula1 = \"energy_consumption ~ \" + \" + \".join(all_cols)  # replace 'target' with your y column\n\nformula1\n\n\n'energy_consumption ~ id + temp_outdoor + humidity + occupancy + floor_area + wall_thickness + roof_age + window_age + basement_depth + garden_size + solar_irradiance + wind_exposure + air_quality_index + street_noise_db + construction_year + insulation + heating_type'\n\n\n\ny, X = dmatrices(formula1, data=df_imputed, return_type=\"dataframe\")\n\nX.shape\nX.head()\n\n\n  \n    \n\n\n\n\n\n\nIntercept\ninsulation[T.low]\ninsulation[T.medium]\nheating_type[T.gas]\nheating_type[T.heatpump]\nid\ntemp_outdoor\nhumidity\noccupancy\nfloor_area\nwall_thickness\nroof_age\nwindow_age\nbasement_depth\ngarden_size\nsolar_irradiance\nwind_exposure\nair_quality_index\nstreet_noise_db\nconstruction_year\n\n\n\n\n0\n1.0\n0.0\n1.0\n1.0\n0.0\n10650.0\n14.539571\n54.861686\n2.0\n119.144884\n19.250735\n10.989307\n12.160120\n1.729808\n6.959375\n111.543136\n7.267778\n64.318067\n70.249003\n1984.0\n\n\n1\n1.0\n0.0\n0.0\n0.0\n0.0\n2041.0\n-3.100954\n46.081049\n1.0\n87.572031\n18.350328\n17.919226\n16.787705\n3.073186\n0.000000\n93.602058\n9.979652\n46.757586\n53.697994\n2005.0\n\n\n2\n1.0\n1.0\n0.0\n0.0\n0.0\n8668.0\n19.113873\n41.197906\n5.0\n146.807567\n15.448155\n19.128304\n13.633392\n2.519892\n23.548927\n124.754506\n4.743144\n40.569975\n68.006558\n2019.0\n\n\n3\n1.0\n1.0\n0.0\n0.0\n1.0\n1114.0\n20.033119\n83.102412\n4.0\n98.795254\n22.616765\n3.649585\n10.350284\n2.723889\n58.652426\n117.456445\n6.498325\n44.741417\n70.375131\n1975.0\n\n\n4\n1.0\n1.0\n0.0\n1.0\n0.0\n13902.0\n11.324897\n58.924253\n5.0\n119.144884\n21.785369\n12.913624\n16.892840\n2.277545\n41.546852\n112.299329\n2.781272\n55.432727\n64.486913\n1982.0\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\ndef get_cv_scores( X, y, cv = 10):\n    model = LinearRegression()\n    scores = cross_val_score(model, X, y, cv=cv,\n                         scoring=\"neg_mean_squared_error\")\n    print(\"Fold MSEs:\", np.round(-scores, 3))\n   #print(\"Mean MSE:\", -scores.mean())\n    return -scores.mean()\n\nMSE = get_cv_scores(X, y)\nprint(np.sqrt(MSE))\n\nFold MSEs: [16.648 13.441 14.511 14.946 11.961 15.645 14.263 16.148 17.097 14.089]\n3.856806059742793\n\n\n\ndf.describe()\n\n\n  \n    \n\n\n\n\n\n\nid\ntemp_outdoor\nhumidity\noccupancy\nfloor_area\nwall_thickness\nroof_age\nwindow_age\nbasement_depth\ngarden_size\nsolar_irradiance\nwind_exposure\nair_quality_index\nstreet_noise_db\nconstruction_year\nenergy_consumption\n\n\n\n\ncount\n2500.000000\n2167.000000\n2047.000000\n2500.000000\n2329.000000\n2500.000000\n2500.000000\n2500.000000\n2500.000000\n2500.000000\n2500.000000\n2500.000000\n2500.000000\n2500.000000\n2500.000000\n2500.000000\n\n\nmean\n9942.823200\n10.127429\n55.100791\n3.030000\n120.007200\n20.126984\n15.064165\n9.949649\n1.997617\n29.967394\n100.471542\n4.996759\n50.037390\n59.812153\n1989.405200\n20.269234\n\n\nstd\n5715.312135\n8.032136\n20.029100\n1.413187\n38.812370\n3.090533\n4.964830\n3.909825\n0.497021\n18.558240\n24.357746\n2.878709\n9.822960\n7.009734\n17.364885\n7.269667\n\n\nmin\n3.000000\n-17.269697\n20.041767\n1.000000\n40.000000\n9.894401\n-1.813256\n-2.488609\n0.196149\n0.000000\n19.401378\n0.000038\n18.267999\n36.060759\n1960.000000\n-9.612926\n\n\n25%\n4997.000000\n4.785499\n38.452879\n2.000000\n91.738291\n17.974027\n11.632397\n7.278004\n1.663767\n16.117557\n84.116128\n2.522654\n43.379340\n54.998895\n1974.000000\n15.044531\n\n\n50%\n9799.000000\n10.278319\n54.861686\n3.000000\n119.144884\n20.147357\n15.132428\n9.962562\n1.987211\n29.599083\n99.675923\n4.967903\n49.962052\n59.804643\n1989.000000\n20.175018\n\n\n75%\n14878.500000\n15.405637\n72.068323\n4.000000\n146.728336\n22.239657\n18.470530\n12.602130\n2.341180\n42.976300\n117.302179\n7.478058\n56.649892\n64.719565\n2005.000000\n25.553412\n\n\nmax\n19998.000000\n39.460076\n89.989108\n5.000000\n245.601138\n34.979828\n31.906336\n24.829253\n3.787984\n100.278477\n184.794535\n9.996766\n87.096641\n82.047848\n2019.000000\n41.445096\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nLet us try various model complexities:\n\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\n\npoly = PolynomialFeatures(degree=3, include_bias=False)\nX_poly = poly.fit_transform(X)\nX_poly.shape\n\n(2500, 1770)\n\n\n\n\nget_cv_scores(X_poly[:,0:1500], y)\n\nFold MSEs: [25.3   26.812 21.28  22.507 21.904 21.761 26.649 22.913 23.978 21.159]\n\n\nnp.float64(23.426260291416394)",
    "crumbs": [
      "Solutions to Exercises",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "SolutionsExercises/03-CrossValidation_kaggle.html#logistic-regression",
    "href": "SolutionsExercises/03-CrossValidation_kaggle.html#logistic-regression",
    "title": "18¬† 1. Load data",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nfrom sklearn.datasets import load_breast_cancer\ndata = load_breast_cancer(as_frame=True)\ndf = data.frame\nX = df.drop(columns=[\"target\"])\ny = df[\"target\"]\n\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport pandas as pd\n\n# 1. Load dataset\ndata = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target, name=\"target\")\nX\n\n\n  \n    \n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows √ó 30 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n\n\n# 2. Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=0, stratify=y\n)\n\n# 3. Fit Logistic Regression\nmodel = LogisticRegression(max_iter=5000)  # dataset is large -&gt; increase max_iter\nmodel.fit(X_train, y_train)\n\n\ncoefs = model.coef_[0]\ncoefs\n\narray([ 8.06561162e-01,  2.07275595e-01, -3.21190501e-01,  2.74613469e-02,\n       -1.20691563e-01, -2.06171773e-01, -4.49946036e-01, -2.38232986e-01,\n       -1.70767162e-01, -2.82744031e-02, -1.44245286e-01,  6.99073782e-01,\n        7.54739050e-01, -1.39404710e-01, -1.73683355e-02,  2.58745656e-03,\n       -7.45527590e-02, -3.40263584e-02, -2.00272642e-02,  6.01715516e-04,\n       -1.95228935e-01, -4.67943429e-01, -1.74646239e-01, -7.03230845e-03,\n       -2.55217829e-01, -6.48115154e-01, -1.17334354e+00, -4.99474610e-01,\n       -4.12014010e-01, -9.20992458e-02])\n\n\n\n# 4. Predictions\ny_pred = model.predict(X_test)\n\ny_pred\n\narray([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0])\n\n\n\n\n\n# 5. Evaluation\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n#print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\n\n\nAccuracy: 0.9370629370629371\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.91      0.92      0.92        53\n           1       0.96      0.94      0.95        90\n\n    accuracy                           0.94       143\n   macro avg       0.93      0.93      0.93       143\nweighted avg       0.94      0.94      0.94       143",
    "crumbs": [
      "Solutions to Exercises",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "SolutionsExercises/03-CrossValidation_kaggle.html#task",
    "href": "SolutionsExercises/03-CrossValidation_kaggle.html#task",
    "title": "18¬† 1. Load data",
    "section": "Task:",
    "text": "Task:\nfit a logistic regression using only sex and pclass.\n\nimport seaborn as sns\n\n# 1. Load Titanic data\ndf = sns.load_dataset(\"titanic\")\n\n# Keep only the needed columns\ndf = df[[\"survived\", \"age\", \"pclass\"]].dropna()\ndf\n\n\n  \n    \n\n\n\n\n\n\nsurvived\nage\npclass\n\n\n\n\n0\n0\n22.0\n3\n\n\n1\n1\n38.0\n1\n\n\n2\n1\n26.0\n3\n\n\n3\n1\n35.0\n1\n\n\n4\n0\n35.0\n3\n\n\n...\n...\n...\n...\n\n\n885\n0\n39.0\n3\n\n\n886\n0\n27.0\n2\n\n\n887\n1\n19.0\n1\n\n\n889\n1\n26.0\n1\n\n\n890\n0\n32.0\n3\n\n\n\n\n714 rows √ó 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nmodel = LogisticRegression(max_iter=1000)  # dataset is large -&gt; increase max_iter\n\nmodel.fit(df[[ \"age\", \"pclass\"]], df[\"survived\"])\nprint(model.coef_)\nmodel.intercept_\n\n[[-0.04149665 -1.22653571]]\n\n\narray([3.532956])",
    "crumbs": [
      "Solutions to Exercises",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Cross Validation</span>"
    ]
  },
  {
    "objectID": "A-ourFunctions.html",
    "href": "A-ourFunctions.html",
    "title": "Appendix A ‚Äî Useful Functions",
    "section": "",
    "text": "Home Office Simulation\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef HomeOfficeSimulation(n = 20, n_weeks = 100, daysInWeek = 5,\n                         days_in_office = 2, seed=123,\n                         plotIt = True,\n                         verbose = False):\n  rng = np.random.default_rng(seed)\n\n  NumEmpInOffice = np.zeros(n+1, dtype = \"int\")#this is a 1D array of length n+1\n  for j in range(n_weeks):\n    weekdays = np.zeros(daysInWeek, dtype = \"int\")#this is a 1D array of length 5\n\n    for i in range(n):\n      empChoice = rng.choice(np.arange(1,daysInWeek+1),days_in_office, replace = False )\n      weekdays[empChoice -1 ] += 1#this is of length 5 !!\n\n    #NumEmpInOffice[weekdays] += 1 #for each of 5 elements I increment by 1\n    for w in weekdays:\n      NumEmpInOffice[w] += 1\n\n    if (verbose):\n      print(weekdays)\n      #print(NumEmpInOffice)\n      print(pd.Series(NumEmpInOffice, index=range(n + 1)).to_dict())\n\n    if np.sum(weekdays)!= 2*n:\n      print(\"sth. is wrong!\") #sanity check!\n  #we would like to return the probability !\n  #of course they have to add to one in total\n  prob = NumEmpInOffice/np.sum(NumEmpInOffice)\n  if plotIt:\n    plt.figure(figsize=(7, 3))\n    plt.bar(np.arange(n+1),prob)\n    plt.xlabel(\"Number of employees in the office\")\n    plt.ylabel(\"Probability\")\n    plt.title(f\"Distribution of employees in office (n={n})\")\n    plt.grid(alpha=0.3)\n    plt.show()\n  return\n  #return NumEmpInOffice/(n_weeks*5)#(n_weeks*daysInWeek)\nonesim=HomeOfficeSimulation(n = 20, n_weeks = 100, daysInWeek = 5, days_in_office = 2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "A-ourFunctions.html#power-calculation",
    "href": "A-ourFunctions.html#power-calculation",
    "title": "Appendix A ‚Äî Useful Functions",
    "section": "Power Calculation",
    "text": "Power Calculation\n\nfrom statsmodels.stats.power import NormalIndPower\nfrom statsmodels.stats.proportion import proportion_effectsize\n\ndef power_prop_test(n=None, p1=None, p2=None, power=None, sig_level=0.05):\n    effect_size = proportion_effectsize(p1, p2)\n    analysis = NormalIndPower()\n    return analysis.solve_power(effect_size=effect_size, nobs1=n, alpha=sig_level, power=power, ratio=1)\n\n\npower_prop_test(n=200, p1=0.5, p2=0.6)\n\nnp.float64(0.5214145419211704)\n\n\n\npower_prop_test(p1=0.5, p2=0.6, power=0.8)\n\n387.1677468578107",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "A-ourFunctions.html#permutation-tests",
    "href": "A-ourFunctions.html#permutation-tests",
    "title": "Appendix A ‚Äî Useful Functions",
    "section": "Permutation Tests",
    "text": "Permutation Tests\n\nimport numpy as np\n\n# Permutation tests\n\ndef permutation_sample(data1, data2):\n    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n\n    # Concatenate the data sets: data\n    data = np.concatenate((data1, data2))\n\n    # Permute the concatenated array: permuted_data\n    permuted_data = np.random.permutation(data)\n    #question: would you have been able to use anotehr random function that you already know ?\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2\n\ndef draw_perm_reps(data_1, data_2, size=100, func):\n    \"\"\"Generate multiple permutation replicates.\"\"\"\n\n    # Initialize array of replicates: perm_replicates\n    perm_replicates = np.empty(size)\n\n    for i in range(size):\n        # Generate permutation sample\n        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n\n        # Compute the test statistic\n        perm_replicates[i] = func(perm_sample_1, perm_sample_2)\n        #The difference in means is our \"test statistic\"\n        #perm_replicates[i] = np.mean(perm_sample_1) - np.mean(perm_sample_2)\n        #in this definition, could we just use np.mean for the function ??\n\n    return perm_replicates\n\ndef diff_of_means(data_1, data_2):\n    \"\"\"Difference in means of two arrays.\"\"\"\n\n    # The difference of means of data_1, data_2: diff\n    diff = np.mean(data_1) - np.mean(data_2)\n\n    return diff\n\ndef toss_biased_coin(p_head, n, n_experiments):\n    return np.random.binomial(n, p_head, size=n_experiments)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "A-ourFunctions.html#empirical-cumulative-distribution-function",
    "href": "A-ourFunctions.html#empirical-cumulative-distribution-function",
    "title": "Appendix A ‚Äî Useful Functions",
    "section": "Empirical Cumulative Distribution Function",
    "text": "Empirical Cumulative Distribution Function\n\ndef ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) / n\n\n    return x, y",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "B-DeeperDive-Pipes.html",
    "href": "B-DeeperDive-Pipes.html",
    "title": "Appendix B ‚Äî Dissecting Pipelines",
    "section": "",
    "text": "named_steps are simply the official way to access parts of a Pipeline or ColumnTransformer after fitting.\nLet me break it down clearly with tiny examples.\n\n\nüß© 1. named_steps ‚Äî Accessing steps inside a Pipeline\nSuppose you have:\npipe = Pipeline([\n    (\"preprocess\", preprocessor),\n    (\"model\", LinearRegression())\n])\nAfter fitting:\npipe.fit(X, y)\nYou can access individual steps like a Python dictionary:\npipe.named_steps[\"preprocess\"]      # returns the ColumnTransformer\npipe.named_steps[\"model\"]           # returns the LinearRegression\nWhy is this needed?\nTo get:\n\nmodel coefficients\nthe fitted OneHotEncoder\nthe transformed feature names\n\n\n\n\nüß© 2. named_transformers_ ‚Äî Accessing parts of a ColumnTransformer\nSuppose your ColumnTransformer is:\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\nInside the pipeline, the name \"cat\" refers to the encoder for categorical columns.\nAfter fitting:\npipe.fit(X, y)\nTo extract the fitted OneHotEncoder:\nohe = pipe.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\nNow ohe is an actual, fitted OneHotEncoder object.\nFrom here you can get feature names:\nohe.get_feature_names_out(categorical_cols)\n\n\n\nüß† Why the underscore?\nsklearn convention:\n\nBefore fitting: named_transformers (no underscore) ‚Üí stores unfitted transformers.\nAfter fitting: named_transformers_ (with underscore) ‚Üí stores fitted transformers.\n\nSame pattern as:\n\ncoef_\nintercept_\nlabels_\n\nThe _ suffix means learned during fitting.\n\n\n\nüì¶ Tiny Minimal Example to Make It Crystal Clear\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"color\": [\"red\", \"blue\", \"red\"],\n    \"size\": [1.2, 3.3, 2.1],\n    \"price\": [10, 15, 12]\n})\n\nX = df[[\"color\", \"size\"]]\ny = df[\"price\"]\n\npre = ColumnTransformer([\n    (\"cat\", OneHotEncoder(drop=\"first\"), [\"color\"]),\n    (\"num\", \"passthrough\", [\"size\"])\n])\n\npipe = Pipeline([\n    (\"preprocess\", pre),\n    (\"model\", LinearRegression())\n])\n\npipe.fit(X, y)\n\n# --- Accessing things ---\nct = pipe.named_steps[\"preprocess\"]               # the ColumnTransformer\nohe = ct.named_transformers_[\"cat\"]               # the fitted OneHotEncoder\n\nprint(\"One-hot categories:\", ohe.categories_)\nprint(\"Feature names:\", ohe.get_feature_names_out([\"color\"]))\n\n\n\nOne-hot categories: [array(['blue', 'red'], dtype=object)]\nFeature names: ['color_red']\n\n\n\npipe.named_steps[\"model\"].coef_\npipe[\"model\"].coef_\n\narray([-0.33333333,  2.22222222])\n\n\n\n\n\nüéØ Summary\n\n\n\n\n\n\n\nWhat you want\nSyntax\n\n\n\n\nGet a step inside a pipeline\npipe.named_steps[\"name\"]\n\n\nGet a transformer inside ColumnTransformer\npipe.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n\n\nGet OHE feature names\nohe.get_feature_names_out(cols)\n\n\n\n\nHow to get the full feature name extraction for the entire pipeline, including numeric + categorical + interactions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Dissecting Pipelines</span>"
    ]
  }
]