[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science in Python",
    "section": "",
    "text": "Preface\nData Science and Tutorial courses taught by Professor Markus Loecher (HWR Berlin).",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#united-notebooks",
    "href": "index.html#united-notebooks",
    "title": "Introduction to Data Science in Python",
    "section": "United Notebooks",
    "text": "United Notebooks\nIn order to organize our lecture materials more coherently and easier to navigate, I have consolidated both classes into one place.\nIn addition, I have imposed an approximate temporal ordering (modified by relevance and topic relations) which might help you.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-HomeOffice.html",
    "href": "01-HomeOffice.html",
    "title": "1¬† Home Office Simulation",
    "section": "",
    "text": "Skeleton code for desired function:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndef office_simulation(n=20, n_weeks=1000, seed=None, plot=True):\n    \"\"\"\n    Simulates office attendance given n employees each in-office 2 of 5 weekdays.\n\n    Parameters\n    ----------\n    n : int\n        Number of employees.\n    n_weeks : int\n        Number of simulated weeks.\n    seed : int or None\n        Random seed for reproducibility.\n    plot : bool\n        Whether to plot the resulting distribution.\n\n    Returns\n    -------\n    dist : array\n        Empirical distribution of #employees present on a random day.\n    overlap_prob : float\n        Probability that two specific employees overlap &gt;=2 days per week.\n    \"\"\"\n\n    rng = np.random.default_rng(seed)\n    days = np.arange(5)\n\n    day_counts = []\n    overlap_count = 0\n\n    for _ in range(n_weeks):\n        # Each employee randomly chooses 2 distinct office days\n\n\n        # Count how many employees come in on each day\n\n\n        # Overlap between two particular employees (say #0 and #1)\n\n\n    # Distribution of employees per day\n\n\n    if plot:\n        plt.figure(figsize=(7, 4))\n\n        plt.xlabel(\"Number of employees in the office\")\n        plt.ylabel(\"Probability\")\n        plt.title(f\"Distribution of employees in office (n={n})\")\n        plt.grid(alpha=0.3)\n        plt.show()\n\n    return dist, overlap_prob\n\n\n  File \"/tmp/ipython-input-3238389192.py\", line 46\n    if plot:\n    ^\nIndentationError: expected an indented block after 'for' statement on line 33\nQ: What is the avarge num employees in the office ?? (n=25)\nA: 25 *(4/10) = 10\nA: 8 or 10 or\nn=25 90% the num will be between 5 and 16 ‚Äúbetween 50% and 160%)\nn = 2500 90% the num will be between ? and ?\n‚ÄúBigger is better (risk)‚Äù\ndist, overlap_prob = office_simulation(n=25, n_weeks=2000, seed=42)\n\n\nprint(f\"\\nP(two specific employees overlap ‚â• 2 days/week): {overlap_prob:.4f}\")\n\n\n\n\n\n\n\n\nDistribution of # employees in the office on a random day:\n 1: 0.000\n 2: 0.000\n 3: 0.002\n 4: 0.007\n 5: 0.020\n 6: 0.044\n 7: 0.080\n 8: 0.120\n 9: 0.151\n10: 0.162\n11: 0.146\n12: 0.114\n13: 0.076\n14: 0.043\n15: 0.021\n16: 0.009\n17: 0.003\n18: 0.001\n19: 0.000\n20: 0.000\n21: 0.000\n22: 0.000\n\nP(two specific employees overlap ‚â• 2 days/week): 0.0998",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "01-HomeOffice.html#skeleton-code-for-desired-function",
    "href": "01-HomeOffice.html#skeleton-code-for-desired-function",
    "title": "1¬† Home Office Simulation",
    "section": "",
    "text": "One week, one employee:\n\nimport numpy as np\n\nrng = np.random.default_rng()\n\n\n#simulate 2 random days in the office\n\n#rng.integers(1,6,2) # this could create duplicates\n\n# WAIT We need to set replace = FALSE !! Because there should be no duplicates!\nempChoice = rng.choice(np.arange(1,6),2, replace = False )\nweekdays = np.zeros(6)\nprint(weekdays)\nprint(empChoice)\n#weekdays[0] += 1\n#weekdays[4] += 1\n\nweekdays[empChoice -1 ] += 1\nweekdays\n\n\n\n[0. 0. 0. 0. 0. 0.]\n[3 4]\n\n\narray([0., 0., 1., 1., 0., 0.])\n\n\nNow we go from one to n employees, so we need a loop !\n\nn = 20\nweekdays = np.zeros(5, dtype = \"int\")\nNumEmpInOffice = np.zeros(n+1, dtype = \"int\")\n\nfor i in range(n):\n  empChoice = rng.choice(np.arange(1,6),2, replace = False )\n  weekdays[empChoice -1 ] += 1\n\n\nNumEmpInOffice[weekdays] += 1\nprint(weekdays)\n\nif np.sum(weekdays)!= 2*n:\n  print(\"sth. is wrong!\") #sanity check!\n\nNumEmpInOffice\n\n[10  4  9 10  7]\n\n\narray([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\nLet us simulate MANY WEEKS !!\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef HomeOfficeSimulation(n = 20, n_weeks = 1000, daysInWeek = 5, days_in_office = 2, seed=123, verbose = False):\n  rng = np.random.default_rng(seed)\n\n  NumEmpInOffice = np.zeros(n+1, dtype = \"int\")#this is a 1D array of length n+1\n  for j in range(n_weeks):\n    weekdays = np.zeros(daysInWeek, dtype = \"int\")#this is a 1D array of length 5\n\n    for i in range(n):\n      empChoice = rng.choice(np.arange(1,daysInWeek+1),days_in_office, replace = False )\n      weekdays[empChoice -1 ] += 1#this is of length 5 !!\n\n    #NumEmpInOffice[weekdays] += 1 #for each of 5 elements I increment by 1\n    for w in weekdays:\n      NumEmpInOffice[w] += 1\n\n    if (verbose):\n      print(weekdays)\n      #print(NumEmpInOffice)\n      print(pd.Series(NumEmpInOffice, index=range(n + 1)).to_dict())\n\n    if np.sum(weekdays)!= 2*n:\n      print(\"sth. is wrong!\") #sanity check!\n  #we would like to return the probability !\n  #of course they have to add to one in total\n  return NumEmpInOffice/np.sum(NumEmpInOffice)\n  #return NumEmpInOffice/(n_weeks*5)#(n_weeks*daysInWeek)\n\n\n\n\none_sim = HomeOfficeSimulation(n = 6, n_weeks = 3, verbose = False)\nnp.sum(one_sim)\n\nnp.float64(5.0)\n\n\nProbability that two particular employees overlap ‚â• 2 days\n\none_sim = HomeOfficeSimulation(n = 2, n_weeks = 1000)\none_sim\n\narray([0.3572, 0.4856, 0.1572])\n\n\n\nx1 = [4, 7, -5, 3]#this is a 1D array\nx2 = [3, 5, 6, 1]#this is a 1D array\n\nx1 + x2# the \"+\" operator for lists is an append operation !!\nnp.array(x1) + np.array(x2)\nnp.array(x1) * np.array(x2)\n#It seems useful that we need numpy in adddition to lists!\n\n#Why would we need pandas on top of numpy !!\ny1 = np.array(x1)\ny1[1]#numeric indexing\n#y1[\"Wednesday\"]#numpy cannot handle other ways of indexing !\n\np1 = pd.Series(y1, index = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\"])\np1[\"Wednesday\"]\np1[[\"Wednesday\",\"Thursday\"]]\n\"Wednesday\" in p1\n\"Friday\" in p1\np1\np1.to_dict()\n\n{'Monday': 4, 'Tuesday': 7, 'Wednesday': -5, 'Thursday': 3}\n\n\n\np1.array\np1.index\n\nIndex(['Monday', 'Tuesday', 'Wednesday', 'Thursday'], dtype='object')\n\n\n\none_sim = HomeOfficeSimulation(n_weeks= 500)\n\n\none_sim = HomeOfficeSimulation(n=5, n_weeks= 2, verbose=1)\n\n[4 3 0 1 2]\n{0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 0}\n[2 2 2 2 2]\n{0: 1, 1: 1, 2: 2, 3: 1, 4: 1, 5: 0}\n\n\n\nnp.sum(one_sim)\n\nnp.int64(6)",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "01-HomeOffice.html#plots",
    "href": "01-HomeOffice.html#plots",
    "title": "1¬† Home Office Simulation",
    "section": "Plots",
    "text": "Plots\nWhat type of plot do we need ???\n\nHistogram ?? Maybe not because there is no need for binning !\nSimple bar plot !\n\n\n#let us plot !!\nplt.figure(figsize=(7, 4))\nplt.bar(np.arange(n+1),one_sim)\nplt.xlabel(\"Number of employees in the office\")\nplt.ylabel(\"Probability\")\nplt.title(f\"Distribution of employees in office (n={n})\")\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\n\nsns.set()\nsns.barplot(one_sim);",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "01-HomeOffice.html#debugging",
    "href": "01-HomeOffice.html#debugging",
    "title": "1¬† Home Office Simulation",
    "section": "Debugging",
    "text": "Debugging\n\nSanity check:\n\nDo the probabilities add to one ?\nRead up on pandas series\nWrite basic code to double check our indexing logic !\n\n\nn=10\nNumEmpInOffice = np.zeros(n+1, dtype = \"int\")\nweekdays = [3,3]\n#NumEmpInOffice[weekdays] += 1\nfor w in weekdays:\n  NumEmpInOffice[w] += 1\n\nprint(NumEmpInOffice)\n\n[0 0 0 1 0 0 0 0 0 0 0]",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "01-HomeOffice.html#extensions",
    "href": "01-HomeOffice.html#extensions",
    "title": "1¬† Home Office Simulation",
    "section": "Extensions",
    "text": "Extensions\n\nFold the plotting code into the function\nMake all ‚Äúparameters‚Äù function arguments, e.g.¬†days_in_a_week (5), days_in_office (2), ‚Ä¶\nCan you use this code ‚Äúas is‚Äù for the following related questions:\n\nProbability that two particular employees overlap ‚â• 2 days\nBirthday problem",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Home Office Simulation</span>"
    ]
  },
  {
    "objectID": "02-BinomialProcesses.html",
    "href": "02-BinomialProcesses.html",
    "title": "2¬† Binomial Sampling",
    "section": "",
    "text": "3 Grouped Operations",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Binomial Sampling</span>"
    ]
  },
  {
    "objectID": "02-BinomialProcesses.html#coin-tossing",
    "href": "02-BinomialProcesses.html#coin-tossing",
    "title": "2¬† Binomial Sampling",
    "section": "Coin Tossing",
    "text": "Coin Tossing\n\nWrite a function which simulates tossing a ‚Äúbiased coin‚Äù (with \\(p=0.157407\\)) \\(108\\) times and computes the proportion of ‚Äúheads‚Äù.\nCall this function ‚Äúmany‚Äù times and plot a histogram\n\n\ndef toss_biased_coin(pHead, n, seed=None):\n  \"\"\"\n    Simulates n tosses of a biased coin and returns the proportion of heads,\n    using rng.choice() for better conceptual clarity.\n\n    Parameters\n    ----------\n    pHead : float\n        Probability of heads (between 0 and 1).\n    n : int\n        Number of tosses.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    float\n        Proportion of heads observed.\n    \"\"\"\n  rng = np.random.default_rng(seed)\n\n  tosses = rng.choice([0,1], size=n, p=[1-pHead,pHead])\n  cts = np.sum(tosses == 1)\n  cts = np.sum(tosses)\n  prop_heads = cts/n\n\n  prop_heads = np.mean(tosses)\n\n  return prop_heads\n\n\n\ntoss_biased_coin(pHead=0.157407, n= 108)#, seed = 123)\n\n0.14814814814814814\n\n\n\n#let us run this many times !\npHead_obs = np.zeros(500)\n\nfor i in range(500):\n  pHead_obs[i] = toss_biased_coin(pHead=0.157407, n= 108)\n\n\n\n#95% interval:\nnp.percentile(pHead_obs, [2.5,97.5])\n\narray([0.10185185, 0.23148148])\n\n\n\n#histogram\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\nRelated Questions:\n\nThe board of a large international company consists of 7 women and 3 men. If sex played no role, i.e.¬†if we assume that the probabilities of m/f were equal, how likely is it to see such asymmetric proportions?\nHow does this assessment change if the board consisted of 100 members and there would be 70 men and 30 women ?\n\n\n#let us run this many times !\nnSim = 1000\npHead_obs = np.zeros(nSim)\n\nfor i in range(nSim):\n  pHead_obs[i] = toss_biased_coin(pHead=0.5, n= 10)\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\n\n#how likely is it that one sees sth. \"as extreme as 7/3\"\nnp.mean(pHead_obs &gt;= 0.7) + np.mean(pHead_obs &lt;= 0.3)\n\n0.327\n\n\n\n#let us run this many times !\nnSim = 10000\npHead_obs = np.zeros(nSim)\n\nfor i in range(nSim):\n  pHead_obs[i] = toss_biased_coin(pHead=0.5, n= 100)\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\n\nnp.mean(pHead_obs &gt;= 0.7) + np.mean(pHead_obs &lt;= 0.3)\n\n0.0\n\n\n\npHead = 0.5\n#What is the exact answer of this question given that you now know sigma !\ns = np.sqrt(pHead*(1-pHead)/100)\nprint(s)\n#how likely to be above 0.7\nzScore = (0.7-0.5)/s\nzScore\n\n#scipy.stat\n\n0.05\n\n\n3.999999999999999\n\n\n\ns = np.std(pHead_obs)\nprint(s)\n0.5 + 2*s\n0.5 - 2*s\n\n0.049661881065863785\n\n\n0.40067623786827244\n\n\nSUPER IMPORTANT !! What is the stdev in the binomial distribution ??\nThe FAMOUS SQRT(N) LAW !!!\n\np= 0.5\n#Variance:\np*(1-p)\nnp.sqrt(p*(1-p)/10)\n\n0.15811388300841897",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Binomial Sampling</span>"
    ]
  },
  {
    "objectID": "03-Intro_Testing.html",
    "href": "03-Intro_Testing.html",
    "title": "3¬† AB Testing",
    "section": "",
    "text": "Permutation 2-sample test\nWe have used the bootstrap to compare two sets of data, both of which are samples. In particular, we can test two-sample hypotheses such as\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m \\neq \\mu_f\\)\nor the one-sided versions:\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m &gt; \\mu_f\\)\n\\(H_0: \\mu_m = \\mu_f, H_A: \\mu_m &lt; \\mu_f\\)\nAnother way to compare 2 distributions (in some ways much more straightforward than the bootstrap) is permutation sampling. It directly simulates the hypothesis that two variables have identical probability distributions.\nA permutation sample of two arrays having respectively \\(n_1\\) and \\(n_2\\) entries is constructed by concatenating the arrays together, scrambling the contents of the concatenated array, and then taking the first \\(n_1\\) entries as the permutation sample of the first array and the last \\(n_2\\) entries as the permutation sample of the second array.\nAt DataCamp the first example offers a nice visualization of this process:\nTake a look at the code in ourFunctions.py to run a permutation test\nLet us apply our first permutation sampling on the Titanic data. (First, we explore the data a bit)\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\n\n\n  \n    \n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\nPclassSurv = titanic.groupby(['pclass', 'survived'])\nPclassSurv.size()\n\n\n\n\n\n\n\n\n\n0\n\n\npclass\nsurvived\n\n\n\n\n\n1\n0\n80\n\n\n1\n136\n\n\n2\n0\n97\n\n\n1\n87\n\n\n3\n0\n372\n\n\n1\n119\n\n\n\n\ndtype: int64\npd.crosstab(titanic.pclass, titanic.survived,margins=True)\n\n\n\n\n\n\n\nsurvived\n0\n1\nAll\n\n\npclass\n\n\n\n\n\n\n\n1\n80\n136\n216\n\n\n2\n97\n87\n184\n\n\n3\n372\n119\n491\n\n\nAll\n549\n342\n891\nWomenOnly = titanic[titanic[\"sex\"]==\"female\"]\npd.crosstab(WomenOnly.pclass, WomenOnly.survived,margins=True)\n\n\n  \n    \n\n\n\n\n\nsurvived\n0\n1\nAll\n\n\npclass\n\n\n\n\n\n\n\n1\n3\n91\n94\n\n\n2\n6\n70\n76\n\n\n3\n72\n72\n144\n\n\nAll\n81\n233\n314\nTest the claim that the survival chances of women in 1st and 2nd class were pretty much the same.\ntitanic_women_p1= WomenOnly[WomenOnly['pclass'] == 1][\"survived\"]\ntitanic_women_p2= WomenOnly[WomenOnly['pclass'] == 2][\"survived\"]\ndiff_reps = draw_perm_reps(titanic_women_p1, titanic_women_p2, size = 500)\nobsmean_p1 = np.mean(titanic_women_p1)\nobsmean_p2 =np.mean(titanic_women_p2)\nobsDiff =obsmean_p1-obsmean_p2\n#We could assume that P1 has a higher surv probability\np_val= np.mean(diff_reps &gt;= obsDiff)\np_val\n\n# We do not reject\n\nnp.float64(0.134)\nWomenOnly = titanic[titanic[\"sex\"]==\"female\"]\nwomen1st = WomenOnly[WomenOnly[\"pclass\"]==1]\nwomen2nd = WomenOnly[WomenOnly[\"pclass\"]==2]\npermutation_sample(women1st.survived, women2nd.survived)\ndiff_reps = draw_perm_reps(women1st.survived, women2nd.survived, size=10000)\nplt.hist(diff_reps);\nplt.axvline(obsDiff, color = \"red\");\npVal = np.mean(diff_reps &lt;= obsDiff)\npVal\n\nnp.float64(0.9552)\nobsDiff\n\nnp.float64(0.04703247480403139)\nWe could choose alpha = 0.05, but keep in mind the following - would you step into a plane that has a 5% crash probability ? - Would you buy a drug that has a 5% chance of severe side effects ?\nWhat is the difference between these two methods (bootstrap, permutation) ?\nTesting the hypothesis that two samples have the same distribution may be done with a bootstrap test, but a permutation test is preferred because it is more accurate (exact, in fact). But a permutation test is not as versatile as the bootstrap.\nWe often want to test the hypothesis that population A and population B have the same mean, but not necessarily the same distribution. This is difficult with a permutation test as it assumes exchangeability.\nWe will get back to this topic!\nMore info..",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "03-Intro_Testing.html#permutation-2-sample-test",
    "href": "03-Intro_Testing.html#permutation-2-sample-test",
    "title": "3¬† AB Testing",
    "section": "",
    "text": "Write down the Null hypothesis and test statistic\nWrite code that generates permutation samples from two data sets\nGenerate many permutation replicates for the relevant Titanic subset\nCompute a p-value",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "03-Intro_Testing.html#sample-t-test",
    "href": "03-Intro_Testing.html#sample-t-test",
    "title": "3¬† AB Testing",
    "section": "2-sample t test",
    "text": "2-sample t test\nOf course there is an equivalent fully parametric 2-sample test, the t-test.\nWe first read in the The National Survey of Family Growth data from the think stats book.\nLook at section 1.7 for a description of the variables.\n\n#preg = pd.read_hdf('data/pregNSFG.h5', 'df')\n#preg = pd.read_csv('data/pregNSFG.csv.gz', compression='gzip')\nurl = \"https://raw.githubusercontent.com/markusloecher/DataScience-HWR/refs/heads/main/data/pregNSFG.csv.gz\"\npreg = pd.read_csv(url, compression='gzip')\n#only look at live births\nlive = preg[preg.outcome == 1]\n\n#define first babies\nfirsts = live[live.birthord == 1]\n\n#and all others\nothers = live[live.birthord != 1]\npreg\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ncaseid\npregordr\nhowpreg_n\nhowpreg_p\nmoscurrp\nnowprgdk\npregend1\npregend2\nnbrnaliv\n...\nlaborfor_i\nreligion_i\nmetro_i\nbasewgt\nadj_mod_basewgt\nfinalwgt\nsecu_p\nsest\ncmintvw\ntotalwgt_lb\n\n\n\n\n0\n0\n1\n1\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n3410.389399\n3869.349602\n6448.271112\n2\n9\nNaN\n8.8125\n\n\n1\n1\n1\n2\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n3410.389399\n3869.349602\n6448.271112\n2\n9\nNaN\n7.8750\n\n\n2\n2\n2\n1\nNaN\nNaN\nNaN\nNaN\n5.0\nNaN\n3.0\n...\n0\n0\n0\n7226.301740\n8567.549110\n12999.542264\n2\n12\nNaN\n9.1250\n\n\n3\n3\n2\n2\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n7226.301740\n8567.549110\n12999.542264\n2\n12\nNaN\n7.0000\n\n\n4\n4\n2\n3\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n7226.301740\n8567.549110\n12999.542264\n2\n12\nNaN\n6.1875\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13588\n13588\n12571\n1\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\n6.1875\n\n\n13589\n13589\n12571\n2\nNaN\nNaN\nNaN\nNaN\n3.0\nNaN\nNaN\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\nNaN\n\n\n13590\n13590\n12571\n3\nNaN\nNaN\nNaN\nNaN\n3.0\nNaN\nNaN\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\nNaN\n\n\n13591\n13591\n12571\n4\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\n7.5000\n\n\n13592\n13592\n12571\n5\nNaN\nNaN\nNaN\nNaN\n6.0\nNaN\n1.0\n...\n0\n0\n0\n4670.540953\n5795.692880\n6269.200989\n1\n78\nNaN\n7.5000\n\n\n\n\n13593 rows √ó 245 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\ntRes = stats.ttest_ind(firsts.prglngth.values, others.prglngth.values)\np = pd.Series([tRes.pvalue,tRes.statistic], index = ['p-value:', 'test statistic:'])\np\n\n\n\n\n\n\n\n\n0\n\n\n\n\np-value:\n0.167554\n\n\ntest statistic:\n1.380215\n\n\n\n\ndtype: float64\n\n\n\n#ttest_ind often underestimates p for unequal variances:\ntRes = stats.ttest_ind(firsts.prglngth.values, others.prglngth.values, equal_var = False)\np = pd.Series([tRes.pvalue,tRes.statistic], index = ['p-value:', 'test statistic:'])\np\n\n\n\n\n\n\n\n\n0\n\n\n\n\np-value:\n0.168528\n\n\ntest statistic:\n1.377059\n\n\n\n\ndtype: float64\n\n\n\nRun a permutation test instead\nExtra credit: Can you reproduce the first p-value from the test statistic ?\n\n\nfrom scipy.stats import binom\n\n\n\nA/B Testing\n\nPerform a permutation test on the DataCamp example:\n\n\n\n\nimage.png\n\n\n\nclickthroughA  = [True] * 45 + [False]* 455\nclickthroughB\n\n[True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n True,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False,\n False]",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>AB Testing</span>"
    ]
  },
  {
    "objectID": "04-Coin Toss Startup.html",
    "href": "04-Coin Toss Startup.html",
    "title": "4¬† p-Value Hacking",
    "section": "",
    "text": "Coin Toss Startup - ‚ÄúProfit or Perish‚Äù\nCoin Toss Startup is a decision experiment about risk, incentives, and ‚Äúp-hacking for profit.‚Äù\nBelow is a complete, ready-to-run Python game (works in Google Colab, Jupyter, or VS Code). The scientific message is: early stopping inflates false positives\nIn addition we can frame it as a business decision: when to ‚Äúgo public‚Äù with results.",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>p-Value Hacking</span>"
    ]
  },
  {
    "objectID": "04-Coin Toss Startup.html#coin-toss-startup-profit-or-perish",
    "href": "04-Coin Toss Startup.html#coin-toss-startup-profit-or-perish",
    "title": "4¬† p-Value Hacking",
    "section": "‚ÄúCoin Toss Startup ‚Äî Profit or Perish‚Äù",
    "text": "‚ÄúCoin Toss Startup ‚Äî Profit or Perish‚Äù\n\nLearning goals\nStudents experience how:\n\nIncentives + noise ‚Üí bad inference\nPeeking too often ‚Üí false success\nStatistical rigor costs short-term profit\n\n\n\n\nModel\n\nEach ‚Äústartup‚Äù flips a coin (true win prob =p_true).\nEach flip costs 1 unit (R&D cost).\nIf they ‚Äúfind significance‚Äù (p &lt; Œ±), they may cash out.\nIf they stop early and it‚Äôs a false positive ‚Üí funders lose everything.\nIf the effect is real ‚Üí reward = base √ó (1 / flip_count) (early discovery = bigger payoff).\n\n\n\n\n\nWhat happens in class\n\n\n\n\n\n\n\nParameter\nBehavior\n\n\n\n\np_true = 0.5\nMost startups lose money, but some ‚Äúget lucky‚Äù ‚Üí false discovery profits\n\n\np_true = 0.55\nTrue effects can be profitable but need more patience\n\n\nHigh peek_every (frequent peeking)\nHigher ‚Äúsuccess‚Äù rate but misleading profits\n\n\nHigh cost_per_flip\nIncreases pressure to stop early\n\n\nLower Œ±\nReduces false profits but increases missed real effects\n\n\n\n\n\n\nüí¨ Discussion prompts\n\nHow often do ‚Äúsuccessful‚Äù startups actually have a real edge?\nWhat happens when cost pressure (or publish-or-perish) grows?\nHow could funders change incentives to reward replication instead of significance?\n\n\nimport numpy as np\n\nnp.sum(np.random.rand(5000) &lt; 0.25)/5000\n\nnp.float64(0.2462)\n\n\n\n5 % 2\n#you can define steps !!\n\nfor x in range(20):\n  print(x % 10 == 0)\n\n\n### üêç Python Code\n\n# If you're in Google Colab, first run this setup cell:\n!pip install ipywidgets --quiet\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, FloatSlider, IntSlider\n\nnp.random.seed(42)\n\ndef simulate_startup_with_payoff(p_true=0.5, n_max=200, alpha=0.05,min_tosses=10,\n                                 peek_every=5, cost_per_flip=1, base_reward=100):\n    \"\"\"\n    Simulate one startup that may stop early when p &lt; alpha.\n    Returns profit, detection_time, and p-value trajectory.\n    \"\"\"\n    heads = 0\n    p_values = []\n    detection_time = None\n    profit = -cost_per_flip * n_max  # assume they run full unless success\n    reward = 0\n\n    for n in range(1, n_max + 1):\n        #this is a coin flip !!! with prob pTrue !\n        if np.random.rand() &lt; p_true:\n            heads += 1\n        #do sth, every peek time !\n        if n % peek_every == 0:\n        #if n &gt;= min_tosses and (n % peek_every == 0):\n            p_hat = heads / n # observed proportion\n            z = (p_hat - 0.5) / np.sqrt(0.25 / n) # test statistic !! z-score!\n            p_val = 1 - stats.norm.cdf(z) # pvalue\n            p_values.append((n, p_val))\n\n            if p_val &lt; alpha: #\n                detection_time = n\n                # compute profit: reward minus accumulated cost\n                reward = base_reward / (n / 10)  # earlier = larger reward\n                profit = reward - cost_per_flip * n\n                break # terminates the for loop !!\n\n    return profit, detection_time, p_values\n\n\ndef plot_startup_with_payoff(p_true=0.5, n_max=200, alpha=0.05,\n                             peek_every=5, n_reps=500, cost_per_flip=1, base_reward=100):\n    \"\"\"\n    Simulate many startups and plot typical trajectory + profit distribution.\n    \"\"\"\n    profits, detections = [], []\n\n    for _ in range(n_reps):\n        prof, det, _ = simulate_startup_with_payoff(p_true, n_max, alpha,\n                                                    peek_every, cost_per_flip, base_reward)\n        profits.append(prof)\n        detections.append(det is not None)\n\n    avg_profit = np.mean(profits)\n    success_rate = np.mean(detections)\n\n    # One example trajectory\n    prof_ex, det_ex, pv_ex = simulate_startup_with_payoff(p_true, n_max, alpha,\n                                                          peek_every, cost_per_flip, base_reward)\n    n_vals, pvals = zip(*pv_ex)\n    plt.figure(figsize=(11,4))\n    plt.subplot(1,2,1)\n    plt.plot(n_vals, pvals, 'o-', label=\"Sequential p-values\")\n    plt.axhline(alpha, color='gray', linestyle='--', label=f'Œ±={alpha}')\n    if det_ex:\n        plt.axvline(det_ex, color='green', linestyle='--', label=f'Stopped at {det_ex}')\n    plt.xlabel(\"Number of tosses\")\n    plt.ylabel(\"p-value\")\n    plt.title(\"Typical startup trajectory\")\n    plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.hist(profits, bins=20)\n    plt.axvline(0, color='red', linestyle='--')\n    plt.title(f\"Profit distribution\\nMean={avg_profit:.1f}, Success rate={success_rate:.2f}\")\n    plt.xlabel(\"Profit\")\n    plt.ylabel(\"Frequency\")\n    plt.tight_layout()\n    plt.show()\n\n\n\nplot_startup_with_payoff(p_true=0.5, n_reps=1)\n\n\n\n\n\n\n\n\n\n# Interactive version\ninteract(plot_startup_with_payoff,\n         p_true=FloatSlider(value=0.5, min=0.5, max=0.7, step=0.01, description=\"True p\"),\n         n_max=IntSlider(value=200, min=50, max=1000, step=50, description=\"Max flips\"),\n         alpha=FloatSlider(value=0.05, min=0.001, max=0.2, step=0.005, description=\"Œ±\"),\n         peek_every=IntSlider(value=5, min=1, max=20, step=1, description=\"Peek every\"),\n         n_reps=IntSlider(value=500, min=50, max=2000, step=50, description=\"Startups\"),\n         cost_per_flip=FloatSlider(value=1, min=0.1, max=5, step=0.1, description=\"Cost/flip\"),\n         base_reward=IntSlider(value=100, min=50, max=500, step=10, description=\"Base reward\"));",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>p-Value Hacking</span>"
    ]
  },
  {
    "objectID": "04-Coin Toss Startup.html#gamified-version",
    "href": "04-Coin Toss Startup.html#gamified-version",
    "title": "4¬† p-Value Hacking",
    "section": "Gamified version",
    "text": "Gamified version\n\n# ================================\n# ü™ô COIN TOSS STARTUP GAME\n# ================================\n# Copy this cell into Google Colab and run all\n# Author: adapted for teaching from ChatGPT (GPT-5)\n# -------------------------------\n\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(12345)\n\n# ========== SIMULATION CORE ==========\n\ndef simulate_startup_with_payoff(p_true=0.5, n_max=200, alpha=0.05,\n                                 peek_every=5, min_tosses=10,\n                                 cost_per_flip=1.0, base_reward=300.0,\n                                 reward_decay_lambda=0.03, rng=None):\n    \"\"\"Simulate one startup run with a peeking testing rule.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n    heads = 0\n    detection_time = None\n\n    for n in range(1, n_max + 1):\n        if rng.random() &lt; p_true:\n            heads += 1\n\n        if n &gt;= min_tosses and (n % peek_every == 0):\n            p_hat = heads / n\n            denom = np.sqrt(max(1e-9, 0.25 / n))\n            z = (p_hat - 0.5) / denom\n            p_val = 1 - stats.norm.cdf(z)\n            if p_val &lt; alpha:\n                detection_time = n\n                break\n\n    total_cost = cost_per_flip * (detection_time if detection_time is not None else n_max)\n\n    if detection_time is not None:\n        if p_true &gt; 0.5:\n            reward = base_reward * np.exp(-reward_decay_lambda * detection_time)\n            profit = reward - total_cost\n            false_positive = False\n            success = True\n        else:\n            profit = -total_cost\n            false_positive = True\n            success = False\n    else:\n        profit = -cost_per_flip * n_max if p_true &gt; 0.5 else 0.0\n        false_positive = False\n        success = False\n\n    return {'profit': profit,\n            'detection_time': detection_time,\n            'success': success,\n            'false_positive': false_positive}\n\n\ndef evaluate_team_strategy(strategy, p_true=0.5, n_reps=1000, n_max=200, rng=None):\n    \"\"\"Monte Carlo evaluation of one team.\"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    profits, successes, false_pos, det_times = [], 0, 0, []\n\n    for _ in range(n_reps):\n        res = simulate_startup_with_payoff(p_true=p_true, n_max=n_max, rng=rng, **strategy)\n        profits.append(res['profit'])\n        if res['success']: successes += 1; det_times.append(res['detection_time'])\n        if res['false_positive']: false_pos += 1\n\n    return {\n        'mean_profit': np.mean(profits),\n        'median_profit': np.median(profits),\n        'success_rate': successes / n_reps,\n        'false_positive_rate': false_pos / n_reps,\n        'mean_detection_time': np.mean(det_times) if det_times else np.nan\n    }\n\n\ndef run_tournament(teams, p_true=0.5, n_reps=1000, n_max=200, rng_seed=123):\n    \"\"\"Evaluate multiple teams and return leaderboard.\"\"\"\n    rng = np.random.default_rng(rng_seed)\n    rows = []\n    for name, strat in teams.items():\n        stats_ = evaluate_team_strategy(strat, p_true=p_true, n_reps=n_reps, n_max=n_max, rng=rng)\n        rows.append({'team': name, **stats_, **strat})\n    df = pd.DataFrame(rows).sort_values('mean_profit', ascending=False).reset_index(drop=True)\n    return df\n\n# ========== DEFINE TEAMS HERE ==========\n\n\n\nteams = {\n    # Teams pick their peek_every, alpha, and min_tosses\n    'team1': {'peek_every':, 'alpha':, 'min_tosses':30,\n                     'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'team2': {'peek_every':, 'alpha':, 'min_tosses':5,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'team3': {'peek_every':1, 'alpha':0.05, 'min_tosses':30,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'team4': {'peek_every':1, 'alpha':0.05, 'min_tosses':30,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'team5': {'peek_every':1, 'alpha':0.05, 'min_tosses':30,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n}\n\n\nteams = {\n    # Teams pick their peek_every, alpha, and min_tosses\n    'Conservative': {'peek_every':20, 'alpha':0.01, 'min_tosses':30,\n                     'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'Aggressive': {'peek_every':1, 'alpha':0.05, 'min_tosses':5,\n                   'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'Balanced': {'peek_every':5, 'alpha':0.02, 'min_tosses':10,\n                 'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n    'SlowButSure': {'peek_every':10, 'alpha':0.01, 'min_tosses':20,\n                    'cost_per_flip':1.0, 'base_reward':120.0, 'reward_decay_lambda':0.02},\n}\n\nteams = {\n    'Conservative': {'peek_every':20, 'alpha':0.01, 'min_tosses':30,\n                     'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'Aggressive': {'peek_every':1, 'alpha':0.05, 'min_tosses':5,\n                   'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'Balanced': {'peek_every':5, 'alpha':0.02, 'min_tosses':10,\n                 'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'SlowButSure': {'peek_every':10, 'alpha':0.01, 'min_tosses':20,\n                    'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n}\n\n\n# ========== RUN TOURNAMENTS ==========\n\nn_reps = 1500\nn_max = 200\n\nprint(\"üîπ Null world (p_true = 0.50)\")\nleader_null = run_tournament(teams, p_true=0.5, n_reps=n_reps, n_max=n_max)\ndisplay(leader_null[['team', 'mean_profit', 'false_positive_rate', 'alpha', 'peek_every']])\n\nprint(\"\\nüîπ Weak effect world (p_true = 0.55)\")\nleader_eff = run_tournament(teams, p_true=0.55, n_reps=n_reps, n_max=n_max)\ndisplay(leader_eff[['team', 'mean_profit', 'success_rate', 'mean_detection_time', 'alpha', 'peek_every']])\n\nprint(\"\\nüîπ Stron effect world (p_true = 0.65)\")\nleader_strong = run_tournament(teams, p_true=0.65, n_reps=n_reps, n_max=n_max)\ndisplay(leader_strong[['team', 'mean_profit', 'success_rate', 'mean_detection_time', 'alpha', 'peek_every']])\n\n# ========== VISUALIZATION ==========\n\nfig, ax = plt.subplots(figsize=(8,4))\nax.bar(leader_eff['team'], leader_eff['mean_profit'], color='skyblue', edgecolor='black')\nax.axhline(0, color='k', lw=1)\nax.set_ylabel(\"Mean Profit (p_true=0.55)\")\nax.set_title(\"üèÜ Team Performance ‚Äî Coin Toss Startup Game\")\nplt.show()\n\n\nleader_null\n\nprint(\"\\nüîπ Strong effect world (p_true = 0.65)\")\nleader_strong = run_tournament(teams, p_true=0.65, n_reps=n_reps, n_max=n_max)\ndisplay(leader_strong[['team', 'mean_profit', 'success_rate', 'mean_detection_time', 'alpha', 'peek_every']])\n\n\nüîπ Stron effect world (p_true = 0.65)\n\n\n\n  \n    \n\n\n\n\n\n\nteam\nmean_profit\nsuccess_rate\nmean_detection_time\nalpha\npeek_every\n\n\n\n\n0\nAggressive\n47.001107\n1.000000\n28.940667\n0.05\n1\n\n\n1\nBalanced\n5.981218\n0.996000\n48.514056\n0.02\n5\n\n\n2\nSlowButSure\n-29.439449\n0.987333\n67.434166\n0.01\n10\n\n\n3\nConservative\n-47.257600\n0.985333\n77.280108\n0.01\n20\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\nteams = {\n    'Conservative': {'peek_every':20, 'alpha':0.01, 'min_tosses':30,\n                     'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'Aggressive': {'peek_every':1, 'alpha':0.05, 'min_tosses':5,\n                   'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'Balanced': {'peek_every':5, 'alpha':0.02, 'min_tosses':10,\n                 'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n    'SlowButSure': {'peek_every':10, 'alpha':0.01, 'min_tosses':20,\n                    'cost_per_flip':0.5, 'base_reward':300.0, 'reward_decay_lambda':0.02},\n}\n\ndone with p= 0.5\n\n\n\nteams = {\n    # Teams pick their peek_every, alpha, and min_tosses\n    'team1': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03},\n    'team2': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03},\n    'team3': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03},\n    'team4': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03},\n    'team5': {'peek_every':, 'alpha':, 'min_tosses':10,\n                     'cost_per_flip':1.0, 'base_reward':300.0, 'reward_decay_lambda':0.03}\n\n}\n\n\n# ================================\n# üîπ MULTI-WORLD TOURNAMENT LEADERBOARD\n# ================================\n\np_values = [0.50, 0.55, 0.60]\nresults = []\n\nfor p in p_values:\n    df = run_tournament(teams, p_true=p, n_reps=500, n_max=200)\n    print(\"done with p=\",p)\n    df['p_true'] = p\n    results.append(df)\n\n# Combine\ndf_all = pd.concat(results, ignore_index=True)\n\n# --- Aggregate performance across worlds ---\nsummary = (\n    df_all.groupby('team')\n    .agg(mean_profit_overall=('mean_profit', 'mean'),\n         min_profit=('mean_profit', 'min'),\n         max_profit=('mean_profit', 'max'),\n         avg_success_rate=('success_rate', 'mean'),\n         avg_false_pos=('false_positive_rate', 'mean'))\n    .sort_values('mean_profit_overall', ascending=False)\n    .reset_index()\n)\n\ndisplay(summary)\n\n# --- Plot ---\nfig, ax = plt.subplots(figsize=(8,4))\nfor team in df_all['team'].unique():\n    subset = df_all[df_all['team'] == team]\n    ax.plot(subset['p_true'], subset['mean_profit'], marker='o', label=team)\n\nax.set_xlabel(\"True success probability (p_true)\")\nax.set_ylabel(\"Mean Profit\")\nax.set_title(\"üìà Team Robustness Across Worlds\")\nax.axhline(0, color='k', lw=1)\nax.legend()\nplt.show()\n\ndone with p= 0.5\ndone with p= 0.55\ndone with p= 0.6\n\n\n\n  \n    \n\n\n\n\n\n\nteam\nmean_profit_overall\nmin_profit\nmax_profit\navg_success_rate\navg_false_pos\n\n\n\n\n0\nteam1\n34.214896\n-27.520000\n101.182283\n0.628667\n0.179333\n\n\n1\nteam5\n20.826768\n-30.600000\n76.154211\n0.620000\n0.175333\n\n\n2\nteam2\n11.599734\n-20.272166\n70.887367\n0.560667\n0.111333\n\n\n3\nteam4\n-20.746398\n-66.027067\n12.887874\n0.502000\n0.063333\n\n\n4\nteam3\n-38.008873\n-105.636113\n1.269494\n0.468000\n0.052667\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n  fig.canvas.print_figure(bytes_io, **kw)",
    "crumbs": [
      "Stats via Simulations",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>p-Value Hacking</span>"
    ]
  },
  {
    "objectID": "05-DataScience_with_Penguins.html",
    "href": "05-DataScience_with_Penguins.html",
    "title": "5¬† Data Science with Penguins",
    "section": "",
    "text": "6 Palmer Penguins",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "05-DataScience_with_Penguins.html#look-at-the-data",
    "href": "05-DataScience_with_Penguins.html#look-at-the-data",
    "title": "5¬† Data Science with Penguins",
    "section": "Look at the data",
    "text": "Look at the data\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npenguins = sns.load_dataset(\"penguins\")\npenguins\n\n\n  \n    \n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nGentoo\nBiscoe\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nGentoo\nBiscoe\n46.8\n14.3\n215.0\n4850.0\nFemale\n\n\n341\nGentoo\nBiscoe\n50.4\n15.7\n222.0\n5750.0\nMale\n\n\n342\nGentoo\nBiscoe\n45.2\n14.8\n212.0\n5200.0\nFemale\n\n\n343\nGentoo\nBiscoe\n49.9\n16.1\n213.0\n5400.0\nMale\n\n\n\n\n344 rows √ó 7 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n#√§map:\n\npDict = {\"Adelie\": 1,\n         \"Chintoo\": 2}",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "05-DataScience_with_Penguins.html#grouping",
    "href": "05-DataScience_with_Penguins.html#grouping",
    "title": "5¬† Data Science with Penguins",
    "section": "Grouping",
    "text": "Grouping\nThe seaborn library offers simple grouping in their plots via the hue argument:",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "05-DataScience_with_Penguins.html#box-plots",
    "href": "05-DataScience_with_Penguins.html#box-plots",
    "title": "5¬† Data Science with Penguins",
    "section": "Box plots",
    "text": "Box plots\nSide-by-side comparisons are much easier with box plots:\n\n# Box plot of body mass by species and sex\nplt.figure(figsize=(5, 2.5))\nsns.boxplot(data = penguins, y =\"body_mass_g\", x=\"species\", hue = \"sex\")\nplt.title('Body Mass by Species ');\nplt.show();",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "05-DataScience_with_Penguins.html#scatter-plots",
    "href": "05-DataScience_with_Penguins.html#scatter-plots",
    "title": "5¬† Data Science with Penguins",
    "section": "Scatter Plots",
    "text": "Scatter Plots\n\n# @title bill_length_mm vs bill_depth_mm\nplt.figure(figsize=(4, 2));\nfrom matplotlib import pyplot as plt\npenguins.plot(x = \"bill_depth_mm\", y = \"bill_length_mm\", kind = \"scatter\");\nplt.figure(figsize=(4, 2));\n# there are clearly MANY scatter plots that we can look at !\n# It would require a lot of code to create that many\n# 4*3/2\n#correlation coefficient quantifies the linear dependence\n# it is in the range [-1,1]\nnp.corrcoef(penguins.bill_depth_mm, penguins.bill_length_mm)\n#What is nan ?\n\narray([[nan, nan],\n       [nan, nan]])\n\n\n&lt;Figure size 400x200 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 400x200 with 0 Axes&gt;",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "05-DataScience_with_Penguins.html#categorical-variables",
    "href": "05-DataScience_with_Penguins.html#categorical-variables",
    "title": "5¬† Data Science with Penguins",
    "section": "Categorical Variables",
    "text": "Categorical Variables\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npenguins = sns.load_dataset(\"penguins\")\n\n\nlm2a = smf.ols('flipper_length_mm ~ species -1', penguins).fit()\nlm2a.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nspecies[Adelie]\n189.9536\n0.540\n351.454\n0.000\n188.891\n191.017\n\n\nspecies[Chinstrap]\n195.8235\n0.805\n243.137\n0.000\n194.239\n197.408\n\n\nspecies[Gentoo]\n217.1870\n0.599\n362.676\n0.000\n216.009\n218.365\n\n\n\n\n\nQuizzes\n\nimport numpy as np\nfrom numpy.random import default_rng\n\nrng = default_rng(70)\n#Generate 500 random integers between 80 and 90 (both boundaries are inclusive!)\nx = rng.integers(80,90, 500)\nx\nnp.mean(x)\nx.mean()\n#every \"fourth value\"\n#print(x[0:10])\n#print(x[:10:4])\nnp.sum(x[::4])\nx &gt; 85\nnp.sum(x &gt; 85)\nnp.sum(x == 81)\nx = np.arange(1,13)\nx\nx.reshape(4,3)\n\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]])\n\n\n\nimport numpy as np\n\nurl = \"https://raw.githubusercontent.com/markusloecher/DataScience2018/master/data/diabetes.csv\"\ndiabetes = np.loadtxt(url, delimiter=\",\", skiprows=1)\nendRow=444\ndiabetes = diabetes[0:endRow,:]\n\n\nnp.mean(diabetes[:,0])\nmax(diabetes[:,1] ) - min(diabetes[:,1] )\ndiabetes[-1,0]\nnp.sum(diabetes[:,6]&gt;0.3825)\nnp.unique(diabetes[:,0], return_counts=True)\n\n(array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.,\n        13., 14., 15., 17.]),\n array([62, 77, 57, 42, 42, 45, 22, 31, 21, 15, 11,  4,  6,  6,  1,  1,  1]))",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Science with Penguins</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html",
    "href": "06-sklearn_pipelines.html",
    "title": "6¬† Sklearn pipelines",
    "section": "",
    "text": "Learning Objectives\nAfter completing this lesson, you will: - understand the need for preprocessing data in general - recognize pitfalls and ‚Äúdanger‚Äù in the process - appreciate the structured and systematic approach offered by pipelines",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#mlr-on-housing-data",
    "href": "06-sklearn_pipelines.html#mlr-on-housing-data",
    "title": "6¬† Sklearn pipelines",
    "section": "1. MLR on housing data",
    "text": "1. MLR on housing data\nWe will use the California Housing Dataset from sklearn.datasets.\nIt contains data on housing prices and district-level demographics in California.\n\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\nimport numpy as np\n\ndata = fetch_california_housing(as_frame=True)\ndf = data.frame\ndf.head()\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n\n\n\n\n\nThe variable MedHouseVal is the median house value in $100,000s.\nWe will predict it using a few numeric predictors.\n\nSelect a Subset of Variables\nUse only the following features:\n\n\n\nFeature\nDescription\n\n\n\n\nMedInc\nMedian income in block group\n\n\nAveRooms\nAverage number of rooms per household\n\n\nAveBedrms\nAverage number of bedrooms per household\n\n\nPopulation\nBlock group population\n\n\nHouseAge\nMedian age of houses in the block group\n\n\n\n\ncols = ['MedHouseVal', 'MedInc', 'AveRooms', 'AveBedrms', 'Population', 'HouseAge']\ndf_sub = df[cols]\ndf_sub.head()\n\n\n\n\n\n\n\n\nMedHouseVal\nMedInc\nAveRooms\nAveBedrms\nPopulation\nHouseAge\n\n\n\n\n0\n4.526\n8.3252\n6.984127\n1.023810\n322.0\n41.0\n\n\n1\n3.585\n8.3014\n6.238137\n0.971880\n2401.0\n21.0\n\n\n2\n3.521\n7.2574\n8.288136\n1.073446\n496.0\n52.0\n\n\n3\n3.413\n5.6431\n5.817352\n1.073059\n558.0\n52.0\n\n\n4\n3.422\n3.8462\n6.281853\n1.081081\n565.0\n52.0\n\n\n\n\n\n\n\n\n\nMultiple Regression using statsmodels\nUse statsmodels.api.OLS to fit a multiple regression model.\n\nimport statsmodels.api as sm\n\nX = df_sub[['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'HouseAge']]\ny = df_sub['MedHouseVal']\n\nX = sm.add_constant(X)\nmodel_sm = sm.OLS(y, X).fit()\nprint(model_sm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            MedHouseVal   R-squared:                       0.538\nModel:                            OLS   Adj. R-squared:                  0.538\nMethod:                 Least Squares   F-statistic:                     4801.\nDate:                Tue, 25 Nov 2025   Prob (F-statistic):               0.00\nTime:                        17:02:38   Log-Likelihood:                -24278.\nNo. Observations:               20640   AIC:                         4.857e+04\nDf Residuals:                   20634   BIC:                         4.862e+04\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -0.4407      0.028    -15.945      0.000      -0.495      -0.387\nMedInc         0.5360      0.004    129.735      0.000       0.528       0.544\nAveRooms      -0.2112      0.006    -35.074      0.000      -0.223      -0.199\nAveBedrms      0.9908      0.030     33.503      0.000       0.933       1.049\nPopulation  1.848e-05   5.09e-06      3.628      0.000     8.5e-06    2.85e-05\nHouseAge       0.0163      0.000     35.182      0.000       0.015       0.017\n==============================================================================\nOmnibus:                     4414.212   Durbin-Watson:                   0.938\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            16609.838\nSkew:                           1.035   Prob(JB):                         0.00\nKurtosis:                       6.876   Cond. No.                     1.16e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.16e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#predictions",
    "href": "06-sklearn_pipelines.html#predictions",
    "title": "6¬† Sklearn pipelines",
    "section": "Predictions",
    "text": "Predictions\n\n# New data equal to an average house !\nX_new = X.mean()\nprint(pd.DataFrame(X_new))\n\n# Predict\npred = model_sm.predict(X_new)\nprint(pred)\nbeta = model_sm.params\nprint(X_new @ beta)\n\n                      0\nconst          1.000000\nMedInc         3.870671\nAveRooms       5.429000\nAveBedrms      1.096675\nPopulation  1425.476744\nHouseAge      28.639486\nNone    2.068558\ndtype: float64\n2.0685581690891244\n\n\n\ndef print_dot_product(beta, x, names, intercept_name=\"Intercept\"):\n    terms = []\n    for b, v, n in zip(beta, x, names):\n        terms.append(f\"{b:.3f}*{v:.3f}\")\n    equation = \" + \".join(terms)\n    print(equation)\n\nprint_dot_product(beta, X_new, names = beta.index.tolist())\n\n-0.441*1.000 + 0.536*3.871 + -0.211*5.429 + 0.991*1.097 + 0.000*1425.477 + 0.016*28.639",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#scikit-learn",
    "href": "06-sklearn_pipelines.html#scikit-learn",
    "title": "6¬† Sklearn pipelines",
    "section": "scikit-learn",
    "text": "scikit-learn\nFrom now on we will switch almost entirely to the sklearn library!\nLet us fit the same model using LinearRegression.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nX = df_sub[['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'HouseAge']]\ny = df_sub['MedHouseVal']\n\nmodel_sk = LinearRegression()\nmodel_sk.fit(X, y)\n\nprint(\"Intercept:\", model_sk.intercept_)\nprint(\"Coefficients:\", model_sk.coef_)\n\nIntercept: -0.440721743746872\nCoefficients: [ 5.36014757e-01 -2.11185756e-01  9.90813314e-01  1.84789639e-05\n  1.63455751e-02]\n\n\n\n# Predictions\nX_new_sk = X.mean().to_frame().T ## convert Series ‚Üí DataFrame with one row\n#print(X_new_sk)\npred = model_sk.predict(X_new_sk)\nprint(pred)\n\n[2.06855817]\n\n\n\n# Compute R¬≤\ny_pred = model_sk.predict(X)\nr2_score(y, y_pred)\n\n0.5377839208402416",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#data-scaling",
    "href": "06-sklearn_pipelines.html#data-scaling",
    "title": "6¬† Sklearn pipelines",
    "section": "Data Scaling",
    "text": "Data Scaling\nMany ML methods need data to be scaled\n\ndf_sub.describe()\n\n\n\n\n\n\n\n\nMedHouseVal\nMedInc\nAveRooms\nAveBedrms\nPopulation\nHouseAge\n\n\n\n\ncount\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n20640.000000\n\n\nmean\n2.068558\n3.870671\n5.429000\n1.096675\n1425.476744\n28.639486\n\n\nstd\n1.153956\n1.899822\n2.474173\n0.473911\n1132.462122\n12.585558\n\n\nmin\n0.149990\n0.499900\n0.846154\n0.333333\n3.000000\n1.000000\n\n\n25%\n1.196000\n2.563400\n4.440716\n1.006079\n787.000000\n18.000000\n\n\n50%\n1.797000\n3.534800\n5.229129\n1.048780\n1166.000000\n29.000000\n\n\n75%\n2.647250\n4.743250\n6.052381\n1.099526\n1725.000000\n37.000000\n\n\nmax\n5.000010\n15.000100\n141.909091\n34.066667\n35682.000000\n52.000000\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n#Manual:\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit + transform\nX_scaled = scaler.fit_transform(X)\n\n# Convert back to a DataFrame with the same column names\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n\nX_scaled.describe()\n\n\n\n\n\n\n\n\nMedInc\nAveRooms\nAveBedrms\nPopulation\nHouseAge\n\n\n\n\ncount\n2.064000e+04\n2.064000e+04\n2.064000e+04\n2.064000e+04\n2.064000e+04\n\n\nmean\n6.609700e-17\n6.609700e-17\n-1.060306e-16\n-1.101617e-17\n5.508083e-18\n\n\nstd\n1.000024e+00\n1.000024e+00\n1.000024e+00\n1.000024e+00\n1.000024e+00\n\n\nmin\n-1.774299e+00\n-1.852319e+00\n-1.610768e+00\n-1.256123e+00\n-2.196180e+00\n\n\n25%\n-6.881186e-01\n-3.994496e-01\n-1.911716e-01\n-5.638089e-01\n-8.453931e-01\n\n\n50%\n-1.767951e-01\n-8.078489e-02\n-1.010650e-01\n-2.291318e-01\n2.864572e-02\n\n\n75%\n4.593063e-01\n2.519615e-01\n6.015869e-03\n2.644949e-01\n6.643103e-01\n\n\nmax\n5.858286e+00\n5.516324e+01\n6.957171e+01\n3.025033e+01\n1.856182e+00\n\n\n\n\n\n\n\n\nmodel_sk = LinearRegression()\nmodel_sk.fit(X_scaled, y)\n\nprint(\"Intercept:\", model_sk.intercept_)\nprint(\"Coefficients:\", model_sk.coef_)\n\nIntercept: 2.068558169089147\nCoefficients: [ 1.01830781 -0.52249747  0.46954581  0.02092622  0.20571319]",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#train-test-splits",
    "href": "06-sklearn_pipelines.html#train-test-splits",
    "title": "6¬† Sklearn pipelines",
    "section": "Train Test Splits",
    "text": "Train Test Splits\nIn the absence of ‚Äúnew‚Äù data we can simulate the process by splitting the data set ourselves and calling one part ‚Äútraining‚Äù and the other ‚Äútest‚Äù data.\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nprint(X.shape)\nprint(X_train.shape)\nprint(X_test.shape)\n\n(20640, 5)\n(16512, 5)\n(4128, 5)",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#putting-it-all-together",
    "href": "06-sklearn_pipelines.html#putting-it-all-together",
    "title": "6¬† Sklearn pipelines",
    "section": "Putting it ‚Äúall‚Äù together",
    "text": "Putting it ‚Äúall‚Äù together\nWe should apply most preprocessing steps to both training and test data. That is easier said than done, because\n\nwe need to apply the identical algorithm to both parts, and\nwe need to avoid data leakage!\n\nImagine the overhead in bookkeeping of manually have to store all the parameters from scaling and e.g.¬†mean/median imputations, etc‚Ä¶\nThat is where pipelines come in and make life much easier.\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\npipe = Pipeline([\n    (\"scale\", StandardScaler()),\n    ('model', LinearRegression())\n])\n\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('scale', StandardScaler()), ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scale', StandardScaler()), ('model', LinearRegression())])StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\n\nScore the Pipeline\nThe code below looks very simple and innocent but just pause and think what all is going on here !\n\ntest_score = pipe.score(X_test, y_test)\ntest_score\n\n0.5089947802907753",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#missing-values",
    "href": "06-sklearn_pipelines.html#missing-values",
    "title": "6¬† Sklearn pipelines",
    "section": "Missing values",
    "text": "Missing values\n\n# Set specific rows to missing\n\n#Example: First 50 rows:\n\ndf_sub.loc[:49, \"HouseAge\"] = np.nan\n\nX = df_sub[['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'HouseAge']]\ny = df_sub['MedHouseVal']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nmodel_sk = LinearRegression()\n#model_sk.fit(X_train, y_train)",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#full-pipeline",
    "href": "06-sklearn_pipelines.html#full-pipeline",
    "title": "6¬† Sklearn pipelines",
    "section": "Full Pipeline",
    "text": "Full Pipeline\nIncluding an imputer!\n\nfrom sklearn.impute import SimpleImputer\n\npipe = Pipeline([\n    (\"impute\", SimpleImputer(strategy=\"median\")),\n    (\"scale\", StandardScaler()),\n    (\"model\", LinearRegression())\n])\n#up to here it's only been setup work\n\npipe.fit(X_train, y_train)#all the action is here !!\n\nPipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('scale', StandardScaler()), ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('impute', SimpleImputer(strategy='median')),\n                ('scale', StandardScaler()), ('model', LinearRegression())])SimpleImputerSimpleImputer(strategy='median')StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\n\ntest_score = pipe.score(X_test, y_test)\ntest_score\n\n0.5089722962899934\n\n\n\ntrain_score = pipe.score(X_train, y_train)\ntrain_score\n\n0.5436563358368143",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#onehot-encoder",
    "href": "06-sklearn_pipelines.html#onehot-encoder",
    "title": "6¬† Sklearn pipelines",
    "section": "OneHot Encoder",
    "text": "OneHot Encoder\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npenguins = sns.load_dataset(\"penguins\").dropna()\npenguins\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\nGentoo\nBiscoe\n47.2\n13.7\n214.0\n4925.0\nFemale\n\n\n340\nGentoo\nBiscoe\n46.8\n14.3\n215.0\n4850.0\nFemale\n\n\n341\nGentoo\nBiscoe\n50.4\n15.7\n222.0\n5750.0\nMale\n\n\n342\nGentoo\nBiscoe\n45.2\n14.8\n212.0\n5200.0\nFemale\n\n\n343\nGentoo\nBiscoe\n49.9\n16.1\n213.0\n5400.0\nMale\n\n\n\n\n333 rows √ó 7 columns\n\n\n\n\n#train/test split 80/20\n\nfrom sklearn.model_selection import train_test_split\n\n#penguins = penguins.dropna()\n\n#X\nX = penguins.drop(columns=\"body_mass_g\")\n#\ny = penguins.body_mass_g\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\nX_train\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nsex\n\n\n\n\n230\nGentoo\nBiscoe\n40.9\n13.7\n214.0\nFemale\n\n\n84\nAdelie\nDream\n37.3\n17.8\n191.0\nFemale\n\n\n303\nGentoo\nBiscoe\n50.0\n15.9\n224.0\nMale\n\n\n22\nAdelie\nBiscoe\n35.9\n19.2\n189.0\nFemale\n\n\n29\nAdelie\nBiscoe\n40.5\n18.9\n180.0\nMale\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n194\nChinstrap\nDream\n50.9\n19.1\n196.0\nMale\n\n\n77\nAdelie\nTorgersen\n37.2\n19.4\n184.0\nMale\n\n\n112\nAdelie\nBiscoe\n39.7\n17.7\n193.0\nFemale\n\n\n277\nGentoo\nBiscoe\n45.5\n15.0\n220.0\nMale\n\n\n108\nAdelie\nBiscoe\n38.1\n17.0\n181.0\nFemale\n\n\n\n\n266 rows √ó 6 columns\n\n\n\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\ncategorical = X_train.select_dtypes(include=['object']).columns\nnumeric = X_train.select_dtypes(include=['number']).columns\n\ncategorical\nnumeric\n\nIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object')\n\n\n\npreprocessor = ColumnTransformer([\n    ('cat', OneHotEncoder(handle_unknown='ignore', drop=\"first\"), categorical),\n    ('num', StandardScaler(), numeric)\n])\n\npreprocessor\n\nColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore'),\n                                 Index(['species', 'island', 'sex'], dtype='object')),\n                                ('num', StandardScaler(),\n                                 Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore'),\n                                 Index(['species', 'island', 'sex'], dtype='object')),\n                                ('num', StandardScaler(),\n                                 Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])catIndex(['species', 'island', 'sex'], dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore')numIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object')StandardScalerStandardScaler()\n\n\n\npipe = Pipeline([\n    #(\"impute\", SimpleImputer(strategy=\"median\")),\n    ('preprocess', preprocessor)\n    #('model', LinearRegression())\n])\n\n\npipe = Pipeline([\n    #(\"impute\", SimpleImputer(strategy=\"median\")),\n    ('preprocess', preprocessor),\n    ('model', LinearRegression())\n])\n\npipe\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore'),\n                                                  Index(['species', 'island', 'sex'], dtype='object')),\n                                                 ('num', StandardScaler(),\n                                                  Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore'),\n                                                  Index(['species', 'island', 'sex'], dtype='object')),\n                                                 ('num', StandardScaler(),\n                                                  Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])),\n                ('model', LinearRegression())])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore'),\n                                 Index(['species', 'island', 'sex'], dtype='object')),\n                                ('num', StandardScaler(),\n                                 Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])catIndex(['species', 'island', 'sex'], dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore')numIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object')StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\n\n#Let us do the work\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore'),\n                                                  Index(['species', 'island', 'sex'], dtype='object')),\n                                                 ('num', StandardScaler(),\n                                                  Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])),\n                ('model', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('cat',\n                                                  OneHotEncoder(drop='first',\n                                                                handle_unknown='ignore'),\n                                                  Index(['species', 'island', 'sex'], dtype='object')),\n                                                 ('num', StandardScaler(),\n                                                  Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])),\n                ('model', LinearRegression())])preprocess: ColumnTransformerColumnTransformer(transformers=[('cat',\n                                 OneHotEncoder(drop='first',\n                                               handle_unknown='ignore'),\n                                 Index(['species', 'island', 'sex'], dtype='object')),\n                                ('num', StandardScaler(),\n                                 Index(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object'))])catIndex(['species', 'island', 'sex'], dtype='object')OneHotEncoderOneHotEncoder(drop='first', handle_unknown='ignore')numIndex(['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm'], dtype='object')StandardScalerStandardScaler()LinearRegressionLinearRegression()\n\n\nHow do you judge the qualitry of a regression ?\n\npipe.score(X_test, y_test)\n\n\n0.8961688345769456",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#the-predictors-explain-89-of-the-variation-of-body-mass",
    "href": "06-sklearn_pipelines.html#the-predictors-explain-89-of-the-variation-of-body-mass",
    "title": "6¬† Sklearn pipelines",
    "section": "The predictors explain 89% of the variation of body mass !",
    "text": "The predictors explain 89% of the variation of body mass !\n‚ÄúTransform‚Äù is only sensible for steops UP TO the final model\n\n\npipe = Pipeline([\n    #(\"impute\", SimpleImputer(strategy=\"median\")),\n    ('preprocess', preprocessor),\n    ('model', LinearRegression())\n])\n\nX_trans = pipe[:-1].transform(X_train)\npd.DataFrame(X_trans)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\n0\n0.0\n1.0\n0.0\n0.0\n0.0\n-0.593727\n-1.750939\n0.935943\n\n\n1\n0.0\n0.0\n1.0\n0.0\n0.0\n-1.261043\n0.323107\n-0.719956\n\n\n2\n0.0\n1.0\n0.0\n0.0\n1.0\n1.093099\n-0.638036\n1.655899\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.520555\n1.031318\n-0.863947\n\n\n4\n0.0\n0.0\n0.0\n0.0\n1.0\n-0.667873\n0.879558\n-1.511908\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n261\n1.0\n0.0\n1.0\n0.0\n1.0\n1.259928\n0.980731\n-0.359978\n\n\n262\n0.0\n0.0\n0.0\n1.0\n1.0\n-1.279579\n1.132491\n-1.223925\n\n\n263\n0.0\n0.0\n0.0\n0.0\n0.0\n-0.816166\n0.272520\n-0.575965\n\n\n264\n0.0\n1.0\n0.0\n0.0\n1.0\n0.258954\n-1.093315\n1.367916\n\n\n265\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.112750\n-0.081585\n-1.439912\n\n\n\n\n266 rows √ó 8 columns\n\n\n\n#Task: fit a linear regression without a pipe\nComment: you still need to one-hot encode !\n\nnumeric_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\ncategorical_cols = [\"species\", \"island\", \"sex\"]\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(drop=None, handle_unknown=\"ignore\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\n\nX_trans = preprocess.fit_transform(X_train[categorical_cols + numeric_cols])\n\ncatNames = preprocess.named_transformers_[\"cat\"].get_feature_names_out()\n#numNames = preprocess.named_transformers_[\"num\"].get_feature_names_out()\n#list(catNames)+list(numNames)\n#X_df = pd.DataFrame(X_trans, columns=list(catNames)+list(numNames))\n#X_df.head()\n\n\nX_train_oh = pd.get_dummies(X_train)\nX_train_oh\n#X_train_oh = pipe[:-1].transform(X_train)\n#pd.DataFrame(X_train_oh)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nspecies_Adelie\nspecies_Chinstrap\nspecies_Gentoo\nisland_Biscoe\nisland_Dream\nisland_Torgersen\nsex_Female\nsex_Male\n\n\n\n\n230\n40.9\n13.7\n214.0\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n84\n37.3\n17.8\n191.0\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n303\n50.0\n15.9\n224.0\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n22\n35.9\n19.2\n189.0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n29\n40.5\n18.9\n180.0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n194\n50.9\n19.1\n196.0\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n77\n37.2\n19.4\n184.0\nTrue\nFalse\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n112\n39.7\n17.7\n193.0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n277\n45.5\n15.0\n220.0\nFalse\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n108\n38.1\n17.0\n181.0\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n266 rows √ó 11 columns\n\n\n\n\nmodel_sk = LinearRegression()\n\nmodel_sk.fit(X_train_oh, y_train)\n\nprint(\"Intercept:\", model_sk.intercept_)\nprint(\"Coefficients:\", model_sk.coef_)\n\nIntercept: -846.0265146981037\nCoefficients: [  17.14973198   66.91629434   15.30729934 -266.29400401 -514.5295567\n  780.82356071   13.14977171   26.0672393   -39.21701101 -195.50683155\n  195.50683155]\n\n\n\npenguins.columns\n\nIndex(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n       'flipper_length_mm', 'body_mass_g', 'sex'],\n      dtype='object')\n\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\nlm2a = smf.ols('body_mass_g ~ species + island + bill_length_mm +bill_depth_mm + flipper_length_mm + sex', penguins).fit()\nlm2a.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-1500.0291\n575.822\n-2.605\n0.010\n-2632.852\n-367.207\n\n\nspecies[T.Chinstrap]\n-260.3063\n88.551\n-2.940\n0.004\n-434.513\n-86.100\n\n\nspecies[T.Gentoo]\n987.7614\n137.238\n7.197\n0.000\n717.771\n1257.752\n\n\nisland[T.Dream]\n-13.1031\n58.541\n-0.224\n0.823\n-128.271\n102.065\n\n\nisland[T.Torgersen]\n-48.0636\n60.922\n-0.789\n0.431\n-167.915\n71.788\n\n\nsex[T.Male]\n387.2243\n48.138\n8.044\n0.000\n292.521\n481.927\n\n\nbill_length_mm\n18.1893\n7.136\n2.549\n0.011\n4.150\n32.229\n\n\nbill_depth_mm\n67.5754\n19.821\n3.409\n0.001\n28.581\n106.570\n\n\nflipper_length_mm\n16.2385\n2.939\n5.524\n0.000\n10.456\n22.021",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "06-sklearn_pipelines.html#diamonds",
    "href": "06-sklearn_pipelines.html#diamonds",
    "title": "6¬† Sklearn pipelines",
    "section": "Diamonds",
    "text": "Diamonds\nTasks:\n\nFit a linear regression (price as outcome) including all columns !\nGet the R^2 on the test data and compare to training\nWhich features seem important\nWhat about x,y,z ? Do they make sense to include in a linear fashion ?\n\n\nimport statsmodels.api as sm\ndf = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\ndf\n\n\n  \n    \n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n53935\n0.72\nIdeal\nD\nSI1\n60.8\n57.0\n2757\n5.75\n5.76\n3.50\n\n\n53936\n0.72\nGood\nD\nSI1\n63.1\n55.0\n2757\n5.69\n5.75\n3.61\n\n\n53937\n0.70\nVery Good\nD\nSI1\n62.8\n60.0\n2757\n5.66\n5.68\n3.56\n\n\n53938\n0.86\nPremium\nH\nSI2\n61.0\n58.0\n2757\n6.15\n6.12\n3.74\n\n\n53939\n0.75\nIdeal\nD\nSI2\n62.2\n55.0\n2757\n5.83\n5.87\n3.64\n\n\n\n\n53940 rows √ó 10 columns",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Sklearn pipelines</span>"
    ]
  },
  {
    "objectID": "07-PolynomialFeatures.html",
    "href": "07-PolynomialFeatures.html",
    "title": "7¬† Polynomial Features",
    "section": "",
    "text": "The Diamonds data\nhttps://bookdown.org/yih_huynh/Guide-to-R-Book/diamonds.html\nTasks:\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# -------------------------------------------------------\n# 1. Load diamonds data\n# -------------------------------------------------------\ndf = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\n\n# Outcome:\ny = df[\"price\"]\n\n# Predictors:\nX = df.drop(columns=[\"price\"])\nprint(\"original number of columns:\", X.shape[1])\n\n# Identify categorical vs numeric columns\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\ncat_cols = categorical_cols\nnumeric_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n\ndf\n\n\noriginal number of columns: 9\n\n\n\n  \n    \n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n53935\n0.72\nIdeal\nD\nSI1\n60.8\n57.0\n2757\n5.75\n5.76\n3.50\n\n\n53936\n0.72\nGood\nD\nSI1\n63.1\n55.0\n2757\n5.69\n5.75\n3.61\n\n\n53937\n0.70\nVery Good\nD\nSI1\n62.8\n60.0\n2757\n5.66\n5.68\n3.56\n\n\n53938\n0.86\nPremium\nH\nSI2\n61.0\n58.0\n2757\n6.15\n6.12\n3.74\n\n\n53939\n0.75\nIdeal\nD\nSI2\n62.2\n55.0\n2757\n5.83\n5.87\n3.64\n\n\n\n\n53940 rows √ó 10 columns\n# Save to Colab's local filesystem\ndf.to_csv(\"diamonds.csv\", index=False)\nPolynomialFeatures from scikit-learn can only be applied to numeric input. So if you have mixed data types (VERY common!), such as the diamonds dataset (which contains categorical columns like cut, color, clarity), you must first convert the categorical variables into numeric form (e.g., one-hot encoding), then apply PolynomialFeatures.\nBelow are the typical ways to do it.",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynomial Features</span>"
    ]
  },
  {
    "objectID": "07-PolynomialFeatures.html#solution",
    "href": "07-PolynomialFeatures.html#solution",
    "title": "7¬† Polynomial Features",
    "section": "Solution",
    "text": "Solution",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynomial Features</span>"
    ]
  },
  {
    "objectID": "07-PolynomialFeatures.html#statsmodels",
    "href": "07-PolynomialFeatures.html#statsmodels",
    "title": "7¬† Polynomial Features",
    "section": "Statsmodels",
    "text": "Statsmodels\n\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\"price ~ carat + depth + cut + color + clarity+ depth+ table + x + y+ z\", data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.916\nModel:                            OLS   Adj. R-squared:                  0.916\nMethod:                 Least Squares   F-statistic:                 2.942e+04\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        13:58:49   Log-Likelihood:            -4.5696e+05\nNo. Observations:               53940   AIC:                         9.140e+05\nDf Residuals:                   53919   BIC:                         9.141e+05\nDf Model:                          20                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept        -4555.1714    373.482    -12.197      0.000   -5287.199   -3823.144\ncut[T.Good]        614.4239     34.337     17.894      0.000     547.122     681.726\ncut[T.Ideal]       877.5691     34.152     25.696      0.000     810.631     944.507\ncut[T.Premium]     806.0243     32.954     24.459      0.000     741.434     870.615\ncut[T.Very Good]   778.4282     32.936     23.635      0.000     713.874     842.982\ncolor[T.E]        -210.8490     18.304    -11.519      0.000    -246.726    -174.972\ncolor[T.F]        -304.2876     18.498    -16.450      0.000    -340.543    -268.032\ncolor[T.G]        -506.9637     18.116    -27.984      0.000    -542.472    -471.456\ncolor[T.H]        -977.9737     19.269    -50.754      0.000   -1015.741    -940.207\ncolor[T.I]       -1438.2773     21.642    -66.459      0.000   -1480.695   -1395.860\ncolor[T.J]       -2322.5649     26.715    -86.940      0.000   -2374.926   -2270.204\nclarity[T.IF]     5404.2365     52.174    103.582      0.000    5301.976    5506.497\nclarity[T.SI1]    3567.7938     44.587     80.020      0.000    3480.404    3655.184\nclarity[T.SI2]    2619.0040     44.788     58.476      0.000    2531.220    2706.788\nclarity[T.VS1]    4525.4001     45.547     99.356      0.000    4436.127    4614.673\nclarity[T.VS2]    4210.1943     44.840     93.893      0.000    4122.307    4298.082\nclarity[T.VVS1]   5061.7344     48.224    104.964      0.000    4967.216    5156.253\nclarity[T.VVS2]   4957.3104     46.901    105.697      0.000    4865.384    5049.237\ncarat             8895.1940     12.079    736.390      0.000    8871.518    8918.870\ndepth              -21.0236      4.079     -5.154      0.000     -29.019     -13.028\ntable              -24.8027      2.978     -8.329      0.000     -30.639     -18.966\n==============================================================================\nOmnibus:                    15255.775   Durbin-Watson:                   0.908\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           184788.880\nSkew:                           1.016   Prob(JB):                         0.00\nKurtosis:                      11.837   Cond. No.                     6.35e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.35e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nmodel = smf.ols(\"carat ~  x + y+ z\", data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  carat   R-squared:                       0.952\nModel:                            OLS   Adj. R-squared:                  0.952\nMethod:                 Least Squares   F-statistic:                 3.536e+05\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        14:00:24   Log-Likelihood:                 45412.\nNo. Observations:               53940   AIC:                        -9.082e+04\nDf Residuals:                   53936   BIC:                        -9.078e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.5668      0.002   -669.401      0.000      -1.571      -1.562\nx              0.3591      0.002    156.316      0.000       0.355       0.364\ny              0.0052      0.002      2.914      0.004       0.002       0.009\nz              0.0784      0.003     29.396      0.000       0.073       0.084\n==============================================================================\nOmnibus:                    57670.233   Durbin-Watson:                   0.784\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         25221338.363\nSkew:                           4.766   Prob(JB):                         0.00\nKurtosis:                     108.504   Cond. No.                         65.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynomial Features</span>"
    ]
  },
  {
    "objectID": "07-PolynomialFeatures.html#interactions",
    "href": "07-PolynomialFeatures.html#interactions",
    "title": "7¬† Polynomial Features",
    "section": "Interactions",
    "text": "Interactions\nAlso known in the business context as synergies\n\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\"price ~ carat + depth\", data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.851\nModel:                            OLS   Adj. R-squared:                  0.851\nMethod:                 Least Squares   F-statistic:                 1.536e+05\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        13:35:39   Log-Likelihood:            -4.7249e+05\nNo. Observations:               53940   AIC:                         9.450e+05\nDf Residuals:                   53937   BIC:                         9.450e+05\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   4045.3332    286.205     14.134      0.000    3484.368    4606.298\ncarat       7765.1407     14.009    554.282      0.000    7737.682    7792.599\ndepth       -102.1653      4.635    -22.041      0.000    -111.251     -93.080\n==============================================================================\nOmnibus:                    14148.858   Durbin-Watson:                   0.992\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           148236.675\nSkew:                           0.962   Prob(JB):                         0.00\nKurtosis:                      10.890   Cond. No.                     2.66e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.66e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nmodel = smf.ols(\"price ~ carat * depth\"), data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  price   R-squared:                       0.852\nModel:                            OLS   Adj. R-squared:                  0.852\nMethod:                 Least Squares   F-statistic:                 1.036e+05\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        14:05:07   Log-Likelihood:            -4.7223e+05\nNo. Observations:               53940   AIC:                         9.445e+05\nDf Residuals:                   53936   BIC:                         9.445e+05\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -7823.7383    592.049    -13.215      0.000   -8984.158   -6663.318\ncarat        2.074e+04    567.672     36.540      0.000    1.96e+04    2.19e+04\ndepth          90.0432      9.588      9.391      0.000      71.251     108.836\ncarat:depth  -210.0753      9.187    -22.868      0.000    -228.081    -192.070\n==============================================================================\nOmnibus:                    14524.513   Durbin-Watson:                   0.995\nProb(Omnibus):                  0.000   Jarque-Bera (JB):           134929.068\nSkew:                           1.030   Prob(JB):                         0.00\nKurtosis:                      10.469   Cond. No.                     9.78e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.78e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nsklearn\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Load data\ndf = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\n\nX = df[[\"carat\", \"depth\"]]    # same variables as in statsmodels example\n\npoly = PolynomialFeatures(\n    degree=2,\n    include_bias=False,\n    interaction_only=True\n)\n\nX_poly = poly.fit_transform(X)\n\nprint(\"Feature names:\", poly.get_feature_names_out([\"carat\", \"depth\"]))\nprint(pd.DataFrame(X_poly, columns=poly.get_feature_names_out([\"carat\", \"depth\"])).head())\n\nFeature names: ['carat' 'depth' 'carat depth']\n   carat  depth  carat depth\n0   0.23   61.5       14.145\n1   0.21   59.8       12.558\n2   0.23   56.9       13.087\n3   0.29   62.4       18.096\n4   0.31   63.3       19.623\n\n\n\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\n\n# Load diamonds\n#df = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\n\nX = df[[\"carat\", \"depth\"]]   # same variables as in statsmodels formula\ny = df[\"price\"]\n\n# Pipeline: interaction terms only\npipe = Pipeline([\n    (\"interaction\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n    (\"model\", LinearRegression())\n])\n\npipe.fit(X, y)\n\nprint(\"Coefficients:\", pipe.named_steps[\"model\"].coef_)\nprint(\"Intercept:\", pipe.named_steps[\"model\"].intercept_)\n\nCoefficients: [20742.59987132    90.04321842  -210.07533218]\nIntercept: -7823.738250998009\n\n\nshow the expanded feature matrix\n\nimport numpy as np\n\nX_trans = pipe.named_steps[\"interaction\"].fit_transform(X)\nprint(X_trans[:5])\n\n[[ 0.23  61.5   14.145]\n [ 0.21  59.8   12.558]\n [ 0.23  56.9   13.087]\n [ 0.29  62.4   18.096]\n [ 0.31  63.3   19.623]]",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Polynomial Features</span>"
    ]
  },
  {
    "objectID": "08-Patsy_Interactions.html",
    "href": "08-Patsy_Interactions.html",
    "title": "8¬† Patsy",
    "section": "",
    "text": "The Diamonds Data again\nhttps://bookdown.org/yih_huynh/Guide-to-R-Book/diamonds.html\nTasks:\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# -------------------------------------------------------\n# 1. Load diamonds data\n# -------------------------------------------------------\n#df = sm.datasets.get_rdataset(\"diamonds\", \"ggplot2\").data\ndf = pd.read_csv(\"diamonds.csv\")\n\n# Outcome:\ny = df[\"price\"]\n\n# Predictors:\nX = df.drop(columns=[\"price\"])\nprint(\"original number of columns:\", X.shape[1])\n\n# Identify categorical vs numeric columns\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\ncat_cols = categorical_cols\nnumeric_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n\ndf\n\noriginal number of columns: 9\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n53935\n0.72\nIdeal\nD\nSI1\n60.8\n57.0\n2757\n5.75\n5.76\n3.50\n\n\n53936\n0.72\nGood\nD\nSI1\n63.1\n55.0\n2757\n5.69\n5.75\n3.61\n\n\n53937\n0.70\nVery Good\nD\nSI1\n62.8\n60.0\n2757\n5.66\n5.68\n3.56\n\n\n53938\n0.86\nPremium\nH\nSI2\n61.0\n58.0\n2757\n6.15\n6.12\n3.74\n\n\n53939\n0.75\nIdeal\nD\nSI2\n62.2\n55.0\n2757\n5.83\n5.87\n3.64\n\n\n\n\n53940 rows √ó 10 columns\nPolynomialFeatures from scikit-learn can only be applied to numeric input.\nStatsmodels allows powerful and compact formula notation, which automatically handles categorical features:\nimport statsmodels.formula.api as smf\n\nmodel = smf.ols(\"price ~ carat*depth\", data=df).fit()\nprint(model.summary().tables[1])\n\n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept   -7823.7383    592.049    -13.215      0.000   -8984.158   -6663.318\ncarat        2.074e+04    567.672     36.540      0.000    1.96e+04    2.19e+04\ndepth          90.0432      9.588      9.391      0.000      71.251     108.836\ncarat:depth  -210.0753      9.187    -22.868      0.000    -228.081    -192.070\n===============================================================================\nmodel = smf.ols(\"price ~ carat*C(cut)\", data=df).fit()\nprint(model.summary().tables[1])\n\n=============================================================================================\n                                coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------\nIntercept                 -1839.0737     84.354    -21.802      0.000   -2004.408   -1673.739\nC(cut)[T.Good]             -583.6544     95.774     -6.094      0.000    -771.372    -395.936\nC(cut)[T.Ideal]            -461.3000     86.569     -5.329      0.000    -630.975    -291.625\nC(cut)[T.Premium]          -540.8313     88.124     -6.137      0.000    -713.554    -368.108\nC(cut)[T.Very Good]        -578.5866     88.732     -6.521      0.000    -752.502    -404.672\ncarat                      5924.4951     72.309     81.933      0.000    5782.769    6066.221\ncarat:C(cut)[T.Good]       1555.1412     86.296     18.021      0.000    1386.000    1724.283\ncarat:C(cut)[T.Ideal]      2267.8962     76.053     29.820      0.000    2118.831    2416.961\ncarat:C(cut)[T.Premium]    1883.2569     76.429     24.641      0.000    1733.456    2033.057\ncarat:C(cut)[T.Very Good]  2011.4769     78.156     25.737      0.000    1858.291    2164.663\n=============================================================================================\nmodel = smf.ols(\"carat ~  x + y+ z\", data=df).fit()\n\nprint(model.summary())#.tables[1])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  carat   R-squared:                       0.952\nModel:                            OLS   Adj. R-squared:                  0.952\nMethod:                 Least Squares   F-statistic:                 3.536e+05\nDate:                Wed, 19 Nov 2025   Prob (F-statistic):               0.00\nTime:                        14:00:24   Log-Likelihood:                 45412.\nNo. Observations:               53940   AIC:                        -9.082e+04\nDf Residuals:                   53936   BIC:                        -9.078e+04\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -1.5668      0.002   -669.401      0.000      -1.571      -1.562\nx              0.3591      0.002    156.316      0.000       0.355       0.364\ny              0.0052      0.002      2.914      0.004       0.002       0.009\nz              0.0784      0.003     29.396      0.000       0.073       0.084\n==============================================================================\nOmnibus:                    57670.233   Durbin-Watson:                   0.784\nProb(Omnibus):                  0.000   Jarque-Bera (JB):         25221338.363\nSkew:                           4.766   Prob(JB):                         0.00\nKurtosis:                     108.504   Cond. No.                         65.2\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Patsy</span>"
    ]
  },
  {
    "objectID": "08-Patsy_Interactions.html#patsy",
    "href": "08-Patsy_Interactions.html#patsy",
    "title": "8¬† Patsy",
    "section": "patsy",
    "text": "patsy\nBringing Statsmodels (R) formula notation to sklearn !\n‚Ä¶and automatically get:\n\nDesign matrices with dummy coding\nPolynomial terms, interactions, splines, contrasts\nFeature Names\nA record of transformations, so predictions on new data work\nNo manual preprocessing (no OneHotEncoder, no PolynomialFeatures)\n\nAll of this is extremely useful before passing the data into sklearn, which only expects numeric matrices.\n\nimport pandas as pd\nimport patsy\nfrom sklearn.linear_model import LinearRegression\n\n# ---------------------------\n# 1. Create example data\n# ---------------------------\ndf = pd.DataFrame({\n    \"y\":   [1, 2, 3, 4, 5, 6],\n    \"x1\":  [1, 2, 3, 4, 5, 6],\n    \"x2\":  [2, 1, 2, 1, 2, 1],\n    \"grp\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\"]\n})\n\n# ---------------------------\n# 2. Build the design matrices with patsy\n# ---------------------------\n# R-style formula:\nformula = \"y ~ x1 + x2 + C(grp) + x1:x2 + I(x1**2)\"\n\ny, X = patsy.dmatrices(formula, df, return_type=\"dataframe\")\n\nprint(X)\n\n   Intercept  C(grp)[T.B]   x1   x2  x1:x2  I(x1 ** 2)\n0        1.0          0.0  1.0  2.0    2.0         1.0\n1        1.0          1.0  2.0  1.0    2.0         4.0\n2        1.0          0.0  3.0  2.0    6.0         9.0\n3        1.0          1.0  4.0  1.0    4.0        16.0\n4        1.0          0.0  5.0  2.0   10.0        25.0\n5        1.0          1.0  6.0  1.0    6.0        36.0\n\n\nNow you can send this to sklearn:\n\nmodel = LinearRegression().fit(X, y)\nprint(model.coef_)\n\n[[ 0.00000000e+00 -6.66133815e-16  1.00000000e+00  4.44089210e-16\n  -6.66133815e-16]]",
    "crumbs": [
      "Data Science + ML",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Patsy</span>"
    ]
  },
  {
    "objectID": "09-Dictionaries.html",
    "href": "09-Dictionaries.html",
    "title": "9¬† Dictionaries",
    "section": "",
    "text": "In this lesson we will get to know and become experts in:\n\nDictionaries\n\nDataCamp, Intermediate Python, Chap 2\n\n\n\nDictionaries\nA dictionary is basically a lookup table. It stores a collection of key-value pairs, where key and value are Python objects. Each key is associated with a value so that a value can be conveniently retrieved, inserted, modified, or deleted given a particular key.\nThe dictionary or dict may be the most important built-in Python data structure. In other programming languages, dictionaries are sometimes called hash maps or associative arrays.\n\n#Analogy to a language dictionary:\nEnglishGerman = {\"slander\": \"Verleumdung\",\n                 \"salient\" : [\"auffallend\", \"hervorstechend\"],\n                 \"secular\" : \"profan\",\n                 \"rejoice\" : \"jubeln\"}\n\n\n#adding elements:\nEnglishGerman[\"rain\"] = \"Regen\"\nEnglishGerman\n\n{'slander': 'Verleumdung',\n 'salient': ['auffallend', 'hervorstechend'],\n 'secular': 'profan',\n 'rejoice': 'jubeln',\n 'rain': 'Regen'}\n\n\n\neurope = {'spain':'madrid', 'france' : 'paris'}\nprint(europe[\"spain\"])\nprint(\"france\" in europe)\nprint(\"paris\" in europe)#only checks the keys!\neurope[\"germany\"] = \"berlin\"\nprint(europe.keys())\nprint(europe.values())\n\"paris\" in europe.values()\n\nmadrid\nTrue\nFalse\ndict_keys(['spain', 'france', 'germany'])\ndict_values(['madrid', 'paris', 'berlin'])\n\n\nTrue\n\n\n\n\nDictionaries from lists\nHow would we convert two lists into a key: value pair dictionary?\nMethod 1: using zip\n\nrooms=['hallway', 'kitchen', 'living room', 'bedroom', 'bathroom']\nareas=[11.25, 18.0, 20.0, 10.75, 9.5]\n#create list of tuples\nhouse = list(zip(rooms, areas))\nhouse\n#house[\"kitchen\"]\n\n[('hallway', 11.25),\n ('kitchen', 18.0),\n ('living room', 20.0),\n ('bedroom', 10.75),\n ('bathroom', 9.5)]\n\n\n\nhouse = dict(zip(rooms, areas))\nhouse\n#house[\"kitchen\"]\n\n{'hallway': 11.25,\n 'kitchen': 18.0,\n 'living room': 20.0,\n 'bedroom': 10.75,\n 'bathroom': 9.5}\n\n\nYou can directly convert a dictionary to a one-column DataFrame where the keys become the row index and the values become the column entries.\n\nimport pandas as pd\ndf = pd.DataFrame.from_dict(house, orient='index', columns=['value'])\nprint(df)\n\n             value\nhallway      11.25\nkitchen      18.00\nliving room  20.00\nbedroom      10.75\nbathroom      9.50\n\n\nIf you need to iterate over both the keys and values, you can use the items method to iterate over the keys and values as 2-tuples:\n\nfor i in range(5):\n  print(i)\n\n0\n1\n2\n3\n4\n\n\n\n#print(list(europe.items()))\n\nfor country, capital in europe.items():\n    print(capital, \"is the capital of\", country)\n\nmadrid is the capital of spain\nparis is the capital of france\nberlin is the capital of germany\n\n\nNote: You can use integers as keys as well. However -unlike in lists- one should not think of them as positional indices!\n\n#Assume you have a basement:\nhouse[0] = 21.5\nhouse\n\n{'hallway': 11.25,\n 'kitchen': 18.0,\n 'living room': 20.0,\n 'bedroom': 10.75,\n 'bathroom': 9.5,\n 0: 21.5}\n\n\n\n#And there is a difference between the string and the integer index!\nhouse[\"0\"] = 30.5\nhouse\n\n{'hallway': 11.25,\n 'kitchen': 18.0,\n 'living room': 20.0,\n 'bedroom': 10.75,\n 'bathroom': 9.5,\n 0: 21.5,\n '0': 30.5}\n\n\nCategorize a list of words by their first letters as a dictionary of lists:\n\nwords = [\"apple\", \"bat\", \"bar\", \"atom\", \"book\"]\n\nby_letter = {}\n\nfor word in words:\n     letter = word[0]\n     if letter not in by_letter:#if key does not exist yet then add the key value pair !\n        by_letter[letter] = [word]\n     else:#otherwise key exists, so just append\n         by_letter[letter].append(word)\n\nby_letter\nby_letter[\"b\"]\n\n['bat', 'bar', 'book']\n\n\n\nw = \"apple\"\nw[0]\n\n'a'\n\n\n\nTasks\n\nWrite a function named word_count() that takes a string as input and returns a dictionary with each word in the string as a key and the number of times it appears as the value, e.g.¬†word_count(\"I really really really like this really nice book\") should yield: {\"I\":1, \"book\":1 , \"like\":1, \"really\": 4, \"this\":1}\nConvert the dict into a pd dataframe and -using Boolean subsetting- only keep rows with counts &gt; 1!\n\n\ndef word_count(text):\n  words = text.split()\n  frequency = {}\n\n  for i in words:\n      if i not in frequency:\n        frequency[i] = 1\n      else:\n        frequency[i] += 1\n\n  return frequency\n\ntext = \"I really, really, really, like this really nice book!\"\nfinal_counts = word_count(text)\nprint(final_counts)\n\n{'I': 1, 'really,': 3, 'like': 1, 'this': 1, 'really': 1, 'nice': 1, 'book!': 1}\n\n\n\ntext = \"I really, really, really, like this really nice book!\"\ntext.split()\n\n['I',\n 'really,',\n 'really,',\n 'really,',\n 'like',\n 'this',\n 'really',\n 'nice',\n 'book!']\n\n\n\n#is there another function that counts how often values occur in an array ?\nimport numpy as np\nx = [1,2,1,1,2,3,3,4,1]#.count()\nnp.unique(x,return_counts=True)\n\n(array([1, 2, 3, 4]), array([4, 2, 2, 1]))\n\n\n\nx = np.array([1,2,1,1,2,3,3,4,1])\nx &gt; 2\n\n#x[x&gt;2]#maybe this retrieves\n\nx == 2#question !! It is not an assignment !!\n\narray([False,  True, False, False,  True, False, False, False, False])\n\n\n\nimport pandas as pd\n\ny = pd.DataFrame(x)\ny[x&gt;2]\n\n\n  \n    \n\n\n\n\n\n\n0\n\n\n\n\n5\n3\n\n\n6\n3\n\n\n7\n4\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\ndf = pd.DataFrame.from_dict(europe, orient='index', columns=['value'])\ndf\n\n\n  \n    \n\n\n\n\n\n\nvalue\n\n\n\n\nspain\nmadrid\n\n\nfrance\nparis\n\n\ngermany\nberlin\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n#df[df[\"value\"] == \"berlin\"]\n\nselRows = (df[\"value\"] == \"berlin\")\ndf[selRows]\n\n\n  \n    \n\n\n\n\n\n\nvalue\n\n\n\n\ngermany\nberlin\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n  \n\n\n\n\n\n10 Titanic\n\nimport seaborn as sns\n\ntitanic = sns.load_dataset('titanic')\ntitanic\n\n\n  \n    \n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n0\n0\n3\nmale\n22.0\n1\n0\n7.2500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n1\n1\n1\nfemale\n38.0\n1\n0\n71.2833\nC\nFirst\nwoman\nFalse\nC\nCherbourg\nyes\nFalse\n\n\n2\n1\n3\nfemale\n26.0\n0\n0\n7.9250\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n3\n1\n1\nfemale\n35.0\n1\n0\n53.1000\nS\nFirst\nwoman\nFalse\nC\nSouthampton\nyes\nFalse\n\n\n4\n0\n3\nmale\n35.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n0\n2\nmale\n27.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n887\n1\n1\nfemale\n19.0\n0\n0\n30.0000\nS\nFirst\nwoman\nFalse\nB\nSouthampton\nyes\nTrue\n\n\n888\n0\n3\nfemale\nNaN\n1\n2\n23.4500\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n889\n1\n1\nmale\n26.0\n0\n0\n30.0000\nC\nFirst\nman\nTrue\nC\nCherbourg\nyes\nTrue\n\n\n890\n0\n3\nmale\n32.0\n0\n0\n7.7500\nQ\nThird\nman\nTrue\nNaN\nQueenstown\nno\nTrue\n\n\n\n\n891 rows √ó 15 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n#get the age column:\ntitanic[\"age\"]\ntitanic.age\n#this was column subsetting\n#can you get rows 1:2\n#if this was numpy you could just do the folllowing:\n#titanic[0:2,:]\n#one reason is the ability to use strings as indices, so integer indexing needs its own handling.\n#titanic.iloc[0:2,:]\n#titanic.row = range(1)\n#get me rows 13 to 24 (ends inclusive)\ntitanic.iloc[12:24,:]\n\n\n  \n    \n\n\n\n\n\n\nsurvived\npclass\nsex\nage\nsibsp\nparch\nfare\nembarked\nclass\nwho\nadult_male\ndeck\nembark_town\nalive\nalone\n\n\n\n\n12\n0\n3\nmale\n20.0\n0\n0\n8.0500\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n13\n0\n3\nmale\n39.0\n1\n5\n31.2750\nS\nThird\nman\nTrue\nNaN\nSouthampton\nno\nFalse\n\n\n14\n0\n3\nfemale\n14.0\n0\n0\n7.8542\nS\nThird\nchild\nFalse\nNaN\nSouthampton\nno\nTrue\n\n\n15\n1\n2\nfemale\n55.0\n0\n0\n16.0000\nS\nSecond\nwoman\nFalse\nNaN\nSouthampton\nyes\nTrue\n\n\n16\n0\n3\nmale\n2.0\n4\n1\n29.1250\nQ\nThird\nchild\nFalse\nNaN\nQueenstown\nno\nFalse\n\n\n17\n1\n2\nmale\nNaN\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nyes\nTrue\n\n\n18\n0\n3\nfemale\n31.0\n1\n0\n18.0000\nS\nThird\nwoman\nFalse\nNaN\nSouthampton\nno\nFalse\n\n\n19\n1\n3\nfemale\nNaN\n0\n0\n7.2250\nC\nThird\nwoman\nFalse\nNaN\nCherbourg\nyes\nTrue\n\n\n20\n0\n2\nmale\n35.0\n0\n0\n26.0000\nS\nSecond\nman\nTrue\nNaN\nSouthampton\nno\nTrue\n\n\n21\n1\n2\nmale\n34.0\n0\n0\n13.0000\nS\nSecond\nman\nTrue\nD\nSouthampton\nyes\nTrue\n\n\n22\n1\n3\nfemale\n15.0\n0\n0\n8.0292\nQ\nThird\nchild\nFalse\nNaN\nQueenstown\nyes\nTrue\n\n\n23\n1\n1\nmale\n28.0\n0\n0\n35.5000\nS\nFirst\nman\nTrue\nA\nSouthampton\nyes\nTrue\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nWhat are the estimated survival probabilities by sex and class separately?\n\nbySex = titanic.groupby(\"sex\").survived\nbySex.mean()\n\n\n\n\n\n\n\n\nsurvived\n\n\nsex\n\n\n\n\n\nfemale\n0.742038\n\n\nmale\n0.188908\n\n\n\n\ndtype: float64\n\n\nCan you repeat the above but only for passengers with age &gt;= 18\n\nbySex = titanic[titanic[\"age\"] &gt;= 18].groupby(\"sex\").survived\nbySex.mean()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 bySex = titanic[titanic[\"age\"] &gt;= 18].groupby(\"sex\").survived\n      2 bySex.mean()\n\nNameError: name 'titanic' is not defined\n\n\n\n\nnoChildren = titanic[titanic[\"age\"] &gt;= 18]\nbySex = noChildren.groupby(\"sex\").survived\nbySex.mean()\n\n\n\n\n\n\n\n\nsurvived\n\n\nsex\n\n\n\n\n\nfemale\n0.771845\n\n\nmale\n0.177215\n\n\n\n\ndtype: float64\n\n\n\nbyClass = titanic.groupby(\"pclass\").survived\nbyClass.mean()\n\nWhat are the estimated survival probabilities by class and sex simultaneously ?\n\nbyClass = titanic.groupby([\"pclass\",\"sex\"]).survived\nbyClass.mean()\n\n\n\n\n\n\n\n\n\nsurvived\n\n\npclass\nsex\n\n\n\n\n\n1\nfemale\n0.968085\n\n\nmale\n0.368852\n\n\n2\nfemale\n0.921053\n\n\nmale\n0.157407\n\n\n3\nfemale\n0.500000\n\n\nmale\n0.135447\n\n\n\n\ndtype: float64\n\n\nCan we also get counts ?\n\nbyClass = titanic.groupby([\"pclass\",\"sex\"]).survived\n#how do you add multiple functions\nbyClass.agg([\"mean\", \"count\"])\n\n\n  \n    \n\n\n\n\n\n\n\nmean\ncount\n\n\npclass\nsex\n\n\n\n\n\n\n1\nfemale\n0.968085\n94\n\n\nmale\n0.368852\n122\n\n\n2\nfemale\n0.921053\n76\n\n\nmale\n0.157407\n108\n\n\n3\nfemale\n0.500000\n144\n\n\nmale\n0.135447\n347\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\nThere seem to be clear differences in probabilties/proportions.\n\nHow can we be sure that they are not due to small sample sizes ?\nCan we put an uncertainty on the estimates ?\nWrite a function which simulates tossing a ‚Äúbiased coin‚Äù (with \\(p=0.157407\\)) \\(108\\) times and computes the proportion of ‚Äúheads‚Äù.\nCall this function ‚Äúmany‚Äù times and plot a histogram\n\n\ndef toss_biased_coin(pHead, n, seed=None):\n  \"\"\"\n    Simulates n tosses of a biased coin and returns the proportion of heads,\n    using rng.choice() for better conceptual clarity.\n\n    Parameters\n    ----------\n    pHead : float\n        Probability of heads (between 0 and 1).\n    n : int\n        Number of tosses.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    float\n        Proportion of heads observed.\n    \"\"\"\n  rng = np.random.default_rng(seed)\n\n  tosses = rng.choice([0,1], size=n, p=[1-pHead,pHead])\n  cts = np.sum(tosses == 1)\n  cts = np.sum(tosses)\n  prop_heads = cts/n\n\n  prop_heads = np.mean(tosses)\n\n  return prop_heads\n\n\n\ntoss_biased_coin(pHead=0.157407, n= 108)#, seed = 123)\n\nnp.float64(0.17592592592592593)\n\n\n\n#let us run this many times !\npHead_obs = np.zeros(500)\n\nfor i in range(500):\n  pHead_obs[i] = toss_biased_coin(pHead=0.157407, n= 108)\n\n\n\n#95% interval:\nnp.percentile(pHead_obs, [2.5,97.5])\n\narray([0.09259259, 0.22222222])\n\n\n\n#histogram\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\nRelated Questions:\n\nThe board of a large international company consists of 7 women and 3 men. If sex played no role, i.e.¬†if we assume that the probabilities of m/f were equal, how likely is it to see such asymmetric proportions?\nHow does this assessment change if the board consisted of 100 members and there would be 70 men and 30 women ?\n\n\n#let us run this many times !\nnSim = 1000\npHead_obs = np.zeros(nSim)\n\nfor i in range(nSim):\n  pHead_obs[i] = toss_biased_coin(pHead=0.5, n= 10)\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\n\n#how likely is it that one sees sth. \"as extreme as 7/3\"\nnp.mean(pHead_obs &gt;= 0.7) + np.mean(pHead_obs &lt;= 0.3)\n\nnp.float64(0.347)\n\n\n\n#let us run this many times !\nnSim = 10000\npHead_obs = np.zeros(nSim)\n\nfor i in range(nSim):\n  pHead_obs[i] = toss_biased_coin(pHead=0.5, n= 100)\n\nplt.figure(figsize=[6,3]);\nplt.hist(pHead_obs);\n\n\n\n\n\n\n\n\n\nnp.mean(pHead_obs &gt;= 0.7) + np.mean(pHead_obs &lt;= 0.3)\n\nnp.float64(0.0)\n\n\n\n#What is the exact answer of this question given that you now know sigma !\ns = np.sqrt(p*(1-p)/100)\ns\n#how likely to be above 0.7\nzScore = (0.7-0.5)/s\nzScore\n\n#scipy.stat\n\nnp.float64(3.999999999999999)\n\n\n\ns = np.std(pHead_obs)\nprint(s)\n0.5 + 2*s\n0.5 - 2*s\n\n0.15604406428954612\n\n\nnp.float64(0.18791187142090776)\n\n\nSUPER IMPORTANT !! What is the stdev in the binomial distribution ??\nThe FAMOUS SQRT(N) LAW !!!\n\np= 0.5\n#Variance:\np*(1-p)\nnp.sqrt(p*(1-p)/10)\n\nnp.float64(0.15811388300841897)",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Dictionaries</span>"
    ]
  },
  {
    "objectID": "10-Intro2Pandas.html",
    "href": "10-Intro2Pandas.html",
    "title": "10¬† Intro to pandas",
    "section": "",
    "text": "Introduction to pandas\nWhile numpy offers a lot of powerful numerical capabilities it lacks some of the necessary convenience and natural of handling data as we encounter them. For example, we would typically like to - mix data types (strings, numbers, categories, Boolean, ‚Ä¶) - refer to columns and rows by names - summarize and visualize data in efficient pivot style manners\nAll of the above (and more) can be achieved easily by extending the concept of an array (or a matrix) to a so called dataframe.",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "10-Intro2Pandas.html#introduction-to-pandas",
    "href": "10-Intro2Pandas.html#introduction-to-pandas",
    "title": "10¬† Intro to pandas",
    "section": "",
    "text": "Series\nFirst we review the concept of a one-dimensional array, which in pandas is referred to as a series. Take a look at chapter 5 in the Python for Data Analysis book.\nA Series is a one-dimensional array-like object containing a sequence of values (of similar types to NumPy types) of the same type and an associated array of data labels, called its index. The simplest Series is formed from only an array of data:\n\nobj = pd.Series([4, 7, -5, 3])\nobj\n\nOften, you‚Äôll want to create a Series with an index identifying each data point with a label:\n\nobj2 = pd.Series([4, 7, -5, 3], index=[\"d\", \"b\", \"a\", \"c\"])\nobj2\n\nCompared with NumPy arrays, you can use labels in the index when selecting single values or a set of values:\n\nprint(obj2[\"a\"])\nobj2[[\"c\", \"a\", \"d\"]]\n\n\n\nData Frames\nThere are many ways to construct a DataFrame, though one of the most common is from a dictionary of equal-length lists or NumPy arrays:\n\ndata = {\"state\": [\"Ohio\", \"Ohio\", \"Ohio\", \"Nevada\", \"Nevada\", \"Nevada\"],\n        \"year\": [2000, 2001, 2002, 2001, 2002, 2003],\n        \"pop\": [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]}\nframe = pd.DataFrame(data)# creates a dataframe out of the data given!\nframe.head(3)\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\nyear\npop\n\n\n\n\n0\nOhio\n2000\n1.5\n\n\n1\nOhio\n2001\n1.7\n\n\n2\nOhio\n2002\n3.6\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe[2,1]#too bad\n\n\n#to get the full row: use the .iloc method\nframe.iloc[2]\nframe.iloc[2,1]\n\n2002\n\n\n\n\nSubsetting/Slicing\nWe first need to understand the attributes index (=rownames) and columns (= column names):\n\nframe.index\n\nRangeIndex(start=0, stop=6, step=1)\n\n\n\n#We can set a column as an index:\nframe2 = frame.set_index(\"year\")\nprint(frame2)\n#\n\n       state  pop\nyear             \n2000    Ohio  1.5\n2001    Ohio  1.7\n2002    Ohio  3.6\n2001  Nevada  2.4\n2002  Nevada  2.9\n2003  Nevada  3.2\n\n\n\n#it would be nice to access elements in the same fashion as numpy\n#frame2[1,1]\nframe[\"pop\"]\n\n0    1.5\n1    1.7\n2    3.6\n3    2.4\n4    2.9\n5    3.2\nName: pop, dtype: float64\n\n\n\nframe.pop\n\n&lt;bound method DataFrame.pop of     state  year  pop\n0    Ohio  2000  1.5\n1    Ohio  2001  1.7\n2    Ohio  2002  3.6\n3  Nevada  2001  2.4\n4  Nevada  2002  2.9\n5  Nevada  2003  3.2&gt;\n\n\n\n\nAsking for rows\nUnfortunately, we cannot use the simple [row,col] notation that we are used to from numpy arrays. (Try asking for frame[0,1])\nInstead, row subsetting can be achieved with either the .loc() or the .iloc() methods. The latter takes integers, the former indices:\n\nframe2.loc[2001] #note that I am not using quotes !!\n#at first glance this looks like I am asking for the row number 2001 !!\n\n\n  \n    \n      \n\n\n\n\n\n\nstate\npop\n\n\nyear\n\n\n\n\n\n\n2001\nOhio\n1.7\n\n\n2001\nNevada\n2.4\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe2.loc[2001,\"state\"]\n\nyear\n2001      Ohio\n2001    Nevada\nName: state, dtype: object\n\n\n\nframe.iloc[0]#first row\n\nstate    Ohio\nyear     2000\npop       1.5\nName: 0, dtype: object\n\n\n\nframe3 = frame.set_index(\"state\", drop=False)\nprint(frame3)\n\n         state  year  pop\nstate                    \nOhio      Ohio  2000  1.5\nOhio      Ohio  2001  1.7\nOhio      Ohio  2002  3.6\nNevada  Nevada  2001  2.4\nNevada  Nevada  2002  2.9\nNevada  Nevada  2003  3.2\n\n\n\nframe3.loc[\"Ohio\"]\n\n\n  \n    \n      \n\n\n\n\n\n\nyear\npop\n\n\nstate\n\n\n\n\n\n\nOhio\n2000\n1.5\n\n\nOhio\n2001\n1.7\n\n\nOhio\n2002\n3.6\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nframe.iloc[2001]# this does not work because we do not have 2001 rows !\n\n\nframe.iloc[0,1]\n\n2000\n\n\n\n\nAsking for columns\n\n#The columns are also an index:\nframe.columns\n\nIndex(['state', 'year', 'pop'], dtype='object')\n\n\nA column in a DataFrame can be retrieved MUCH easier: as a Series either by dictionary-like notation or by using the dot attribute notation:\n\nframe[\"state\"]\n\n0      Ohio\n1      Ohio\n2      Ohio\n3    Nevada\n4    Nevada\n5    Nevada\nName: state, dtype: object\n\n\n\nframe.year#equivalent to frame[\"year\"]\n\n0    2000\n1    2001\n2    2002\n3    2001\n4    2002\n5    2003\nName: year, dtype: int64\n\n\n\n\nSummary Stats\nJust like in numpy you can compute sums, means, counts and many other summaries along rows and columns, by specifying the axis argument:\n\nheight = np.array([1.79, 1.85, 1.95, 1.55])\nweight = np.array([70, 80, 85, 65])\nhw = np.array([height, weight]).transpose()\n\nhw\n\narray([[ 1.79, 70.  ],\n       [ 1.85, 80.  ],\n       [ 1.95, 85.  ],\n       [ 1.55, 65.  ]])\n\n\n\ndf = pd.DataFrame(hw, columns = [\"height\", \"weight\"])\nprint(df)\n\n   height  weight\n0    1.79    70.0\n1    1.85    80.0\n2    1.95    85.0\n3    1.55    65.0\n\n\n\ndf = pd.DataFrame(hw , columns = [\"height\", \"weight\"],\n                  index = [\"Peter\", \"Matilda\", \"Bee\", \"Tom\"])\nprint(df)\n\n         height  weight\nPeter      1.79    70.0\nMatilda    1.85    80.0\nBee        1.95    85.0\nTom        1.55    65.0\n\n\nCan you extract:\n\nAll weights\nPeter‚Äôs height\nBee‚Äôs full info\nthe average height\nget all persons with height greater than 180cm\n\n\n#see Lab5\n\n\nprint(df.mean(axis=0))\nprint(df.mean(axis=1))# are these averages sensible ?\n\nheight     1.785\nweight    75.000\ndtype: float64\nPeter      35.895\nMatilda    40.925\nBee        43.475\nBee        33.275\ndtype: float64\n\n\nSome methods are neither reductions nor accumulations. describe is one such example, producing multiple summary statistics in one shot:\n\ndf.describe()\n\n\n\n\n\n\n\n\nheight\nweight\n\n\n\n\ncount\n4.000\n4.000000\n\n\nmean\n1.785\n75.000000\n\n\nstd\n0.170\n9.128709\n\n\nmin\n1.550\n65.000000\n\n\n25%\n1.730\n68.750000\n\n\n50%\n1.820\n75.000000\n\n\n75%\n1.875\n81.250000\n\n\nmax\n1.950\n85.000000",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "10-Intro2Pandas.html#built-in-data-sets",
    "href": "10-Intro2Pandas.html#built-in-data-sets",
    "title": "10¬† Intro to pandas",
    "section": "Built in data sets",
    "text": "Built in data sets",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "10-Intro2Pandas.html#gapminder-data",
    "href": "10-Intro2Pandas.html#gapminder-data",
    "title": "10¬† Intro to pandas",
    "section": "Gapminder Data",
    "text": "Gapminder Data\nhttps://www.gapminder.org/fw/world-health-chart/\nhttps://www.ted.com/talks/hans_rosling_the_best_stats_you_ve_ever_seen#t-241405\n\nYou‚Äôve never seen data presented like this. With the drama and urgency of a sportscaster, statistics guru Hans Rosling debunks myths about the so-called ‚Äúdeveloping world.‚Äù\n\n\n#!pip install gapminder\n#!conda install gapminder\nfrom gapminder import gapminder\n#gapminder.to_csv(\"../datasets/gapminder.csv\")\n\n\ngapminder\n\n\n  \n    \n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1699\nZimbabwe\nAfrica\n1987\n62.351\n9216418\n706.157306\n\n\n1700\nZimbabwe\nAfrica\n1992\n60.377\n10704340\n693.420786\n\n\n1701\nZimbabwe\nAfrica\n1997\n46.809\n11404948\n792.449960\n\n\n1702\nZimbabwe\nAfrica\n2002\n39.989\n11926563\n672.038623\n\n\n1703\nZimbabwe\nAfrica\n2007\n43.487\n12311143\n469.709298\n\n\n\n\n1704 rows √ó 6 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nrowNum = range(1, len(gapminder) + 1)\nrowNum[1703]\nlen(gapminder)\nN=gapminder.shape[0]\nN\n\n1704\n\n\n\n#gapminder.insert(0, \"rownum\", range(1, len(gapminder) + 1))\ngapminder.iloc[0:(7+1)]\n\n\n  \n    \n\n\n\n\n\n\nrownum\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\n1\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\n2\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\n3\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\n4\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\n5\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n5\n6\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.113360\n\n\n6\n7\nAfghanistan\nAsia\n1982\n39.854\n12881816\n978.011439\n\n\n7\n8\nAfghanistan\nAsia\n1987\n40.822\n13867957\n852.395945\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n    \n      \n\n\n    \n        \n    \n\n      \n\n\n\n      \n    \n\n    \n  \n\n\n\n#find the unique years\n\n#get the years:\ngapminder[\"year\"]\nnp.unique(gapminder.year)\n\narray([1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002,\n       2007])\n\n\n\n#get all rows with year 1952:\n#Hint:\n#either use Boolean subsetting\ngapminder[\"year\"] == 1952\ngapminder[gapminder[\"year\"] == 1952]\n#or use an index !!\n\n\n  \n    \n      \n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n12\nAlbania\nEurope\n1952\n55.230\n1282697\n1601.056136\n\n\n24\nAlgeria\nAfrica\n1952\n43.077\n9279525\n2449.008185\n\n\n36\nAngola\nAfrica\n1952\n30.015\n4232095\n3520.610273\n\n\n48\nArgentina\nAmericas\n1952\n62.485\n17876956\n5911.315053\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n1644\nVietnam\nAsia\n1952\n40.412\n26246839\n605.066492\n\n\n1656\nWest Bank and Gaza\nAsia\n1952\n43.160\n1030585\n1515.592329\n\n\n1668\nYemen, Rep.\nAsia\n1952\n32.548\n4963829\n781.717576\n\n\n1680\nZambia\nAfrica\n1952\n42.038\n2672000\n1147.388831\n\n\n1692\nZimbabwe\nAfrica\n1952\n48.451\n3080907\n406.884115\n\n\n\n\n142 rows √ó 6 columns",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "10-Intro2Pandas.html#handling-files",
    "href": "10-Intro2Pandas.html#handling-files",
    "title": "10¬† Intro to pandas",
    "section": "Handling Files",
    "text": "Handling Files\nGet to know your friends\n\npd.read_csv\npd.read_table\npd.read_excel\n\n\n'''url = \"https://drive.google.com/file/d/1oIvCdN15UEwt4dCyjkArekHnTrivN43v/view?usp=share_link\"\nurl='https://drive.google.com/uc?id=' + url.split('/')[-2]\ngapminder = pd.read_csv(url, index_col=0)\ngapminder.head()'''\n\n\n  \n    \n      \n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n1\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.853030\n\n\n2\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.100710\n\n\n3\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.197138\n\n\n4\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.981106\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ngapminder.sort_values(by=\"year\").head()\n\n\n\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\n0\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.445314\n\n\n528\nFrance\nEurope\n1952\n67.410\n42459667\n7029.809327\n\n\n540\nGabon\nAfrica\n1952\n37.003\n420702\n4293.476475\n\n\n1656\nWest Bank and Gaza\nAsia\n1952\n43.160\n1030585\n1515.592329\n\n\n552\nGambia\nAfrica\n1952\n30.000\n284320\n485.230659\n\n\n\n\n\n\n\n\n#How many countries?\nCtryCts = gapminder[\"country\"].value_counts()\nCtryCts\n#note the similarity with np.unique(..., return_counts=True)\n\nAfghanistan          12\nPakistan             12\nNew Zealand          12\nNicaragua            12\nNiger                12\n                     ..\nEritrea              12\nEquatorial Guinea    12\nEl Salvador          12\nEgypt                12\nZimbabwe             12\nName: country, Length: 142, dtype: int64\n\n\n\nfrom numpy.random import default_rng\nrng = default_rng()\nrng.choice(gapminder[\"country\"].unique(),2)\ngapminder[\"year\"].unique()\n\narray([1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, 2002,\n       2007])\n\n\n\n#How meaningful are the column stats?\nprint(gapminder.mean(axis=0))\ngapminder.describe()\n\nyear         1.979500e+03\nlifeExp      5.947444e+01\npop          2.960121e+07\ngdpPercap    7.215327e+03\ndtype: float64\n\n\n/var/folders/h4/k73g68ds6xj791sf8cpmlxlc0000gn/T/ipykernel_33611/633466148.py:2: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  print(gapminder.mean(axis=0))\n\n\n\n\n\n\n\n\n\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\ncount\n1704.00000\n1704.000000\n1.704000e+03\n1704.000000\n\n\nmean\n1979.50000\n59.474439\n2.960121e+07\n7215.327081\n\n\nstd\n17.26533\n12.917107\n1.061579e+08\n9857.454543\n\n\nmin\n1952.00000\n23.599000\n6.001100e+04\n241.165876\n\n\n25%\n1965.75000\n48.198000\n2.793664e+06\n1202.060309\n\n\n50%\n1979.50000\n60.712500\n7.023596e+06\n3531.846988\n\n\n75%\n1993.25000\n70.845500\n1.958522e+07\n9325.462346\n\n\nmax\n2007.00000\n82.603000\n1.318683e+09\n113523.132900\n\n\n\n\n\n\n\nSort the index before you slice!!\nChoose a time range and specific countries\n\ngapminder2 = gapminder.set_index(\"year\").sort_index()\ngap1982_92 = gapminder2.loc[1982:1992].reset_index()\ngap1982_92 = gap1982_92.set_index(\"country\").sort_index()\ngap1982_92.loc[\"Afghanistan\":\"Albania\"]\n\n\n\n\n\n\n\n\nyear\ncontinent\nlifeExp\npop\ngdpPercap\n\n\ncountry\n\n\n\n\n\n\n\n\n\nAfghanistan\n1982\nAsia\n39.854\n12881816\n978.011439\n\n\nAfghanistan\n1987\nAsia\n40.822\n13867957\n852.395945\n\n\nAfghanistan\n1992\nAsia\n41.674\n16317921\n649.341395\n\n\nAlbania\n1992\nEurope\n71.581\n3326498\n2497.437901\n\n\nAlbania\n1987\nEurope\n72.000\n3075321\n3738.932735\n\n\nAlbania\n1982\nEurope\n70.420\n2780097\n3630.880722\n\n\n\n\n\n\n\n\ngap1982_92.loc[\"Afghanistan\":\"Albania\",\"lifeExp\"].mean()\n\n56.0585\n\n\n\nimport pandas as pd\n\ncars = pd.read_csv(\"https://raw.githubusercontent.com/markusloecher/DataScience-HWR/main/data/Auto.csv\")\n#d\ncars.shape\n\n(392, 10)\n\n\n\ncars.head()\n\n\n  \n    \n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\nManufacturer\n\n\n\n\n0\n18.0\n8\n307.0\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\nchevrolet\n\n\n1\n15.0\n8\n350.0\n165\n3693\n11.5\n70\n1\nbuick skylark 320\nbuick\n\n\n2\n18.0\n8\n318.0\n150\n3436\n11.0\n70\n1\nplymouth satellite\nplymouth\n\n\n3\n16.0\n8\n304.0\n150\n3433\n12.0\n70\n1\namc rebel sst\namc\n\n\n4\n17.0\n8\n302.0\n140\n3449\n10.5\n70\n1\nford torino\nford",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Intro to pandas</span>"
    ]
  },
  {
    "objectID": "11-Dimensions.html",
    "href": "11-Dimensions.html",
    "title": "11¬† Dimensions as ‚Äúaxis‚Äù",
    "section": "",
    "text": "Row-wise vs.¬†Column-wise operations",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Dimensions as \"axis\"</span>"
    ]
  },
  {
    "objectID": "11-Dimensions.html#averages",
    "href": "11-Dimensions.html#averages",
    "title": "11¬† Dimensions as ‚Äúaxis‚Äù",
    "section": "Averages",
    "text": "Averages\nHere‚Äôs a quick example showing how axis works for row-wise vs column-wise averages in pandas.\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'A': [1, 4, 7],\n    'B': [2, 5, 8],\n    'C': [3, 6, 9]\n})\n\n# Column-wise mean (down the rows)\ncol_means = df.mean(axis=0)\n\n# Row-wise mean (across the columns)\nrow_means = df.mean(axis=1)\n\nprint(\"Column-wise means:\")\nprint(col_means)\n\nprint(\"\\nRow-wise means:\")\nprint(row_means)\n\nOutput:\nColumn-wise means:\nA    4.0\nB    5.0\nC    6.0\ndtype: float64\n\nRow-wise means:\n0    2.0\n1    5.0\n2    8.0\ndtype: float64\n\naxis=0 ‚Üí operate down columns ‚Üí gives you column-wise results.\naxis=1 ‚Üí operate across columns ‚Üí gives you row-wise results.\n\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'A': [1, 4, 7],\n    'B': [2, 5, 8],\n    'C': [3, 6, 9]\n})\n\n# Column-wise mean (down the rows)\ncol_means = df.mean(axis=0)\n\n# Row-wise mean (across the columns)\nrow_means = df.mean(axis=1)\n\nprint(\"Column-wise means:\")\nprint(col_means)\n\nprint(\"\\nRow-wise means:\")\nprint(row_means)\n\nColumn-wise means:\nA    4.0\nB    5.0\nC    6.0\ndtype: float64\n\nRow-wise means:\n0    2.0\n1    5.0\n2    8.0\ndtype: float64\n\n\nThink about data for which it is genuinely meaningful to compute both row-wise and column-wise averages. Here are the some candidates you can actually load directly in Python or R, with explanations why both directions matter.",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Dimensions as \"axis\"</span>"
    ]
  },
  {
    "objectID": "11-Dimensions.html#horizontal-vs.-vertical-concatenations",
    "href": "11-Dimensions.html#horizontal-vs.-vertical-concatenations",
    "title": "11¬† Dimensions as ‚Äúaxis‚Äù",
    "section": "Horizontal vs.¬†vertical concatenations",
    "text": "Horizontal vs.¬†vertical concatenations\nhttps://wesmckinney.com/book/data-wrangling#prep_concat\nBelow is a minimal example showing horizontal vs.¬†vertical concatenation in pandas.",
    "crumbs": [
      "Coding Competencies",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Dimensions as \"axis\"</span>"
    ]
  },
  {
    "objectID": "A-ourFunctions.html",
    "href": "A-ourFunctions.html",
    "title": "Appendix A ‚Äî Useful Functions",
    "section": "",
    "text": "Home Office Simulation\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef HomeOfficeSimulation(n = 20, n_weeks = 100, daysInWeek = 5,\n                         days_in_office = 2, seed=123,\n                         plotIt = True,\n                         verbose = False):\n  rng = np.random.default_rng(seed)\n\n  NumEmpInOffice = np.zeros(n+1, dtype = \"int\")#this is a 1D array of length n+1\n  for j in range(n_weeks):\n    weekdays = np.zeros(daysInWeek, dtype = \"int\")#this is a 1D array of length 5\n\n    for i in range(n):\n      empChoice = rng.choice(np.arange(1,daysInWeek+1),days_in_office, replace = False )\n      weekdays[empChoice -1 ] += 1#this is of length 5 !!\n\n    #NumEmpInOffice[weekdays] += 1 #for each of 5 elements I increment by 1\n    for w in weekdays:\n      NumEmpInOffice[w] += 1\n\n    if (verbose):\n      print(weekdays)\n      #print(NumEmpInOffice)\n      print(pd.Series(NumEmpInOffice, index=range(n + 1)).to_dict())\n\n    if np.sum(weekdays)!= 2*n:\n      print(\"sth. is wrong!\") #sanity check!\n  #we would like to return the probability !\n  #of course they have to add to one in total\n  prob = NumEmpInOffice/np.sum(NumEmpInOffice)\n  if plotIt:\n    plt.figure(figsize=(7, 3))\n    plt.bar(np.arange(n+1),prob)\n    plt.xlabel(\"Number of employees in the office\")\n    plt.ylabel(\"Probability\")\n    plt.title(f\"Distribution of employees in office (n={n})\")\n    plt.grid(alpha=0.3)\n    plt.show()\n  return\n  #return NumEmpInOffice/(n_weeks*5)#(n_weeks*daysInWeek)\nonesim=HomeOfficeSimulation(n = 20, n_weeks = 100, daysInWeek = 5, days_in_office = 2)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "A-ourFunctions.html#power-calculation",
    "href": "A-ourFunctions.html#power-calculation",
    "title": "Appendix A ‚Äî Useful Functions",
    "section": "Power Calculation",
    "text": "Power Calculation\n\nfrom statsmodels.stats.power import NormalIndPower\nfrom statsmodels.stats.proportion import proportion_effectsize\n\ndef power_prop_test(n=None, p1=None, p2=None, power=None, sig_level=0.05):\n    effect_size = proportion_effectsize(p1, p2)\n    analysis = NormalIndPower()\n    return analysis.solve_power(effect_size=effect_size, nobs1=n, alpha=sig_level, power=power, ratio=1)\n\n\npower_prop_test(n=200, p1=0.5, p2=0.6)\n\nnp.float64(0.5214145419211704)\n\n\n\npower_prop_test(p1=0.5, p2=0.6, power=0.8)\n\n387.1677468578107",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "A-ourFunctions.html#permutation-tests",
    "href": "A-ourFunctions.html#permutation-tests",
    "title": "Appendix A ‚Äî Useful Functions",
    "section": "Permutation Tests",
    "text": "Permutation Tests\n\nimport numpy as np\n\n# Permutation tests\n\ndef permutation_sample(data1, data2):\n    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n\n    # Concatenate the data sets: data\n    data = np.concatenate((data1, data2))\n\n    # Permute the concatenated array: permuted_data\n    permuted_data = np.random.permutation(data)\n    #question: would you have been able to use anotehr random function that you already know ?\n\n    # Split the permuted array into two: perm_sample_1, perm_sample_2\n    perm_sample_1 = permuted_data[:len(data1)]\n    perm_sample_2 = permuted_data[len(data1):]\n\n    return perm_sample_1, perm_sample_2\n\ndef draw_perm_reps(data_1, data_2, size=100, func):\n    \"\"\"Generate multiple permutation replicates.\"\"\"\n\n    # Initialize array of replicates: perm_replicates\n    perm_replicates = np.empty(size)\n\n    for i in range(size):\n        # Generate permutation sample\n        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n\n        # Compute the test statistic\n        perm_replicates[i] = func(perm_sample_1, perm_sample_2)\n        #The difference in means is our \"test statistic\"\n        #perm_replicates[i] = np.mean(perm_sample_1) - np.mean(perm_sample_2)\n        #in this definition, could we just use np.mean for the function ??\n\n    return perm_replicates\n\ndef diff_of_means(data_1, data_2):\n    \"\"\"Difference in means of two arrays.\"\"\"\n\n    # The difference of means of data_1, data_2: diff\n    diff = np.mean(data_1) - np.mean(data_2)\n\n    return diff\n\ndef toss_biased_coin(p_head, n, n_experiments):\n    return np.random.binomial(n, p_head, size=n_experiments)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "A-ourFunctions.html#empirical-cumulative-distribution-function",
    "href": "A-ourFunctions.html#empirical-cumulative-distribution-function",
    "title": "Appendix A ‚Äî Useful Functions",
    "section": "Empirical Cumulative Distribution Function",
    "text": "Empirical Cumulative Distribution Function\n\ndef ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) / n\n\n    return x, y",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Useful Functions</span>"
    ]
  },
  {
    "objectID": "B-DeeperDive Pipes.html",
    "href": "B-DeeperDive Pipes.html",
    "title": "Appendix B ‚Äî Dissecting Pipelines",
    "section": "",
    "text": "named_steps are simply the official way to access parts of a Pipeline or ColumnTransformer after fitting.\nLet me break it down clearly with tiny examples.\n\n\nüß© 1. named_steps ‚Äî Accessing steps inside a Pipeline\nSuppose you have:\npipe = Pipeline([\n    (\"preprocess\", preprocessor),\n    (\"model\", LinearRegression())\n])\nAfter fitting:\npipe.fit(X, y)\nYou can access individual steps like a Python dictionary:\npipe.named_steps[\"preprocess\"]      # returns the ColumnTransformer\npipe.named_steps[\"model\"]           # returns the LinearRegression\nWhy is this needed?\nTo get:\n\nmodel coefficients\nthe fitted OneHotEncoder\nthe transformed feature names\n\n\n\n\nüß© 2. named_transformers_ ‚Äî Accessing parts of a ColumnTransformer\nSuppose your ColumnTransformer is:\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols)\n    ]\n)\nInside the pipeline, the name \"cat\" refers to the encoder for categorical columns.\nAfter fitting:\npipe.fit(X, y)\nTo extract the fitted OneHotEncoder:\nohe = pipe.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\nNow ohe is an actual, fitted OneHotEncoder object.\nFrom here you can get feature names:\nohe.get_feature_names_out(categorical_cols)\n\n\n\nüß† Why the underscore?\nsklearn convention:\n\nBefore fitting: named_transformers (no underscore) ‚Üí stores unfitted transformers.\nAfter fitting: named_transformers_ (with underscore) ‚Üí stores fitted transformers.\n\nSame pattern as:\n\ncoef_\nintercept_\nlabels_\n\nThe _ suffix means learned during fitting.\n\n\n\nüì¶ Tiny Minimal Example to Make It Crystal Clear\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"color\": [\"red\", \"blue\", \"red\"],\n    \"size\": [1.2, 3.3, 2.1],\n    \"price\": [10, 15, 12]\n})\n\nX = df[[\"color\", \"size\"]]\ny = df[\"price\"]\n\npre = ColumnTransformer([\n    (\"cat\", OneHotEncoder(drop=\"first\"), [\"color\"]),\n    (\"num\", \"passthrough\", [\"size\"])\n])\n\npipe = Pipeline([\n    (\"preprocess\", pre),\n    (\"model\", LinearRegression())\n])\n\npipe.fit(X, y)\n\n# --- Accessing things ---\nct = pipe.named_steps[\"preprocess\"]               # the ColumnTransformer\nohe = ct.named_transformers_[\"cat\"]               # the fitted OneHotEncoder\n\nprint(\"One-hot categories:\", ohe.categories_)\nprint(\"Feature names:\", ohe.get_feature_names_out([\"color\"]))\n\n\n\nOne-hot categories: [array(['blue', 'red'], dtype=object)]\nFeature names: ['color_red']\n\n\n\npipe.named_steps[\"model\"].coef_\npipe[\"model\"].coef_\n\narray([-0.33333333,  2.22222222])\n\n\n\n\n\nüéØ Summary\n\n\n\n\n\n\n\nWhat you want\nSyntax\n\n\n\n\nGet a step inside a pipeline\npipe.named_steps[\"name\"]\n\n\nGet a transformer inside ColumnTransformer\npipe.named_steps[\"preprocess\"].named_transformers_[\"cat\"]\n\n\nGet OHE feature names\nohe.get_feature_names_out(cols)\n\n\n\n\nHow to get the full feature name extraction for the entire pipeline, including numeric + categorical + interactions.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Dissecting Pipelines</span>"
    ]
  }
]